{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "import getpass\n",
    "if not os.path.isfile('model_module.py') or not os.path.isfile('plot_module.py'):\n",
    "    current_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    additional_path = '/'.join(current_path.split('/')[:-1])\n",
    "    sys.path.append(additional_path)\n",
    "    \n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import create_vocabulary\n",
    "from model_module import get_positions_in_vocabulary\n",
    "from model_module import filter_text\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w', encoding='utf-8')\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    f.close() \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('enwik8_clean', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "print(len(text))\n",
    "f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350671584\n"
     ]
    }
   ],
   "source": [
    "f = open('input.txt', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "print(len(text))\n",
    "f.close() \n",
    "#(not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('very_small.txt', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "print(len(text))\n",
    "f.close() \n",
    "#different\n",
    "offset = 2\n",
    "valid_size = 5\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', ',', '.', '?', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n"
     ]
    }
   ],
   "source": [
    "# словарь и позиции символов в нем для русского\n",
    "vocabulary = ['\\n', ' ', '!', ',', '.', '?', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n",
    "vocabulary_size = len(vocabulary)\n",
    "characters_positions_in_vocabulary = get_positions_in_vocabulary(vocabulary)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"lowercase = \"абвгдеёжзийклмнопрстуфхцчшщьыъэюя\"\n",
    "uppercase = \"AБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ\"\n",
    "other = \" \\n.,;:()?!\\\"-\"\n",
    "allowed_characters = list()\n",
    "allowed_characters.extend(lowercase)\n",
    "allowed_characters.extend(uppercase)\n",
    "allowed_characters.extend(other)\n",
    "text = filter_text(text, allowed_characters)\"\"\"\n",
    "vocabulary = create_vocabulary(text)\n",
    "vocabulary_size = len(vocabulary)\n",
    "characters_positions_in_vocabulary = get_positions_in_vocabulary(vocabulary)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(MODEL):\n",
    "    \n",
    "    \n",
    "    def layer(self,\n",
    "              idx,                   \n",
    "              state,                 \n",
    "              bottom_up):\n",
    "\n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            X = tf.concat([bottom_up, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat,\n",
    "                                               [3*self._num_nodes[idx], self._num_nodes[idx]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")\n",
    "\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            prepaired_input = tf.multiply(modification_vector, input_gate, name=\"prepaired_input\")\n",
    "            prepaired_memory = tf.multiply(state[1], output_gate, name=\"prepaired_memory\")\n",
    "            new_memory = tf.add(prepaired_memory, prepaired_input, name=\"new_memory\")\n",
    "            new_hidden = tf.multiply(tf.tanh(new_memory, name=\"tanh_result_for_new_hidden\"),\n",
    "                                     output_gate,\n",
    "                                     name=\"new_hidden\")\n",
    "        return new_hidden, new_memory\n",
    "    \n",
    "\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "            hidden = inp\n",
    "            new_state = list()\n",
    "            for idx in range(self._num_layers):\n",
    "                hidden, memory = self.layer(idx,\n",
    "                                            state[idx],\n",
    "                                            hidden)\n",
    "                new_state.append((hidden, memory))\n",
    "\n",
    "            return new_state\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            last_hidden_states = list()\n",
    "            state = saved_state\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state  = self.iteration(emb, state, emb_idx)\n",
    "                last_hidden_states.append(state[-1][0])\n",
    "            last_hidden_states = tf.concat(last_hidden_states, 0, name=\"hidden_concat_in_RNN_module\")\n",
    "\n",
    "            return state, last_hidden_states\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_state):\n",
    "        with tf.name_scope('output_module'):\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(hidden_state,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1e-6,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "                 matr_init_parameter=10000.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\":7,\n",
    "                         \"embedding_size\": 8,\n",
    "                         \"output_embedding_size\": 9,\n",
    "                         \"init_parameter\": 10,\n",
    "                         \"matr_init_parameter\": 11,\n",
    "                         \"type\": 12}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"LSTM_matrix_%s_initializer\"\n",
    "                init_bias_name = \"LSTM_bias_%s_initializer\" \n",
    "                matr_name = \"LSTM_matrix_%s\"\n",
    "                bias_name = \"LSTM_bias_%s\"\n",
    "                \n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._embedding_size + self._num_nodes[0],\n",
    "                                                                      4 * self._num_nodes[0]],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(self._init_parameter*matr_init_parameter/(self._embedding_size+self._num_nodes[0])),\n",
    "                                                                     name=init_matr_name%0),\n",
    "                                                 name=matr_name%0))\n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[0]],\n",
    "                                                        name=init_bias_name%0),\n",
    "                                               name=bias_name%0))\n",
    "                if self._num_layers > 1:\n",
    "                    for i in range(self._num_layers - 1):\n",
    "                        self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[i] + self._num_nodes[i+1],\n",
    "                                                                              4 * self._num_nodes[i+1]],\n",
    "                                                                             mean=0.,\n",
    "                                                                             stddev=math.sqrt(self._init_parameter*matr_init_parameter/(self._num_nodes[i]+self._num_nodes[i+1])),\n",
    "                                                                             name=init_matr_name%(i+1)),\n",
    "                                                         name=matr_name%(i+1)))\n",
    "                        self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[i+1]],\n",
    "                                                                name=init_bias_name%(i+1)),\n",
    "                                                       name=bias_name%(i+1)))\n",
    "\n",
    "                dim_classifier_input = self._num_nodes[-1]\n",
    "                \n",
    "                # output module variables\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "\n",
    "\n",
    "                    saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers):\n",
    "                        saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 0)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 0)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 1)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 1))))\n",
    "\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states = self.RNN_module(embedded_inputs, saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers):\n",
    "                        save_list.append(tf.assign(saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0])\n",
    "                    if self._num_layers > 1:\n",
    "                        for i in range(self._num_layers-1):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    regularizer = tf.contrib.layers.l2_regularizer(.5)\n",
    "                    l2_loss = regularizer(self.output_embedding_weights)\n",
    "                    output_embedding_weights_shape = self.output_embedding_weights.get_shape().as_list()\n",
    "                    l2_divider = float(output_embedding_weights_shape[0] * output_embedding_weights_shape[1])\n",
    "                    #optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)                    \n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss + l2_loss / l2_divider))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1))))\n",
    "\n",
    "\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    " \n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._matr_init_parameter)\n",
    "        metadata.append('LSTM_stacked')\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [127, 89, 61],\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.276666 learning rate: 0.009613\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "фвеемСИъг!эБЯ,еУВЮЁЕБ хДКУ\n",
      "рЮАРШ.р\n",
      "тш,Гф?ЬКпЭдЫаЯйГн!ХШЦДнМ Пйэ?РЛпопИЁЁн?ЖЪПисё\n",
      "Ь!Бв,ЖННФЭнооШФЭфКАУ\n",
      "ыЬеЮГБйЯтЮИВйЛгыГАлаДчВсФеапЩМотцХЯОЗВЫойыЕЛдАчЩкеъзфърРрыв\n",
      "ЧлпЪ\n",
      "яЮАЯЖхЁОлЯУЖябхВЯинШНН,уЪмЗцЖыЛбьЮр!ДъуЧЛ,ЪжЮмШЁЪш!ЫчжтМКаПйгНХМ?ПБЧрххвЩЁю\n",
      "?мЧ?эЫСЙъьЛщЕяН!ФХШЦМтДЕяЁ!Б ФрнЬ еУН!ьячцяЭрэ,ЩЪЫСшНРЕЙ?ЗеЦчдэъРнтЪшеёс?шУБмЩем\n",
      "еФБбеПдайнЬЧтЩМ\n",
      "ОГшЭгнъмЫпжКёПгЮгЬ ТпдЪмх.шУЁСдяДВЛьЕаНПЪидПю ИЦеоЙкЛуВняаЩ кзИю\n",
      "================================================================================\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "Average loss at step 100: 3.336854 learning rate: 0.009613\n",
      "Percentage_of correct: 14.29%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Адрима эмугу.\n",
      "ообуемлоаге госишоничой мибос поличне бовеша товотом, яум гву.\n",
      "Ч,с\n",
      "Маволиго.лдезе хосёту ?.дя Х\n",
      "Бо\n",
      "Уялиа..чтосде сь Язымардо тота Емжикиани ноб,.яй\n",
      "э эго. Рчво.\n",
      "намеылодво зо восс \n",
      "Юаши Вволваинусду Лмры?п Нчябо.чыава.хеикабеь с\n",
      "Св Огипа налтакы.\n",
      "Сачртиы чеюдортинойдад явоть зо\n",
      "Апиманиноноты кен?ча Ншобектой\n",
      "Чекашота?жь зе е Янипи зуе.,нокисро.втумо зь Се Флжо.\n",
      "Эа гуме ного   ть Фхоне ИТ\n",
      "================================================================================\n",
      "Validation percentage of correct: 23.00%\n",
      "\n",
      "Average loss at step 200: 2.709245 learning rate: 0.009613\n",
      "Percentage_of correct: 23.69%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "НЙУНЩл\n",
      "Ъсебёю.\n",
      "Олёкею.\n",
      "От вамаранор.\n",
      "А татор мижет,е.\n",
      "Одуйс.\n",
      "Тазоремте,геа.\n",
      "Гет \n",
      "Эи\n",
      "Ч дочнех вебед.\n",
      "Теядьй те неэмжя езуленинс дноте.\n",
      "Нит мовядет отаривасна муже\n",
      "В.л свонрету Зедеел бихьсет ю рож ти тытиту,.\n",
      "Пез,чее!\n",
      "Кажя, не шабоцтенил екоз \n",
      "ЙКвр.\n",
      "Июдясьне тесаме.\n",
      "Нолиме?\n",
      "Помека вжиле.\n",
      "Пы непно.\n",
      "Ои днокс мабине!\n",
      "Э гезерв\n",
      "Еброрьл?\n",
      "Ч?тти тебяь сеолем кемнира певате?\n",
      "Я семьм итинили.\n",
      "Ял.з ке косрихаю.\n",
      "Я\n",
      "================================================================================\n",
      "Validation percentage of correct: 25.40%\n",
      "\n",
      "Average loss at step 300: 2.521528 learning rate: 0.009613\n",
      "Percentage_of correct: 26.86%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Поряно, праирениеш вьзя, клданюшь ны набыге, полипрота втот.\n",
      "Слаабо покнунананил\n",
      "дою зтеке Фыча том.. о выт!\n",
      "Мен...........\n",
      "хешет,.\n",
      "Паче наткяга наля, мазять дон\n",
      "Хонся дозташесароть отжогуя мени?..\n",
      "тет поговонашь тос?я?\n",
      "Ивнала ех в поллият же\n",
      "Ч.\n",
      "Что поряввонень.\n",
      "Сит теды принлидьсего чтот в чтогосем ны не мешить муречь.\n",
      "Я\n",
      "Дннрик, прокобо чом кобес вут подныше отбрегронечют инкмэ чедуй..щене мне стикно\n",
      "================================================================================\n",
      "Validation percentage of correct: 25.00%\n",
      "\n",
      "Average loss at step 400: 2.412961 learning rate: 0.009613\n",
      "Percentage_of correct: 30.60%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "фес р теши минуки уклудявере.\n",
      "Нокехли леземленнонавнКця.\n",
      "Днет, табисы, кововал.\n",
      "\n",
      "Гразслдех всумани на сдерьля.\n",
      "Но снорытатем.\n",
      "Но раму что аблилки тот.\n",
      "Де\n",
      "Блакру \n",
      " здапила...\n",
      "Я мере.\n",
      "А вень.\n",
      "Я наже.\n",
      "Дат дишена маснде.\n",
      "ПИлали, и опнала нокисле \n",
      "мадайням додена.\n",
      "Я долэ хвеет, всала, нобо о.\n",
      "Но прунрал.\n",
      "Хотода течол йдетел мн\n",
      "ьтя в она эет скаготе отужяполийны разюрко мым.\n",
      "Но смоме?\n",
      "Вво Маднетно, чтоби.\n",
      "Е\n",
      "================================================================================\n",
      "Validation percentage of correct: 31.60%\n",
      "\n",
      "Average loss at step 500: 2.247862 learning rate: 0.009613\n",
      "Percentage_of correct: 34.63%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Горена, то, Толой внедеть разношое эпосодопривинке.\n",
      "У семию, что егона был седит\n",
      "Жета одам собач?\n",
      "Никорух, это приборой, Радегпанныйми мок Бат Манхушача!\n",
      "Их ями,\n",
      "Тийдегди забЩредеться рунна, голишно, из нажитьво нет плавта.\n",
      "Вы дету так не мен\n",
      "вогарибовжо был не восачили стоваки осем в кМ\n",
      "ть думоповощь не тебе ты эктильно,\n",
      "Йожям кортон хол, что с экто полёби.\n",
      "Повригомилонех?\n",
      "Я?титя апероче?\n",
      "Это дрегес \n",
      "================================================================================\n",
      "Validation percentage of correct: 35.40%\n",
      "\n",
      "Average loss at step 600: 2.115588 learning rate: 0.009613\n",
      "Percentage_of correct: 38.19%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "ъьйрадостсь.\n",
      "Астащенал уборуна?\n",
      "Ты бы свою предлишься разремай рас фрепот занича\n",
      "я сэмфийтсе хотялась?\n",
      "Правду, только ты мошумение?\n",
      "Не тебе сказаломая, дал на ли\n",
      "дит, как ты гроть омивет.\n",
      "Не накоряте, что?\n",
      "Я не простичишь что пресскич вы лене\n",
      "маним что они хорошаясь.\n",
      "Она сказать всеем на разохен.\n",
      "Вам понер?\n",
      "Нуань брась?\n",
      "П\n",
      "Исжиски...\n",
      "Я денькох, его учами всего срадитей.\n",
      "Если мы задрурызию.\n",
      "Го меиеним?\n",
      "\n",
      "================================================================================\n",
      "Validation percentage of correct: 38.40%\n",
      "\n",
      "Average loss at step 700: 1.961049 learning rate: 0.009613\n",
      "Percentage_of correct: 41.35%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "шите комберы?\n",
      "Ну шам это!\n",
      "Хорошо.\n",
      "Прежньх изза примен.\n",
      "Вратя новно за совтрые со\n",
      "РЛкте.\n",
      "Собра сдейсе меня иммпа Авляет.\n",
      "Подячала дол, как не вукабние сребили не \n",
      "Ьодите делает для стучум Хаконные как реящие повнать?\n",
      "Так что Выи тринома.\n",
      "Хорош\n",
      "Олики.\n",
      "Католуфтоль.\n",
      "Ъадгтивзайми усузковсела во вы.\n",
      "Да.\n",
      "Плител все покачатель, с\n",
      "Утульные пожание во всегда новей брится.\n",
      "Мне тут дол?\n",
      "ридем?\n",
      "О, она оставорчийст\n",
      "================================================================================\n",
      "Validation percentage of correct: 39.40%\n",
      "\n",
      "Average loss at step 800: 1.883975 learning rate: 0.009613\n",
      "Percentage_of correct: 43.65%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Йочему, чтобы всё предлёмы нас и деньгу...\n",
      "Брудна ли?\n",
      "Коенной тут деньм, я нет?\n",
      "\n",
      "Жоними это кликом с дна сойдем это норится.\n",
      "Аззамам?\n",
      "Нет полода, этользи от козт\n",
      "Зрий.\n",
      "Почем бы поштрил предрескл демли об с очистать, прощивай вести, и это вся?\n",
      "ченачать думаете.\n",
      "Тремих эти няспо всё боренатомилый передок вохту её пытала из \n",
      "и?\n",
      "Такой твой я вмешьй ее.\n",
      "Бочешься, что да.\n",
      "Кто с ради Ковины такай очев бы, по\n",
      "================================================================================\n",
      "Validation percentage of correct: 44.20%\n",
      "\n",
      "Average loss at step 900: 1.824652 learning rate: 0.009613\n",
      "Percentage_of correct: 45.38%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Бресит раз последная.\n",
      "Простиматевности.\n",
      "Что я набротами?\n",
      "Я радно!\n",
      "Ты нужно присо\n",
      "Ибли меня.\n",
      "Он....\n",
      "то, чем тебе кротале цесторок только вызади мердр тебя, списиц\n",
      "Скоростной  БОсветил просто с локсойте ори не наноспётал вай рабрею кацный нажин\n",
      "!\n",
      "Эписобая.\n",
      "Возможно его не похоже В наковы эними честости.\n",
      "Не слоза?\n",
      "Коротаешь?\n",
      "Эрицеском в своего чест.\n",
      "Зовиктной.\n",
      "Да, ерицие момствия свидко тебя человеки выр\n",
      "================================================================================\n",
      "Validation percentage of correct: 43.40%\n",
      "\n",
      "Average loss at step 1000: 1.777983 learning rate: 0.009613\n",
      "Percentage_of correct: 46.07%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Асули.\n",
      "Когда обидить это?\n",
      "У черточь тебе...\n",
      "Но он выул.\n",
      "Бромы?\n",
      "Заплательно обхед\n",
      "Хе.\n",
      "Знал тлийчала новом.\n",
      "И это я сюда.!\n",
      "Дэренного достатно.\n",
      "И срадочкы.\n",
      "Вакогда.\n",
      "Энситун Перл.\n",
      "Карток.\n",
      "Я знаю.\n",
      "Он головили от сынсхи?\n",
      "Ник это разможно.\n",
      "Урамить д\n",
      "рудов и утрушну.\n",
      "Все Сонтились в сулнинствии набрались?\n",
      "Что отец с этом уполки?\n",
      "\n",
      "Евас на проготовия спечался!\n",
      "Там долго врами убила гощений.\n",
      "Я всем окщите одевал\n",
      "================================================================================\n",
      "Validation percentage of correct: 44.00%\n",
      "\n",
      "Average loss at step 1100: 1.732003 learning rate: 0.009613\n",
      "Percentage_of correct: 47.06%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "рен прошливая моя дудосшить.\n",
      "Я твои кумион Кооинна.\n",
      "Ты?..\n",
      "Во вхамать этим держем\n",
      "пюить так сейчас детцим?\n",
      "Хорошо еще за Ифлерт малорарат.\n",
      "Это обхасалсся, что мы \n",
      "чиком по возца я я?\n",
      "Я отключайтесь...\n",
      "Ну, ты все.\n",
      "Или быть ой...\n",
      "Зашить.\n",
      "Возвращ\n",
      "фсю?\n",
      "Ты нужен вы проина ода.\n",
      "Алав.\n",
      "Не могли бы с либо меня квенилкой Манн?\n",
      "Нет н\n",
      "пресем тебе спатью.\n",
      "Тысячись ветно.\n",
      "Доктор.\n",
      "Знаю...\n",
      "Прословать.\n",
      "Оне не накроех в\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 43.20%\n",
      "\n",
      "Average loss at step 1200: 1.709271 learning rate: 0.009613\n",
      "Percentage_of correct: 48.19%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Ъпсер Дерле.\n",
      "Я не знаю, всяч в следующем?\n",
      "Вашта не думаешь, во ..\n",
      "Всё как дене в\n",
      "Жтачая тогда потому что вернуться его, который полануталиро.\n",
      "О тебе внути всё го\n",
      "РГ.\n",
      "Теперь привукы и с мартом Рарце.\n",
      "Теперь, это место, мы и свое исчези.\n",
      "Он пот\n",
      "верцев!\n",
      "Фамсу вы.\n",
      "И вот не плечать ещё потому лешку конецсе серетн, мам!\n",
      "Не мерп\n",
      "фипу.\n",
      "Как вы вы имеем внего, что себя рвалем в жила.\n",
      "Так не просто приупявает вс\n",
      "================================================================================\n",
      "Validation percentage of correct: 45.00%\n",
      "\n",
      "Average loss at step 1300: 1.695893 learning rate: 0.009613\n",
      "Percentage_of correct: 48.16%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Вонк сделаться тебя же не вещь таким чте вы был в водные, бесс свой у меня.\n",
      "Да д\n",
      "Ёнами, но стровно ты кадито?\n",
      "Прослупеты.\n",
      "Здравствуйте.\n",
      "Поздно не видело быть на \n",
      "ружия для боненс.\n",
      "Но може реамилкекы.\n",
      "И, господила, мы от процести.\n",
      "И он хенлечн\n",
      "упашть рождениях?\n",
      "Жена, помашная Альшую вопросок, потоми что мы не тебя ялжатару\n",
      "Вобрекскатких деружение.\n",
      "Иложаться.\n",
      "Вы жовлю ушёл.\n",
      "Мкто Банчж в это, но я вина.\n",
      "\n",
      "================================================================================\n",
      "Validation percentage of correct: 46.40%\n",
      "\n",
      "Average loss at step 1400: 1.692303 learning rate: 0.009613\n",
      "Percentage_of correct: 48.62%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "вирюлоса!\n",
      "Окарор, тэдрего рецец.\n",
      "Тоже оправности разбладшим с полова, как такую \n",
      "Эвалист.\n",
      "Я хотишь.\n",
      "Это очень Твоему стомкой, чан.\n",
      "Звонил, как есть здесь здесь.\n",
      "\n",
      "же сюда.\n",
      "Даров рада.\n",
      "Делко.\n",
      "Они больше выглидлен хорошо, Гран вашу прокитой у те\n",
      "ёк...\n",
      "Наверно поднишь тебя так, что и ижен Чанным коммениста?\n",
      "Ах, вы зумлива про\n",
      "Анровку Бомно больше сестра.\n",
      "Ты звольно немного встазажая.\n",
      "Да они просто...\n",
      "Рейн\n",
      "================================================================================\n",
      "Validation percentage of correct: 45.00%\n",
      "\n",
      "Average loss at step 1500: 1.677271 learning rate: 0.009613\n",
      "Percentage_of correct: 48.44%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "ЕДосарии, завых СтарОн исложение того.\n",
      "Больно.\n",
      "Тогда там.\n",
      "Они имку.\n",
      "Эмы коек вер\n",
      "Могдах вместе.\n",
      "Мы с набязовата, что себе нет?\n",
      "Почему?\n",
      "Знает, это так в этого еще\n",
      "Еэтот несвольно.\n",
      "Нет, если бы не могу избегать Мать!\n",
      "Ссату сам.\n",
      "Ну?\n",
      "И проблему, \n",
      "Хэ Свен отдание сеть Это!!\n",
      "Они говорил мне за момент Умела, и помочь, поэтому бы\n",
      "крете.\n",
      "Я так что он, скрусие.\n",
      "Кто это.\n",
      "Хорошо, и я ещё не единственная заревелой\n",
      "================================================================================\n",
      "Validation percentage of correct: 46.00%\n",
      "\n",
      "Average loss at step 1600: 1.665675 learning rate: 0.009613\n",
      "Percentage_of correct: 49.35%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "зь челоме.\n",
      "Предложиться возлятельный!\n",
      "Я продолжално, кудаа!\n",
      "Ты завтра приказрять\n",
      "ыатером у нас темнена.\n",
      "Взнаете, парня, всегда говорил за меня трании?\n",
      "Наторой те\n",
      "С ДБ ББ дбировали потомщим осполкой придумать.\n",
      "Дядем офиовали, такое мрень.\n",
      "Хол \n",
      "Нлюч его яссак хорошая запевка.\n",
      "Твой ворочка новестою наста, поднишь ее!\n",
      "Нет, Ме\n",
      "Яры.\n",
      "Есть высочество, мама Стэндиве.\n",
      "Присутивать, то не лжали со мной, даже нет.\n",
      "================================================================================\n",
      "Validation percentage of correct: 45.60%\n",
      "\n",
      "Average loss at step 1700: 1.642291 learning rate: 0.009613\n",
      "Percentage_of correct: 49.74%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Цоллхн, отличнике.\n",
      "Вечером твос никто будь найти.\n",
      "Больной.\n",
      "А скак что знаешь...\n",
      "\n",
      "ЭаЭтей.\n",
      "Он назад не как.\n",
      "Моего и захудило.\n",
      "Хм...\n",
      "не же весь.\n",
      "Спусти Года моусвом\n",
      "Ыленно?\n",
      "И перед вольстах.\n",
      "Суд?\n",
      "Д вас сделать, когда ли знания, что тоже начинать\n",
      "доху.\n",
      "Я был предлагаю с нееи прошло немного него это.\n",
      "Меня вороно нас не вам ска\n",
      "ОРос Грэма Харор?\n",
      "Так что зачем вставаешь?\n",
      "Не вратить это подфопокосла?\n",
      "Нет.\n",
      "буд\n",
      "================================================================================\n",
      "Validation percentage of correct: 45.80%\n",
      "\n",
      "Average loss at step 1800: 1.620649 learning rate: 0.009613\n",
      "Percentage_of correct: 50.27%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "фстет были верны этеби тоже решьточным сормвах игрок.\n",
      "Не должен были для меня я \n",
      "чиноок.\n",
      "Трубай, чем Хута пытаемся звонием надержита ИОЛТСА!\n",
      "По лабор, и он здесь\n",
      "РОРХА ЧЬЫ   позже не ему свежуем.\n",
      "Защитить нуждаще.\n",
      "И кажелщие.\n",
      "Скичай.\n",
      "Давай на\n",
      "хорить от придласили.\n",
      "Я смогла ты не утов минута.\n",
      "Я собак.\n",
      "Что Го вы прой не мое\n",
      "Дляновый...\n",
      "Я поговоришь, что я не встречаеми Эснив.\n",
      "Добрости.\n",
      "Ладно, нет.\n",
      "Дифош\n",
      "================================================================================\n",
      "Validation percentage of correct: 45.20%\n",
      "\n",
      "Average loss at step 1900: 1.635050 learning rate: 0.009613\n",
      "Percentage_of correct: 49.87%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "йте жалой от этот прохагодаректикаже.\n",
      "Я всё не хоть направильной Три их иди Харс\n",
      "Щжестью, я поймул мой и можно.\n",
      "В потаковила о се., есть, мой метло перед маттаке\n",
      "ИСо, ко тет, рейчать только как блассом.\n",
      "У тебя ноагала Летан Ситоллега ФОБ нбиМ\n",
      "Аверкых у парня, а значита!\n",
      "Заключила день...\n",
      "Почему ты пёрают кга моем ротчет б\n",
      "\n",
      "похоже такое сдетши.\n",
      "Делацу ище нового, что Дэ.\n",
      "Так что...\n",
      "отчетивым.\n",
      "Мужчинное\n",
      "================================================================================\n",
      "Validation percentage of correct: 46.80%\n",
      "\n",
      "INFO:tensorflow:inference_debugging/first is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 2000     Percentage = 48.58%     Time = 250s     Learning rate = 0.0096\n"
     ]
    }
   ],
   "source": [
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            50,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            20,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=2000,\n",
    "            print_intermediate_results = True,\n",
    "          save_path='inference_debugging/first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
