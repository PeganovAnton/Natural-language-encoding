{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "import getpass\n",
    "if not os.path.isfile('model_module.py') or not os.path.isfile('plot_module.py'):\n",
    "    current_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    additional_path = '/'.join(current_path.split('/')[:-1])\n",
    "    sys.path.append(additional_path)\n",
    "    \n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56184664\n"
     ]
    }
   ],
   "source": [
    "f = open('enwik8_clean2', 'rb')\n",
    "text = f.read().decode('utf8')\n",
    "print(len(text))\n",
    "f.close() \n",
    "(not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\t\n",
      " !\"'(),-.?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "print(vocabulary)\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(string_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix,\n",
    "                keep_dims=True):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\")\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=keep_dims,\n",
    "                                     name=\"reduce_mean_in_L2_norm\")\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\")\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.transpose(tf.multiply(tf.transpose(state[2],\n",
    "                                                                       name=\"transposed_state2_in_top_down_prepaired\"),\n",
    "                                                          tf.transpose(top_down,\n",
    "                                                                       name=\"transposed_top_down_in_top_down_prepaired\"),\n",
    "                                                          name=\"multiply_in_top_down_prepaired\"),\n",
    "                                              name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            boundary_state_reversed = tf.subtract(one, state[2], name=\"boundary_state_reversed\")\n",
    "            state0_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_reversed,\n",
    "                                                                     name=\"transposed_boundary_state_reversed_in_state0_prepaired\"),\n",
    "                                                        tf.transpose(state[0],\n",
    "                                                                     name=\"transposed_state0_state0_prepaired\"),\n",
    "                                                        name=\"multiply_in_state0_prepaired\"),\n",
    "                                            name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate, None, 'forget_gate_layer%s' % idx, keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                                      name=\"logical_and_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"),\n",
    "                                                     name=\"to_float_in_copy_flag\"),\n",
    "                                         name=\"copy_flag\")\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                                      name=\"to_float_in_flush_flag\"),\n",
    "                                          name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg,\n",
    "                          \"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate,\n",
    "                                          None,\n",
    "                                          \"forget_gate_layer%s\"%(self._num_layers-1),\n",
    "                                          keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                helper = {\"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, helper\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": self.gradient_name}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "        \"\"\"X = tf.sign(X)\"\"\"\n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\")\n",
    "        return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "            helpers = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "\n",
    "            hidden = inp\n",
    "            boundary = activated_boundary_states\n",
    "            # All layers except for the first and the last ones\n",
    "            for idx in range(num_layers-1):\n",
    "                hidden, memory, boundary, helper = self.not_last_layer(idx,\n",
    "                                                                       state[idx],\n",
    "                                                                       hidden,\n",
    "                                                                       state[idx+1][0],\n",
    "                                                                       boundary)\n",
    "                helpers.append(helper)\n",
    "                new_state.append((hidden, memory, boundary))\n",
    "                boundaries.append(boundary)\n",
    "            hidden, memory, helper = self.last_layer(state[-1],\n",
    "                                                     hidden,\n",
    "                                                     boundary)\n",
    "            helpers.append(helper)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\"),\n",
    "                      \"L2_forget_gate\": tf.stack([helper[\"L2_forget_gate\"] for helper in helpers],\n",
    "                                                 name=\"L2_forget_gate_for_iteration%s\"%iter_idx)}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm,\n",
    "                      \"L2_forget_gate\": tf.stack([helper['L2_forget_gate'] for helper in iteration_helpers],\n",
    "                                                 name=\"L2_forget_gate_all\")}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.transpose(tf.sigmoid(tf.matmul(concat,\n",
    "                                                                    self.output_module_gates_weights,\n",
    "                                                                    name=\"matmul_in_output_module_gates\"),\n",
    "                                                          name=\"sigmoid_in_output_module_gates\"),\n",
    "                                               name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=\"tr_hidden_state_total_%s\"%idx)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=\"tr_gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"),\n",
    "                                               name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "            \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1.,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "                 matr_init_parameter=1000.,\n",
    "                 override_appendix='',\n",
    "                 init_bias=-0.01):               \n",
    "                                                   \n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter\n",
    "        self._init_bias = init_bias\n",
    "        self.gradient_name = 'HardSigmoid' + override_appendix\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"matr_init_parameter\": 14,\n",
    "                         \"init_bias\":15,\n",
    "                         \"type\": 16}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            tf.set_random_seed(1)\n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                \n",
    "                def compute_dim_and_bias(layer_idx):\n",
    "                    bias_init_values = [0.]*(4*self._num_nodes[layer_idx])\n",
    "                    if layer_idx == self._num_layers - 1:\n",
    "                        input_dim = self._num_nodes[-1] + self._num_nodes[-2]\n",
    "                        output_dim = 4 * self._num_nodes[-1]\n",
    "                    else:\n",
    "                        output_dim = 4 * self._num_nodes[layer_idx] + 1\n",
    "                        bias_init_values.append(self._init_bias)\n",
    "                        if layer_idx == 0:\n",
    "                            input_dim = self._embedding_size + self._num_nodes[0] + self._num_nodes[1]\n",
    "                        else:\n",
    "                            input_dim = self._num_nodes[layer_idx - 1] + self._num_nodes[layer_idx] + self._num_nodes[layer_idx+1]\n",
    "                    stddev = math.sqrt(self._init_parameter*matr_init_parameter/input_dim)\n",
    "                    return input_dim, output_dim, bias_init_values, stddev\n",
    "                \n",
    "                for layer_idx in range(self._num_layers):\n",
    "                    input_dim, output_dim, bias_init_values, stddev = compute_dim_and_bias(layer_idx)\n",
    "                    self.Biases.append(tf.Variable(bias_init_values,\n",
    "                                                   name=bias_name%layer_idx))         \n",
    "                    self.Matrices.append(tf.Variable(tf.truncated_normal([input_dim,\n",
    "                                                                          output_dim],\n",
    "                                                                         mean=0.,\n",
    "                                                                         stddev=stddev,\n",
    "                                                                         name=init_matr_name%0),\n",
    "                                                     name=matr_name%layer_idx))    \n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_embedding_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "                    train_inputs_for_slice = tf.stack(train_inputs,\n",
    "                                                      axis=1,\n",
    "                                                      name=\"train_inputs_for_slice\")\n",
    "                    self.train_input_print = tf.reshape(tf.split(train_inputs_for_slice,\n",
    "                                                                 [1, self._batch_size-1],\n",
    "                                                                 name=\"split_in_train_print\")[0],\n",
    "                                                        [self._num_unrollings, -1],\n",
    "                                                        name=\"train_print\")\n",
    "\n",
    "\n",
    "                    self.saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 0)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 0)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 1)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 1)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                      name=saved_state_init_templ%(i, 2)),\n",
    "                                                             trainable=False,\n",
    "                                                              name=saved_state_templ%(i, 2))))\n",
    "                    self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                             tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "                    \n",
    "                    @tf.RegisterGradient(self.gradient_name)\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, self.saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "                    \n",
    "                    self.train_hard_sigm_arg = tf.reshape(tf.split(train_helper[\"hard_sigm_arg\"],\n",
    "                                                                   [1, self._batch_size-1],\n",
    "                                                                   name=\"split_in_train_hard_sigm_arg\")[0],\n",
    "                                                          [self._num_unrollings, -1],\n",
    "                                                          name=\"train_hard_sigm_arg\")\n",
    "                    \n",
    "                    L2_forget_gate_reduced = tf.reduce_mean(train_helper['L2_forget_gate'],\n",
    "                                                            axis=0,\n",
    "                                                            name=\"L2_forget_gate_reduced\")\n",
    "                    \n",
    "                    self.L2_forget_gate = tf.unstack(L2_forget_gate_reduced, name=\"L2_forget_gate_unstacked\")\n",
    "                    \n",
    "                    flush_fractions_stacked = tf.reduce_mean(train_helper['all_boundaries'],\n",
    "                                                             axis=[0, 1],\n",
    "                                                             name='flush_fractions_stacked')\n",
    "                    \n",
    "                    self.flush_fractions = tf.split(flush_fractions_stacked,\n",
    "                                                    self._num_layers-1,\n",
    "                                                    axis=0,\n",
    "                                                    name='flush_fractions')\n",
    "                    \n",
    "                    L2_hard_sigm_arg_stacked = self.L2_norm(train_helper['hard_sigm_arg'],\n",
    "                                                            [0, 1],\n",
    "                                                            'hard_sigm_arg_stacked',\n",
    "                                                            keep_dims=False)\n",
    "                    \n",
    "                    self.L2_hard_sigm_arg = tf.split(L2_hard_sigm_arg_stacked,\n",
    "                                                     self._num_layers-1,\n",
    "                                                     name='hard_sigm_arg')\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(self.saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    #optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    print_list = tf.split(self._train_prediction, self._num_unrollings, name=\"print_list\")\n",
    "                    print_for_slice = tf.stack(print_list, axis=1, name=\"print_for_slice\")\n",
    "                    self.train_output_print = tf.reshape(tf.split(print_for_slice,\n",
    "                                                                  [1, self._batch_size-1],\n",
    "                                                                  name=\"split_in_train_print\")[0],\n",
    "                                                         [self._num_unrollings, -1],\n",
    "                                                         name=\"train_print\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "                    \n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "                # creating control dictionary\n",
    "                all_vars = tf.global_variables()\n",
    "                self.control_dictionary = dict()\n",
    "                #print('building graph')\n",
    "                for variable in all_vars:\n",
    "                    \n",
    "                    list_to_form_name = variable.name.split('/')\n",
    "                    if ':' in list_to_form_name[-1]:\n",
    "                        list_to_form_name[-1] = list_to_form_name[-1].split(':')[0]\n",
    "                    if len(list_to_form_name) < 2:\n",
    "                        name = list_to_form_name[0] \n",
    "                    else:\n",
    "                        name = list_to_form_name[0] + '_' + list_to_form_name[-1]\n",
    "                    norm = self.L2_norm(tf.to_float(variable,\n",
    "                                                    name=\"to_float_in_control_dictionary_for_\"+list_to_form_name[-1]),\n",
    "                                        None,\n",
    "                                        list_to_form_name[-1],\n",
    "                                        keep_dims=False)\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        self.control_dictionary[name] = tf.summary.scalar(name+'_sum', \n",
    "                                                                          norm)\n",
    "                    #print(name, ': ', norm.get_shape().as_list())\n",
    "            forget_template = 'self.L2_forget_gate[%s]'\n",
    "            flush_fractions_template = 'self.flush_fractions[%s]'\n",
    "            L2_hard_sigm_template = 'self.L2_hard_sigm_arg[%s]'\n",
    "            for layer_idx in range(self._num_layers):\n",
    "                self.control_dictionary[forget_template % layer_idx] = tf.summary.scalar(forget_template % layer_idx +'_sum', \n",
    "                                                                                         self.L2_forget_gate[layer_idx])\n",
    "            for layer_idx in range(self._num_layers-1):\n",
    "                self.control_dictionary[flush_fractions_template % layer_idx] = tf.summary.scalar(flush_fractions_template % layer_idx +'_sum', \n",
    "                                                                                                  tf.reshape(self.flush_fractions[layer_idx],\n",
    "                                                                                                             [],\n",
    "                                                                                                             name='reshaping_flush_fractions_%s'%layer_idx))\n",
    "                self.control_dictionary[L2_hard_sigm_template % layer_idx] = tf.summary.scalar(L2_hard_sigm_template % layer_idx +'_sum', \n",
    "                                                                                               tf.reshape(self.L2_hard_sigm_arg[layer_idx],\n",
    "                                                                                                          [],\n",
    "                                                                                                          name='reshaping_L2_hard_sigm_%s'%layer_idx)) \n",
    "            self.control_dictionary['loss'] = tf.summary.scalar('loss_sum', self._loss) \n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "       \n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._matr_init_parameter)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n"
     ]
    }
   ],
   "source": [
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 10,\n",
    "                 3,\n",
    "                 [128, 128, 128],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-99c379a65188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"HM_LSTM/logging/first_summary_log\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.run(1,                # number of times learning_rate is decreased\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# a factor by which learning_rate is decreased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;31m# each 'train_frequency' steps loss and percent correctly predicted letters is calculated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=201,\n",
    "            add_operations=['self.train_hard_sigm_arg', 'self.flush_fractions', 'self.L2_hard_sigm_arg', 'self.L2_forget_gate'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200],\n",
    "            path_to_file_for_saving_collection='first.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='plotting_check.txt')\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'first.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f0f1ca6ab381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpickle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'first.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msave\u001b[0m  \u001b[0;31m# hint to help gc free up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'first.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_file = 'first.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    save  # hint to help gc free up memory\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_parameter_value = 1e-6\n",
    "matr_init_parameter_value = 10000\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:peganov/HM_LSTM/folder_name/name_of_run/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 17.49%     Time = 18s     Learning rate = 0.0009\n"
     ]
    }
   ],
   "source": [
    "model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ 'folder_name' +'/'+'name_of_run'+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 18.29095699999999, 'metadata': [64, 30, 3, [128, 128, 128], 10, 0.8, 100, 100, 0.5, 0.5, 1000, 128, 1024, 1e-06, 10000, 'HM_LSTM'], 'data': {'train': {'perplexity': [24.632861709594728], 'BPC': [4.5201946005579066], 'step': [-1], 'percentage': [17.48645833333333]}, 'validation': {'perplexity': [24.349007891654967], 'BPC': [4.355342844963074], 'step': [-1], 'percentage': [17.8]}}}\n"
     ]
    }
   ],
   "source": [
    "print(model._results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           init parameter:  1e-05\n",
      "      matr init parameter:  50\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 24.06%     Time = 17s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      matr init parameter:  100\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 14.06%     Time = 18s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables\n",
      "      matr init parameter:  1000\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 13.94%     Time = 16s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables\n",
      "      matr init parameter:  10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5956ab78e8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                          \u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# a factor by which the learning rate decreases each 'half_life'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                          \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                          fixed_num_steps=True)\n\u001b[0m\u001b[1;32m     38\u001b[0m         text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n\u001b[1;32m     39\u001b[0m                                                 \u001b[0;34m'peganov/HM_LSTM/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname_of_run\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/variables'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36msimple_run\u001b[0;34m(self, num_averaging_iterations, save_path, min_num_steps, loss_frequency, block_of_steps, num_stairs, decay, stop_percent, save_steps, optional_feed_dict, half_life_fixed, fixed_num_steps)\u001b[0m\n\u001b[1;32m    424\u001b[0m                                     \u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                                     fixed_num_steps=fixed_num_steps)\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mdata_for_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_percentages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_averaging_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_num_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mGLOBAL_STEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_num_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36mcalculate_percentages\u001b[0;34m(self, session, num_averaging_iterations)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_unrollings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_parameters = [1e-5, 1e-6, 1e-7, 1e-8]\n",
    "matr_init_parameters = [50, 100, 1000, 10000, 100000]\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "results_GL = list()\n",
    "run_idx = 0\n",
    "for init_parameter_value in init_parameters:\n",
    "    print(' '*10, \"init parameter: \", init_parameter_value)\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        print(' '*5, \"matr init parameter: \", matr_init_parameter_value)\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        folder_name = 'nn%sis%ssg%sshl%s' % (num_nodes, init_slope, slope_growth, slope_half_life)\n",
    "        model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value,\n",
    "                        override_appendix=str(run_idx))\n",
    "        model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)\n",
    "        text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                                                [10, 75, None])\n",
    "        for i in range(4):\n",
    "            text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', folder_name, name_of_run, 'plots'],\n",
    "                            name_of_run+'#%s' % i,\n",
    "                            show=False)\n",
    "        results_GL.append(model._results[-1])\n",
    "        run_idx += 1\n",
    "        model.destroy()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9bb3de01da55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ops'"
     ]
    }
   ],
   "source": [
    "print(tf.ops._gradient_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/server/thirteenth'\n",
    "pickle_file = 'thirteenth.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory\n",
    "model._results = results_GL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_GL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-313904d4aa83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_GL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_GL' is not defined"
     ]
    }
   ],
   "source": [
    "print(results_GL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_all([0],\n",
    "               plot_validation=True,\n",
    "               indent=1,\n",
    "               save_folder='HM_LSTM/server/thirteenth',\n",
    "               show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling test_folder/plot_debug.pickle\n"
     ]
    }
   ],
   "source": [
    "results_GL = model._results\n",
    "folder_name = 'test_folder'\n",
    "file_name = 'plot_debug.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/server/HM_LSTM3/nn128is0.5sg0.5shl1000'\n",
    "pickle_file = 'nn128is0.5sg0.5shl1000.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAEkCAYAAADpUq91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEipjjDHGmBD9PzMgBiOeLGROAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3eca2dcc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['HM_LSTM3', 'init_tuning', 'plots'],\n",
    "                    'nn128is0.5sg0.5shl1000',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "var = 'aaa'\n",
    "if isinstance(var, str):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def split_to_path_name(path):\n",
    "        parts = path.split('/')\n",
    "        name = parts[-1]\n",
    "        path = '/'.join(parts[:-1])\n",
    "        return path, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "bbbb.pickle\n"
     ]
    }
   ],
   "source": [
    "path, name = split_to_path_name('bbbb.pickle')\n",
    "print(path == '')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'peganov/HM_LSTM_fixed/track_nan'\n",
    "pickle_file = 'track_nan.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    five_input = dictionary['self.train_input_print'][0]\n",
    "    ten_input = dictionary['self.train_input_print'][1]\n",
    "    five_boundaries = dictionary['self.boundaries_for_plot'][0]\n",
    "    ten_boundaries = dictionary['self.boundaries_for_plot'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_boundaries(nparray):\n",
    "    nparray[nparray > 0.] = 1.\n",
    "    nparray[nparray < 0.] = 0.\n",
    "    return nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_double_list(nparray):\n",
    "    length = nparray.shape[0]\n",
    "    double_list = [list(), list()]\n",
    "    for i in range(length):\n",
    "        double_list[0].append(nparray[i, 0])\n",
    "        double_list[1].append(nparray[i, 1])\n",
    "    return double_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_boundaries = transform_to_double_list(five_boundaries)\n",
    "ten_boundaries = transform_to_double_list(ten_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      " towards large businesses and \n",
      "leInsider and PowerPage, were \n"
     ]
    }
   ],
   "source": [
    "print(five_boundaries)\n",
    "print(ten_boundaries)\n",
    "print(five_input)\n",
    "print(ten_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import text_boundaries_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAABFCAYAAADerkV1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOTAzM7MKJwdmZmZW4eTAzMzMKv4fUzXmn/XH5d8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5a02a6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_boundaries_plot(ten_input, ten_boundaries, 'train 10000', ['HM_LSTM', 'server', 'plotting_check', 'plots'], 'train_10000.png', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HM_LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cecbf5cff237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = HM_LSTM(64,\n\u001b[0m\u001b[1;32m      6\u001b[0m                  \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                  \u001b[0mcharacters_positions_in_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HM_LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "model = HM_LSTM(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 10,\n",
    "                 3,\n",
    "                 [56, 59, 62],\n",
    "                 1.,               # init_slope\n",
    "                 0.1,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=1e-7,\n",
    "                 matr_init_parameter=10000)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "summary_dict = {'summaries_collection_frequency': 10,\n",
    "                'summary_tensors': [\"self.control_dictionary['embeddings_matrix_variable']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_2']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_2']\",\n",
    "                                    \"self.control_dictionary['output_gates_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_bias']\",\n",
    "                                    \"self.control_dictionary['output_weights']\",\n",
    "                                    \"self.control_dictionary['output_bias']\"]}\n",
    "\n",
    "saved_state_templ = \"'train_1_saved_state_layer%s_number%s'\"\n",
    "\n",
    "for i in range(model._num_layers):\n",
    "    for j in range(2):\n",
    "        summary_dict['summary_tensors'].append('self.control_dictionary[' + saved_state_templ % (i, j) + ']')\n",
    "for layer_idx in range(model._num_layers):\n",
    "    summary_dict['summary_tensors'].append(\"self.control_dictionary['self.L2_forget_gate[%s]']\"%layer_idx)\n",
    "\n",
    "\n",
    "logdir = \"peganov/HM_LSTM/track_nan/logging/first_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            10,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            10,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=101,\n",
    "            add_operations=['self.train_hard_sigm_arg'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "           #debug=True,\n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "\n",
    "            path_to_file_for_saving_collection='peganov/HM_LSTM/track_nan/track_nan.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='peganov/HM_LSTM/track_nan/track_nan.txt',\n",
    "           save_path=\"peganov/HM_LSTM/track_nan/variables\",\n",
    "             summarizing_logdir=logdir,\n",
    "            summary_dict=summary_dict)\n",
    "results_GL = list(model._results)\n",
    "text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/track_nan/variables',\n",
    "                                                [10, 75, None])\n",
    "\n",
    "for i in range(4):\n",
    "    text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', 'track_nan', 'plots'],\n",
    "                            'plot#%s' % i,\n",
    "                            show=False)\n",
    "\n",
    "folder_name = 'peganov/HM_LSTM/track_nan'\n",
    "file_name = 'track_nan_result.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/HM_LSTM_basic/nn128is0.5sg0.5shl1000'\n",
    "pickle_file = 'nn128is0.5sg0.5shl1000.pickle'\n",
    "init_parameters=[1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "matr_init_parameters=[50., 100., 1000., 10000., 100000., 1000000.]\n",
    "results_GL = list()\n",
    "for init_parameter_value in init_parameters:\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        with open(folder_name + '/' + name_of_run + '/' + name_of_run + '.pickle', 'rb') as f:\n",
    "            save = pickle.load(f)\n",
    "            result = save['results_GL']\n",
    "            results_GL.append(result)\n",
    "            del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['HM_LSTM_basic', 'nn128is0.5sg0.5shl1000'],\n",
    "                    'nn128is0.5sg0.5shl1000',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
