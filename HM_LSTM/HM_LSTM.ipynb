{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "import getpass\n",
    "if not os.path.isfile('model_module.py') or not os.path.isfile('plot_module.py'):\n",
    "    current_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    additional_path = '/'.join(current_path.split('/')[:-1])\n",
    "    sys.path.append(additional_path)\n",
    "    \n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import create_vocabulary\n",
    "from model_module import get_positions_in_vocabulary\n",
    "from model_module import filter_text\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w', encoding='utf-8')\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    f.close() \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56184664\n"
     ]
    }
   ],
   "source": [
    "f = open('enwik8_clean', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "print(len(text))\n",
    "f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351395484\n"
     ]
    }
   ],
   "source": [
    "f = open('input.txt', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "print(len(text))\n",
    "f.close() \n",
    "#(not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"lowercase = \"абвгдеёжзийклмнопрстуфхцчшщьыъэюя\"\n",
    "uppercase = \"AБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ\"\n",
    "other = \" \\n.,;:()?!\\\"-\"\n",
    "allowed_characters = list()\n",
    "allowed_characters.extend(lowercase)\n",
    "allowed_characters.extend(uppercase)\n",
    "allowed_characters.extend(other)\n",
    "text = filter_text(text, allowed_characters)\"\"\"\n",
    "vocabulary = create_vocabulary(text)\n",
    "vocabulary_size = len(vocabulary)\n",
    "characters_positions_in_vocabulary = get_positions_in_vocabulary(vocabulary)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix,\n",
    "                keep_dims=True):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\")\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=keep_dims,\n",
    "                                     name=\"reduce_mean_in_L2_norm\")\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\")\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.multiply(state[2],\n",
    "                                             top_down,\n",
    "                                             name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.multiply(boundary_state_down,\n",
    "                                              bottom_up,\n",
    "                                              name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            boundary_state_reversed = tf.subtract(one, state[2], name=\"boundary_state_reversed\")\n",
    "            state0_prepaired = tf.multiply(boundary_state_reversed,\n",
    "                                           state[0],\n",
    "                                           name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate, None, 'forget_gate_layer%s' % idx, keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                  [[0.]],\n",
    "                                                                  name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                         tf.equal(boundary_state_down,\n",
    "                                                                  [[1.]],\n",
    "                                                                  name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                         name=\"logical_and_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                [[0.]],\n",
    "                                                                name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                       tf.equal(boundary_state_down,\n",
    "                                                                [[0.]],\n",
    "                                                                name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                       name=\"logical_and_in_copy_flag\"),\n",
    "                                        name=\"copy_flag\")\n",
    "                flush_flag = tf.to_float(tf.equal(state[2],\n",
    "                                                  [[1.]],\n",
    "                                                  name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                         name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                memory = state[1]\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(forget_gate,\n",
    "                                                             memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(input_gate,\n",
    "                                                             modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(input_gate,\n",
    "                                                     modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                new_memory = tf.add(tf.add(update_term,\n",
    "                                           copy_term,\n",
    "                                           name=\"add_update_and_copy_in_new_memory\"),\n",
    "                                    flush_term,\n",
    "                                    name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                hidden = state[0]\n",
    "                copy_term = tf.multiply(copy_flag, hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    output_gate,\n",
    "                                                    name=\"multiply_subtract_and_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.add(copy_term, else_term, name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg,\n",
    "                          \"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.multiply(boundary_state_down,\n",
    "                                              bottom_up,\n",
    "                                              name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate,\n",
    "                                          None,\n",
    "                                          \"forget_gate_layer%s\"%(self._num_layers-1),\n",
    "                                          keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                   1.,\n",
    "                                                   name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                          name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                memory = state[1]\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(forget_gate,\n",
    "                                                             memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(input_gate,\n",
    "                                                             modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, memory, name=\"copy_term\")\n",
    "                new_memory = tf.add(update_term,\n",
    "                                    copy_term,\n",
    "                                    name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                hidden = state[0]\n",
    "                copy_term = tf.multiply(copy_flag, hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    output_gate,\n",
    "                                                    name=\"multiply_subtract_and_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.add(copy_term, else_term, name=\"new_hidden\")\n",
    "                helper = {\"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, helper\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": self.gradient_name}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "        \"\"\"X = tf.sign(X)\"\"\"\n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\")\n",
    "        return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "            helpers = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "\n",
    "            hidden = inp\n",
    "            boundary = activated_boundary_states\n",
    "            # All layers except for the first and the last ones\n",
    "            for idx in range(num_layers-1):\n",
    "                hidden, memory, boundary, helper = self.not_last_layer(idx,\n",
    "                                                                       state[idx],\n",
    "                                                                       hidden,\n",
    "                                                                       state[idx+1][0],\n",
    "                                                                       boundary)\n",
    "                helpers.append(helper)\n",
    "                new_state.append((hidden, memory, boundary))\n",
    "                boundaries.append(boundary)\n",
    "            hidden, memory, helper = self.last_layer(state[-1],\n",
    "                                                     hidden,\n",
    "                                                     boundary)\n",
    "            helpers.append(helper)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\"),\n",
    "                      \"L2_forget_gate\": tf.stack([helper[\"L2_forget_gate\"] for helper in helpers],\n",
    "                                                 name=\"L2_forget_gate_for_iteration%s\"%iter_idx)}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm,\n",
    "                      \"L2_forget_gate\": tf.stack([helper['L2_forget_gate'] for helper in iteration_helpers],\n",
    "                                                 name=\"L2_forget_gate_all\")}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        # hidden_states is list of hidden_states by layer, concatenated along batch dimension\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.sigmoid(tf.matmul(concat,\n",
    "                                                       self.output_module_gates_weights,\n",
    "                                                       name=\"matmul_in_output_module_gates\"),\n",
    "                                             name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=1,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                       hidden_state,\n",
    "                                                       name=\"gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.concat(gated_hidden_states,\n",
    "                                            1,\n",
    "                                            name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "            \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1e-6,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "                 matr_init_parameter=100000.,\n",
    "                 override_appendix='',\n",
    "                 init_bias=0.,\n",
    "                 regularization_rate=0.):               \n",
    "                                                   \n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter\n",
    "        self._init_bias = init_bias\n",
    "        self._regularization_rate = regularization_rate\n",
    "        self.gradient_name = 'HardSigmoid' + override_appendix\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"matr_init_parameter\": 14,\n",
    "                         \"init_bias\":15,\n",
    "                         \"regularization_rate\": 16,\n",
    "                         \"type\": 17}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            tf.set_random_seed(1)\n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initializer\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                \n",
    "                def compute_dim_and_bias(layer_idx):\n",
    "                    bias_init_values = [0.]*(4*self._num_nodes[layer_idx])\n",
    "                    if layer_idx == self._num_layers - 1:\n",
    "                        input_dim = self._num_nodes[-1] + self._num_nodes[-2]\n",
    "                        output_dim = 4 * self._num_nodes[-1]\n",
    "                    else:\n",
    "                        output_dim = 4 * self._num_nodes[layer_idx] + 1\n",
    "                        bias_init_values.append(self._init_bias)\n",
    "                        if layer_idx == 0:\n",
    "                            input_dim = self._embedding_size + self._num_nodes[0] + self._num_nodes[1]\n",
    "                        else:\n",
    "                            input_dim = self._num_nodes[layer_idx - 1] + self._num_nodes[layer_idx] + self._num_nodes[layer_idx+1]\n",
    "                    stddev = math.sqrt(self._init_parameter*matr_init_parameter/input_dim)\n",
    "                    return input_dim, output_dim, bias_init_values, stddev\n",
    "                \n",
    "                for layer_idx in range(self._num_layers):\n",
    "                    input_dim, output_dim, bias_init_values, stddev = compute_dim_and_bias(layer_idx)\n",
    "                    self.Biases.append(tf.Variable(bias_init_values,\n",
    "                                                   name=bias_name%layer_idx))         \n",
    "                    self.Matrices.append(tf.Variable(tf.truncated_normal([input_dim,\n",
    "                                                                          output_dim],\n",
    "                                                                         mean=0.,\n",
    "                                                                         stddev=stddev,\n",
    "                                                                         name=init_matr_name%0),\n",
    "                                                     name=matr_name%layer_idx))    \n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_embedding_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "                    train_inputs_for_slice = tf.stack(train_inputs,\n",
    "                                                      axis=1,\n",
    "                                                      name=\"train_inputs_for_slice\")\n",
    "                    self.train_input_print = tf.reshape(tf.split(train_inputs_for_slice,\n",
    "                                                                 [1, self._batch_size-1],\n",
    "                                                                 name=\"split_in_train_print\")[0],\n",
    "                                                        [self._num_unrollings, -1],\n",
    "                                                        name=\"train_print\")\n",
    "\n",
    "\n",
    "                    self.saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 0)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 0)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 1)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 1)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                      name=saved_state_init_templ%(i, 2)),\n",
    "                                                             trainable=False,\n",
    "                                                              name=saved_state_templ%(i, 2))))\n",
    "                    self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                             tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "                    \n",
    "                    @tf.RegisterGradient(self.gradient_name)\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, self.saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "                    \n",
    "                    self.train_hard_sigm_arg = tf.reshape(tf.split(train_helper[\"hard_sigm_arg\"],\n",
    "                                                                   [1, self._batch_size-1],\n",
    "                                                                   name=\"split_in_train_hard_sigm_arg\")[0],\n",
    "                                                          [self._num_unrollings, -1],\n",
    "                                                          name=\"train_hard_sigm_arg\")\n",
    "                    \n",
    "                    L2_forget_gate_reduced = tf.reduce_mean(train_helper['L2_forget_gate'],\n",
    "                                                            axis=0,\n",
    "                                                            name=\"L2_forget_gate_reduced\")\n",
    "                    \n",
    "                    self.L2_forget_gate = tf.unstack(L2_forget_gate_reduced, name=\"L2_forget_gate_unstacked\")\n",
    "                    \n",
    "                    flush_fractions_stacked = tf.reduce_mean(train_helper['all_boundaries'],\n",
    "                                                             axis=[0, 1],\n",
    "                                                             name='flush_fractions_stacked')\n",
    "                    \n",
    "                    self.flush_fractions = tf.split(flush_fractions_stacked,\n",
    "                                                    self._num_layers-1,\n",
    "                                                    axis=0,\n",
    "                                                    name='flush_fractions')\n",
    "                    \n",
    "                    L2_hard_sigm_arg_stacked = self.L2_norm(train_helper['hard_sigm_arg'],\n",
    "                                                            [0, 1],\n",
    "                                                            'hard_sigm_arg_stacked',\n",
    "                                                            keep_dims=False)\n",
    "                    \n",
    "                    self.L2_hard_sigm_arg = tf.split(L2_hard_sigm_arg_stacked,\n",
    "                                                     self._num_layers-1,\n",
    "                                                     name='hard_sigm_arg')\n",
    "                    \n",
    "                    self.l2_output_weights = self.L2_norm(self.output_embedding_weights,\n",
    "                                                        None,\n",
    "                                                        'output',\n",
    "                                                        keep_dims=False)\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(self.saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "                    regularizer = tf.contrib.layers.l2_regularizer(.5)\n",
    "                    l2_loss = regularizer(self.output_embedding_weights)\n",
    "                    output_embedding_weights_shape = self.output_embedding_weights.get_shape().as_list()\n",
    "                    l2_divider = float(output_embedding_weights_shape[0] * output_embedding_weights_shape[1])\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    #optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss + l2_loss / l2_divider))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    print_list = tf.split(self._train_prediction, self._num_unrollings, name=\"print_list\")\n",
    "                    print_for_slice = tf.stack(print_list, axis=1, name=\"print_for_slice\")\n",
    "                    self.train_output_print = tf.reshape(tf.split(print_for_slice,\n",
    "                                                                  [1, self._batch_size-1],\n",
    "                                                                  name=\"split_in_train_print\")[0],\n",
    "                                                         [self._num_unrollings, -1],\n",
    "                                                         name=\"train_print\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "                    \n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "                # creating control dictionary\n",
    "                all_vars = tf.global_variables()\n",
    "                self.control_dictionary = dict()\n",
    "                #print('building graph')\n",
    "                for variable in all_vars:\n",
    "                    \n",
    "                    list_to_form_name = variable.name.split('/')\n",
    "                    if ':' in list_to_form_name[-1]:\n",
    "                        list_to_form_name[-1] = list_to_form_name[-1].split(':')[0]\n",
    "                    if len(list_to_form_name) < 2:\n",
    "                        name = list_to_form_name[0] \n",
    "                    else:\n",
    "                        name = list_to_form_name[0] + '_' + list_to_form_name[-1]\n",
    "                    norm = self.L2_norm(tf.to_float(variable,\n",
    "                                                    name=\"to_float_in_control_dictionary_for_\"+list_to_form_name[-1]),\n",
    "                                        None,\n",
    "                                        list_to_form_name[-1],\n",
    "                                        keep_dims=False)\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        self.control_dictionary[name] = tf.summary.scalar(name+'_sum', \n",
    "                                                                          norm)\n",
    "                    #print(name, ': ', norm.get_shape().as_list())\n",
    "            forget_template = 'self.L2_forget_gate[%s]'\n",
    "            flush_fractions_template = 'self.flush_fractions[%s]'\n",
    "            L2_hard_sigm_template = 'self.L2_hard_sigm_arg[%s]'\n",
    "            for layer_idx in range(self._num_layers):\n",
    "                self.control_dictionary[forget_template % layer_idx] = tf.summary.scalar(forget_template % layer_idx +'_sum', \n",
    "                                                                                         self.L2_forget_gate[layer_idx])\n",
    "            for layer_idx in range(self._num_layers-1):\n",
    "                self.control_dictionary[flush_fractions_template % layer_idx] = tf.summary.scalar(flush_fractions_template % layer_idx +'_sum', \n",
    "                                                                                                  tf.reshape(self.flush_fractions[layer_idx],\n",
    "                                                                                                             [],\n",
    "                                                                                                             name='reshaping_flush_fractions_%s'%layer_idx))\n",
    "                self.control_dictionary[L2_hard_sigm_template % layer_idx] = tf.summary.scalar(L2_hard_sigm_template % layer_idx +'_sum', \n",
    "                                                                                               tf.reshape(self.L2_hard_sigm_arg[layer_idx],\n",
    "                                                                                                          [],\n",
    "                                                                                                          name='reshaping_L2_hard_sigm_%s'%layer_idx)) \n",
    "            self.control_dictionary['loss'] = tf.summary.scalar('loss_sum', self._loss) \n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "       \n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._matr_init_parameter)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append(self._regularization_rate)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n"
     ]
    }
   ],
   "source": [
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 10,\n",
    "                 3,\n",
    "                 [128, 128, 128],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.158884 learning rate: 0.002582\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "YAfN,BkN\n",
      ",.\"bCRihk(\n",
      "KefRXq\t)ZK-hg GPZtPhkNZqAWROzhK)FKvOmV.\n",
      "(.YzALjX-BmY.VWiWRj)\n",
      "qu\"wg'VzM-UN\n",
      "fY\n",
      "Z'vnBdAqLq j,PE\td\"bsj\"B,ZO(zFD\n",
      "Niwgf-VtFkCs,SHeIn(\"x\"TJFJjbWqrML\n",
      "quWo!dqCsTaMzGl!\"H'dmu\"bxdX'Jy)AUkEzmPEREW\"l,Rfz??nSXy\n",
      "'.(ACrADNT\"dW\n",
      "YfCWmX-jI\"f\n",
      "xYA-rOpKjS!,tnm\n",
      "MVCojI,R!iXZhKOGISO(tZuMbuBKuht\"kKxPlx\n",
      "spW\"rApYvVAeublnhaRj(UvJt\n",
      "nEcEu,\")BkM\"a\"?pa\t\tzz.jhvWg)oN\"sv kAjhaxdyQL\"SM?tJYdRkjOMVQI(txlTSN,faEgsUKaZZ.(\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [  1.02825405e-03   5.81262502e-05]\n",
      "1:\n",
      "self.sigm_arg:  [ -2.94010795e-04   4.48494717e-07]\n",
      "2:\n",
      "self.sigm_arg:  [  3.91208130e-04   8.09852136e-05]\n",
      "3:\n",
      "self.sigm_arg:  [  2.76119477e-04   5.07745681e-05]\n",
      "4:\n",
      "self.sigm_arg:  [  2.99882784e-04   5.91181342e-05]\n",
      "5:\n",
      "self.sigm_arg:  [ -6.25791086e-04   1.01746912e-06]\n",
      "6:\n",
      "self.sigm_arg:  [ -5.24708776e-05   1.04327330e-06]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.00041681  0.00010462]\n",
      "8:\n",
      "self.sigm_arg:  [ -2.94304278e-04   1.07682638e-06]\n",
      "9:\n",
      "self.sigm_arg:  [  1.33316222e-04   8.72762976e-05]\n",
      "10:\n",
      "self.sigm_arg:  [ -6.26118155e-04   1.08710731e-06]\n",
      "11:\n",
      "self.sigm_arg:  [  5.42928756e-04   8.33804806e-05]\n",
      "12:\n",
      "self.sigm_arg:  [  2.34804902e-04   4.84794800e-05]\n",
      "13:\n",
      "self.sigm_arg:  [  3.00029526e-04   5.92454453e-05]\n",
      "14:\n",
      "self.sigm_arg:  [ -2.94129364e-04   1.09144014e-06]\n",
      "15:\n",
      "self.sigm_arg:  [  7.64403376e-05   8.12434955e-05]\n",
      "16:\n",
      "self.sigm_arg:  [  2.99702282e-04   5.92542128e-05]\n",
      "17:\n",
      "self.sigm_arg:  [ -2.69597018e-04   1.09386303e-06]\n",
      "18:\n",
      "self.sigm_arg:  [ -2.29701560e-04   1.08685629e-06]\n",
      "19:\n",
      "self.sigm_arg:  [  4.90342616e-04   8.95805788e-05]\n",
      "20:\n",
      "self.sigm_arg:  [ -2.76417064e-04   1.09018504e-06]\n",
      "21:\n",
      "self.sigm_arg:  [  2.86179013e-04   7.36953734e-05]\n",
      "22:\n",
      "self.sigm_arg:  [  5.45317773e-04   5.33195634e-05]\n",
      "23:\n",
      "self.sigm_arg:  [ -2.76138569e-04   1.09039968e-06]\n",
      "24:\n",
      "self.sigm_arg:  [ -6.16239675e-04   1.08509914e-06]\n",
      "25:\n",
      "self.sigm_arg:  [  3.99433484e-04   8.93034521e-05]\n",
      "26:\n",
      "self.sigm_arg:  [  2.99796666e-04   5.92519464e-05]\n",
      "27:\n",
      "self.sigm_arg:  [ -6.25806802e-04   1.09237874e-06]\n",
      "28:\n",
      "self.sigm_arg:  [  3.95516952e-04   7.80416958e-05]\n",
      "29:\n",
      "self.sigm_arg:  [ -6.26026478e-04   1.09240784e-06]\n",
      "30:\n",
      "self.sigm_arg:  [  4.87599173e-04   8.28846169e-05]\n",
      "31:\n",
      "self.sigm_arg:  [  1.41839031e-04   5.76042185e-05]\n",
      "32:\n",
      "self.sigm_arg:  [ -2.32461651e-04   1.08982124e-06]\n",
      "33:\n",
      "self.sigm_arg:  [ -2.76661332e-04   1.08480810e-06]\n",
      "34:\n",
      "self.sigm_arg:  [  2.84816633e-04   8.58041094e-05]\n",
      "35:\n",
      "self.sigm_arg:  [  5.11779508e-05   4.59730836e-05]\n",
      "36:\n",
      "self.sigm_arg:  [  3.00182117e-04   5.92482502e-05]\n",
      "37:\n",
      "self.sigm_arg:  [ -6.55567928e-05   1.09264795e-06]\n",
      "38:\n",
      "self.sigm_arg:  [ -6.15136814e-04   1.08623055e-06]\n",
      "39:\n",
      "self.sigm_arg:  [  2.78050487e-04   9.22656691e-05]\n",
      "40:\n",
      "self.sigm_arg:  [  2.99473439e-04   5.92509678e-05]\n",
      "41:\n",
      "self.sigm_arg:  [ -5.94150260e-05   1.09218229e-06]\n",
      "42:\n",
      "self.sigm_arg:  [  2.41609188e-04   7.68658138e-05]\n",
      "43:\n",
      "self.sigm_arg:  [ -2.94528261e-04   1.09077803e-06]\n",
      "44:\n",
      "self.sigm_arg:  [  2.91191711e-04   8.89295407e-05]\n",
      "45:\n",
      "self.sigm_arg:  [  2.50701414e-04   4.36525188e-05]\n",
      "46:\n",
      "self.sigm_arg:  [ -2.94173107e-04   1.09263340e-06]\n",
      "47:\n",
      "self.sigm_arg:  [ -8.19623470e-04   1.08621236e-06]\n",
      "48:\n",
      "self.sigm_arg:  [  2.91017350e-04   8.87975984e-05]\n",
      "49:\n",
      "self.sigm_arg:  [  5.45035757e-04   5.33229286e-05]\n",
      "50:\n",
      "self.sigm_arg:  [ -2.94299214e-04   1.09151290e-06]\n",
      "51:\n",
      "self.sigm_arg:  [ -6.59401470e-04   1.08567758e-06]\n",
      "52:\n",
      "self.sigm_arg:  [ -4.67406993e-04   1.08277811e-06]\n",
      "53:\n",
      "self.sigm_arg:  [ 0.00030106  0.00010955]\n",
      "54:\n",
      "self.sigm_arg:  [ -6.26249355e-04   1.09337918e-06]\n",
      "55:\n",
      "self.sigm_arg:  [  3.97238531e-04   8.17553737e-05]\n",
      "56:\n",
      "self.sigm_arg:  [  5.45364281e-04   5.33175480e-05]\n",
      "57:\n",
      "self.sigm_arg:  [ -2.32670631e-04   1.09038149e-06]\n",
      "58:\n",
      "self.sigm_arg:  [ -2.94822530e-04   1.08511733e-06]\n",
      "59:\n",
      "self.sigm_arg:  [  4.80213785e-04   9.46050059e-05]\n",
      "60:\n",
      "self.sigm_arg:  [  2.99532490e-04   5.92507495e-05]\n",
      "61:\n",
      "self.sigm_arg:  [ -2.94129422e-04   1.09177847e-06]\n",
      "62:\n",
      "self.sigm_arg:  [  7.64403449e-05   8.12436847e-05]\n",
      "63:\n",
      "self.sigm_arg:  [  2.99702253e-04   5.92542965e-05]\n",
      "64:\n",
      "self.sigm_arg:  [ -3.86345724e-04   1.09390669e-06]\n",
      "65:\n",
      "self.sigm_arg:  [  2.48318858e-04   7.10327440e-05]\n",
      "66:\n",
      "self.sigm_arg:  [ -2.32875682e-04   1.09288078e-06]\n",
      "67:\n",
      "self.sigm_arg:  [  2.34566483e-04   7.26714570e-05]\n",
      "68:\n",
      "self.sigm_arg:  [  2.99795123e-04   5.92503711e-05]\n",
      "69:\n",
      "self.sigm_arg:  [  1.69838051e-04   5.04211275e-05]\n",
      "70:\n",
      "self.sigm_arg:  [ -2.76213716e-04   1.09272798e-06]\n",
      "71:\n",
      "self.sigm_arg:  [  4.08007996e-04   7.06782957e-05]\n",
      "72:\n",
      "self.sigm_arg:  [ -7.90829290e-05   1.09198220e-06]\n",
      "73:\n",
      "self.sigm_arg:  [ -4.38238712e-05   1.08587403e-06]\n",
      "74:\n",
      "self.sigm_arg:  [ -2.87528936e-04   1.08285815e-06]\n",
      "75:\n",
      "self.sigm_arg:  [  4.53894172e-05   9.62698759e-05]\n",
      "76:\n",
      "self.sigm_arg:  [  2.99655483e-04   5.92549768e-05]\n",
      "77:\n",
      "self.sigm_arg:  [ -2.94129393e-04   1.09333553e-06]\n",
      "78:\n",
      "self.sigm_arg:  [  7.64403521e-05   8.12444632e-05]\n",
      "79:\n",
      "self.sigm_arg:  [  2.99702282e-04   5.92547076e-05]\n",
      "80:\n",
      "self.sigm_arg:  [ -8.69383977e-04   1.09411769e-06]\n",
      "81:\n",
      "self.sigm_arg:  [ -2.96281185e-04   1.08698362e-06]\n",
      "82:\n",
      "self.sigm_arg:  [ -6.05796704e-05   1.08344022e-06]\n",
      "83:\n",
      "self.sigm_arg:  [ 0.0002986   0.00010786]\n",
      "84:\n",
      "self.sigm_arg:  [ -2.76395032e-04   1.09289533e-06]\n",
      "85:\n",
      "self.sigm_arg:  [  2.86180846e-04   7.36970396e-05]\n",
      "86:\n",
      "self.sigm_arg:  [  2.99604551e-04   5.92505858e-05]\n",
      "87:\n",
      "self.sigm_arg:  [ -7.76633620e-04   1.09217865e-06]\n",
      "88:\n",
      "self.sigm_arg:  [ -2.80069944e-04   1.08599045e-06]\n",
      "89:\n",
      "self.sigm_arg:  [  5.52402169e-04   8.98774815e-05]\n",
      "90:\n",
      "self.sigm_arg:  [ -2.33042738e-04   1.08998859e-06]\n",
      "91:\n",
      "self.sigm_arg:  [ -2.76660692e-04   1.08493180e-06]\n",
      "92:\n",
      "self.sigm_arg:  [  4.06644773e-04   8.27902622e-05]\n",
      "93:\n",
      "self.sigm_arg:  [  2.99840787e-04   5.92527140e-05]\n",
      "94:\n",
      "self.sigm_arg:  [ -1.70412241e-05   1.09230234e-06]\n",
      "95:\n",
      "self.sigm_arg:  [ -3.00140557e-04   1.08604502e-06]\n",
      "96:\n",
      "self.sigm_arg:  [  3.87149252e-04   9.14841730e-05]\n",
      "97:\n",
      "self.sigm_arg:  [ -3.87028587e-04   1.09078530e-06]\n",
      "98:\n",
      "self.sigm_arg:  [ -4.64328361e-04   1.08530287e-06]\n",
      "99:\n",
      "self.sigm_arg:  [ 0.00030248  0.00010289]\n",
      "validation example (input and output):\n",
      "input:\n",
      "\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", \n",
      "********************\n",
      "output:\n",
      "QtXBvoYA(AcLwXuPicD\trPR)FPQECVLpVs jqha'AbAjbpzV fETENP(VxW\tp,CASrJMUMQPeN\"jEMqjywMQZZRK)DSamkLmF?eV\n",
      "********************\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      "h spiritua\n",
      "self.train_hard_sigm_arg:  [[ 0.18840963 -0.24316914]\n",
      " [-0.09192818  0.00424938]\n",
      " [-0.15308559 -0.04647562]\n",
      " [ 0.57095248 -0.09267272]\n",
      " [-0.05359627  0.0008319 ]\n",
      " [-0.03109321 -0.04634654]\n",
      " [ 0.50817245  0.0249048 ]\n",
      " [ 0.09738715 -0.17229991]\n",
      " [-0.02752254 -0.00083368]\n",
      " [-0.04355967 -0.00083368]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.53396225]\n",
      "   [1]: [ 0.24905658]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.27814391]\n",
      "   [1]: [ 0.11338153]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.511439\n",
      "   [1]: 0.471367\n",
      "   [2]: 0.486419\n",
      "Average loss at step 100: 3.124172 learning rate: 0.002582\n",
      "Percentage_of correct: 16.73%\n",
      "0:\n",
      "self.sigm_arg:  [-0.05083831 -0.00778873]\n",
      "1:\n",
      "self.sigm_arg:  [-0.29739892 -0.00778873]\n",
      "2:\n",
      "self.sigm_arg:  [-0.00914199 -0.00778873]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.04984406 -0.11906537]\n",
      "4:\n",
      "self.sigm_arg:  [ -7.78995454e-02   3.79458070e-05]\n",
      "5:\n",
      "self.sigm_arg:  [-0.29161027 -0.02910984]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.07438966 -0.24135405]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.20644328 -0.28295746]\n",
      "8:\n",
      "self.sigm_arg:  [-0.02402245  0.00390929]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.15393685 -0.12233589]\n",
      "10:\n",
      "self.sigm_arg:  [-0.02628853 -0.00169861]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.05406237 -0.12516274]\n",
      "12:\n",
      "self.sigm_arg:  [ 0.01841635 -0.0531882 ]\n",
      "13:\n",
      "self.sigm_arg:  [-0.08521754  0.00193838]\n",
      "14:\n",
      "self.sigm_arg:  [-0.26143152 -0.04297055]\n",
      "15:\n",
      "self.sigm_arg:  [ 0.15508671 -0.18692018]\n",
      "16:\n",
      "self.sigm_arg:  [-0.08439583  0.00180544]\n",
      "17:\n",
      "self.sigm_arg:  [-0.21807341 -0.04475687]\n",
      "18:\n",
      "self.sigm_arg:  [ 0.4901439  -0.25009203]\n",
      "19:\n",
      "self.sigm_arg:  [ 0.07280188 -0.11956088]\n",
      "20:\n",
      "self.sigm_arg:  [-0.06490442  0.00345727]\n",
      "21:\n",
      "self.sigm_arg:  [-0.01803206 -0.04566932]\n",
      "22:\n",
      "self.sigm_arg:  [ 0.64074695 -0.12954119]\n",
      "23:\n",
      "self.sigm_arg:  [-0.05637491  0.00117262]\n",
      "24:\n",
      "self.sigm_arg:  [-0.14555052 -0.04602311]\n",
      "25:\n",
      "self.sigm_arg:  [-0.0128789  -0.00421777]\n",
      "26:\n",
      "self.sigm_arg:  [ 0.34639493  0.05450038]\n",
      "27:\n",
      "self.sigm_arg:  [-0.01990196 -0.04580054]\n",
      "28:\n",
      "self.sigm_arg:  [ 0.0158511  -0.08776099]\n",
      "29:\n",
      "self.sigm_arg:  [-0.0335951   0.00040937]\n",
      "30:\n",
      "self.sigm_arg:  [ 0.05016036 -0.16257998]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.06213053 -0.08706407]\n",
      "32:\n",
      "self.sigm_arg:  [ 0.19656201 -0.24339777]\n",
      "33:\n",
      "self.sigm_arg:  [-0.06454587  0.00333646]\n",
      "34:\n",
      "self.sigm_arg:  [-0.01813555 -0.04605988]\n",
      "35:\n",
      "self.sigm_arg:  [ 0.64784938 -0.13173246]\n",
      "36:\n",
      "self.sigm_arg:  [-0.08218794  0.0012026 ]\n",
      "37:\n",
      "self.sigm_arg:  [-0.12921767 -0.04619828]\n",
      "38:\n",
      "self.sigm_arg:  [ 0.7344045   0.00802506]\n",
      "39:\n",
      "self.sigm_arg:  [ 0.09367308 -0.16034886]\n",
      "40:\n",
      "self.sigm_arg:  [-0.07339121 -0.00133859]\n",
      "41:\n",
      "self.sigm_arg:  [-0.29012179 -0.00133859]\n",
      "42:\n",
      "self.sigm_arg:  [-0.03903905 -0.00133859]\n",
      "43:\n",
      "self.sigm_arg:  [ 0.24865583 -0.01143884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44:\n",
      "self.sigm_arg:  [ -7.79004544e-02  -5.15342690e-05]\n",
      "45:\n",
      "self.sigm_arg:  [ -2.26129204e-01  -5.15342690e-05]\n",
      "46:\n",
      "self.sigm_arg:  [ 0.27160281 -0.00997316]\n",
      "47:\n",
      "self.sigm_arg:  [ 0.07316556 -0.11153366]\n",
      "48:\n",
      "self.sigm_arg:  [ 0.0795665  -0.11420396]\n",
      "49:\n",
      "self.sigm_arg:  [ 0.08026987 -0.12570539]\n",
      "50:\n",
      "self.sigm_arg:  [-0.02166149  0.00320562]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.22124939 -0.21437483]\n",
      "52:\n",
      "self.sigm_arg:  [ 0.30646738 -0.42994964]\n",
      "53:\n",
      "self.sigm_arg:  [-0.08751333  0.00254796]\n",
      "54:\n",
      "self.sigm_arg:  [-0.2807163  -0.04638572]\n",
      "55:\n",
      "self.sigm_arg:  [-0.12468441 -0.00422785]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.0432766  -0.12768999]\n",
      "57:\n",
      "self.sigm_arg:  [ 0.19821064 -0.24370581]\n",
      "58:\n",
      "self.sigm_arg:  [-0.02190976  0.00334171]\n",
      "59:\n",
      "self.sigm_arg:  [ 0.17511161 -0.1652582 ]\n",
      "60:\n",
      "self.sigm_arg:  [-0.073713   -0.00124964]\n",
      "61:\n",
      "self.sigm_arg:  [-0.27260503 -0.00124964]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.15442666 -0.18404198]\n",
      "63:\n",
      "self.sigm_arg:  [-0.0874486   0.00260387]\n",
      "64:\n",
      "self.sigm_arg:  [-0.30109304 -0.04652017]\n",
      "65:\n",
      "self.sigm_arg:  [-0.12431368 -0.00423012]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.50319499 -0.25042829]\n",
      "67:\n",
      "self.sigm_arg:  [ 0.0169818  -0.05305661]\n",
      "68:\n",
      "self.sigm_arg:  [-0.08764151  0.00262622]\n",
      "69:\n",
      "self.sigm_arg:  [-0.16550624 -0.04648046]\n",
      "70:\n",
      "self.sigm_arg:  [ 0.50035834  0.02544484]\n",
      "71:\n",
      "self.sigm_arg:  [ 0.06889297 -0.1330878 ]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.10647412 -0.13676375]\n",
      "73:\n",
      "self.sigm_arg:  [ 0.16159099 -0.23863792]\n",
      "74:\n",
      "self.sigm_arg:  [-0.02275026  0.00355069]\n",
      "75:\n",
      "self.sigm_arg:  [ 0.19290735 -0.17402017]\n",
      "76:\n",
      "self.sigm_arg:  [-0.07432876 -0.00107756]\n",
      "77:\n",
      "self.sigm_arg:  [-0.27193967 -0.00107756]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.15446973 -0.18387064]\n",
      "79:\n",
      "self.sigm_arg:  [-0.08767746  0.00266522]\n",
      "80:\n",
      "self.sigm_arg:  [-0.21616392 -0.04638566]\n",
      "81:\n",
      "self.sigm_arg:  [ 0.25821179 -0.01408288]\n",
      "82:\n",
      "self.sigm_arg:  [ 0.17319596 -0.24127862]\n",
      "83:\n",
      "self.sigm_arg:  [-0.08769473  0.0026838 ]\n",
      "84:\n",
      "self.sigm_arg:  [-0.30109152 -0.04641356]\n",
      "85:\n",
      "self.sigm_arg:  [-0.09543248 -0.00422681]\n",
      "86:\n",
      "self.sigm_arg:  [ 0.46931422  0.0533244 ]\n",
      "87:\n",
      "self.sigm_arg:  [ 0.0792194  -0.15045705]\n",
      "88:\n",
      "self.sigm_arg:  [-0.04697177 -0.00153776]\n",
      "89:\n",
      "self.sigm_arg:  [-0.03090855 -0.00153776]\n",
      "90:\n",
      "self.sigm_arg:  [ 0.80979073 -0.24603578]\n",
      "91:\n",
      "self.sigm_arg:  [-0.06276936  0.00288382]\n",
      "92:\n",
      "self.sigm_arg:  [-0.04653109 -0.04602266]\n",
      "93:\n",
      "self.sigm_arg:  [ 0.34053507  0.05466254]\n",
      "94:\n",
      "self.sigm_arg:  [ 0.11080517 -0.18288757]\n",
      "95:\n",
      "self.sigm_arg:  [-0.00654014 -0.00094419]\n",
      "96:\n",
      "self.sigm_arg:  [ 0.05048305  0.0113384 ]\n",
      "97:\n",
      "self.sigm_arg:  [-0.05083891 -0.04573768]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.21057743 -0.43204686]\n",
      "99:\n",
      "self.sigm_arg:  [-0.08526404  0.0019582 ]\n",
      "validation example (input and output):\n",
      "input:\n",
      " ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", \n",
      "********************\n",
      "output:\n",
      "lpr tse ttt wCueohsoNdadfidztns ieuea gintoscm\"ocitooni oasoPera f  te)i dtiHs,mr ymtutfn ocpwlrsceM\n",
      "********************\n",
      "Validation percentage of correct: 19.60%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      "n of the s\n",
      "self.train_hard_sigm_arg:  [[-0.00073171 -0.02022451]\n",
      " [-0.2409272  -0.02022451]\n",
      " [ 0.00569924 -0.0149287 ]\n",
      " [ 0.15862134 -0.20045619]\n",
      " [-0.2460846  -0.00956703]\n",
      " [ 0.12699582 -0.29326022]\n",
      " [ 0.29634851 -0.33584577]\n",
      " [-0.02375089 -0.02081295]\n",
      " [-0.27366611 -0.02081295]\n",
      " [ 0.12379152 -0.21658692]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.54150939]\n",
      "   [1]: [ 0.09811321]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.14007989]\n",
      "   [1]: [ 0.15885493]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.511873\n",
      "   [1]: 0.450007\n",
      "   [2]: 0.487853\n",
      "Average loss at step 200: 2.795030 learning rate: 0.002582\n",
      "Percentage_of correct: 24.47%\n",
      "0:\n",
      "self.sigm_arg:  [-0.17157295 -0.00714853]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.02631244 -0.00616128]\n",
      "2:\n",
      "self.sigm_arg:  [-0.03673695  0.0003347 ]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.09641547 -0.18243012]\n",
      "4:\n",
      "self.sigm_arg:  [-0.22381826 -0.0128916 ]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.03048219 -0.01445389]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.10079873 -0.24473609]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.05181455 -0.54000545]\n",
      "8:\n",
      "self.sigm_arg:  [-0.06743035 -0.01852347]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.00876826 -0.20875829]\n",
      "10:\n",
      "self.sigm_arg:  [-0.02406793 -0.00626123]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.01521338 -0.22707997]\n",
      "12:\n",
      "self.sigm_arg:  [ 0.00396161 -0.04985081]\n",
      "13:\n",
      "self.sigm_arg:  [-0.24201338  0.00373568]\n",
      "14:\n",
      "self.sigm_arg:  [ 0.02266684 -0.0413781 ]\n",
      "15:\n",
      "self.sigm_arg:  [ 0.18816134 -0.20266901]\n",
      "16:\n",
      "self.sigm_arg:  [-0.25373968 -0.01242714]\n",
      "17:\n",
      "self.sigm_arg:  [ 0.06713229 -0.24725811]\n",
      "18:\n",
      "self.sigm_arg:  [ 0.31688806 -0.32705319]\n",
      "19:\n",
      "self.sigm_arg:  [ 0.04256349 -0.19113685]\n",
      "20:\n",
      "self.sigm_arg:  [-0.07060913 -0.00676014]\n",
      "21:\n",
      "self.sigm_arg:  [ 0.04412614 -0.15917338]\n",
      "22:\n",
      "self.sigm_arg:  [ 0.05686511 -0.24179502]\n",
      "23:\n",
      "self.sigm_arg:  [-0.06305137 -0.00727893]\n",
      "24:\n",
      "self.sigm_arg:  [-0.08002319 -0.00727893]\n",
      "25:\n",
      "self.sigm_arg:  [-0.047457   -0.00727893]\n",
      "26:\n",
      "self.sigm_arg:  [-0.30641955 -0.00727893]\n",
      "27:\n",
      "self.sigm_arg:  [ 0.00507407 -0.00072124]\n",
      "28:\n",
      "self.sigm_arg:  [ 0.04319061 -0.09716232]\n",
      "29:\n",
      "self.sigm_arg:  [-0.05594168  0.00292343]\n",
      "30:\n",
      "self.sigm_arg:  [-0.00469737 -0.04526818]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.13082077 -0.20502083]\n",
      "32:\n",
      "self.sigm_arg:  [ 0.28940654 -0.33681709]\n",
      "33:\n",
      "self.sigm_arg:  [-0.08441105 -0.01694152]\n",
      "34:\n",
      "self.sigm_arg:  [ 0.04080293 -0.16744275]\n",
      "35:\n",
      "self.sigm_arg:  [-0.00779139 -0.00242635]\n",
      "36:\n",
      "self.sigm_arg:  [-0.08981083 -0.00242635]\n",
      "37:\n",
      "self.sigm_arg:  [ 0.22788624 -0.25397044]\n",
      "38:\n",
      "self.sigm_arg:  [-0.00501704 -0.00687837]\n",
      "39:\n",
      "self.sigm_arg:  [ 0.03430603 -0.1475866 ]\n",
      "40:\n",
      "self.sigm_arg:  [-0.24301599 -0.00624595]\n",
      "41:\n",
      "self.sigm_arg:  [ 0.06791741 -0.21750449]\n",
      "42:\n",
      "self.sigm_arg:  [ 0.03271727 -0.04268683]\n",
      "43:\n",
      "self.sigm_arg:  [-0.03873713  0.00157993]\n",
      "44:\n",
      "self.sigm_arg:  [-0.23443845 -0.04674136]\n",
      "45:\n",
      "self.sigm_arg:  [ 0.04075183 -0.2159889 ]\n",
      "46:\n",
      "self.sigm_arg:  [ 0.02067959  0.01506999]\n",
      "47:\n",
      "self.sigm_arg:  [ 0.04671745 -0.21747518]\n",
      "48:\n",
      "self.sigm_arg:  [ 0.117995   -0.17625175]\n",
      "49:\n",
      "self.sigm_arg:  [ 0.06408897 -0.25238949]\n",
      "50:\n",
      "self.sigm_arg:  [-0.03805457 -0.00922015]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.05067138 -0.11999289]\n",
      "52:\n",
      "self.sigm_arg:  [ 0.006421   -0.21160772]\n",
      "53:\n",
      "self.sigm_arg:  [-0.31651956  0.00115558]\n",
      "54:\n",
      "self.sigm_arg:  [ 0.03040516 -0.05410513]\n",
      "55:\n",
      "self.sigm_arg:  [-0.04757903 -0.00195415]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.07580311 -0.24042109]\n",
      "57:\n",
      "self.sigm_arg:  [ 0.29955888 -0.33865905]\n",
      "58:\n",
      "self.sigm_arg:  [-0.05886447 -0.01883877]\n",
      "59:\n",
      "self.sigm_arg:  [ 0.02888937 -0.16917731]\n",
      "60:\n",
      "self.sigm_arg:  [-0.23554631 -0.00401564]\n",
      "61:\n",
      "self.sigm_arg:  [ 0.01973462 -0.00971695]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.15799791 -0.185385  ]\n",
      "63:\n",
      "self.sigm_arg:  [-0.26397106 -0.00635605]\n",
      "64:\n",
      "self.sigm_arg:  [-0.03519583 -0.00635605]\n",
      "65:\n",
      "self.sigm_arg:  [ 0.46397159 -0.2459998 ]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.32936022 -0.32543346]\n",
      "67:\n",
      "self.sigm_arg:  [-0.01183391 -0.02286357]\n",
      "68:\n",
      "self.sigm_arg:  [-0.28302407 -0.02286357]\n",
      "69:\n",
      "self.sigm_arg:  [ 0.04660951 -0.20430802]\n",
      "70:\n",
      "self.sigm_arg:  [-0.02870972 -0.00866907]\n",
      "71:\n",
      "self.sigm_arg:  [-0.00117    -0.00866907]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.06634205 -0.18513185]\n",
      "73:\n",
      "self.sigm_arg:  [ 0.09874253 -0.2756542 ]\n",
      "74:\n",
      "self.sigm_arg:  [-0.0900347  -0.01081191]\n",
      "75:\n",
      "self.sigm_arg:  [-0.00802668 -0.01081191]\n",
      "76:\n",
      "self.sigm_arg:  [-0.06951289 -0.01081191]\n",
      "77:\n",
      "self.sigm_arg:  [ 0.02510771 -0.02977218]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.14026289 -0.18236306]\n",
      "79:\n",
      "self.sigm_arg:  [-0.26938656 -0.00368934]\n",
      "80:\n",
      "self.sigm_arg:  [ 0.05624497 -0.18303888]\n",
      "81:\n",
      "self.sigm_arg:  [ -2.12865267e-02  -4.84697521e-05]\n",
      "82:\n",
      "self.sigm_arg:  [ 0.08578009 -0.24500278]\n",
      "83:\n",
      "self.sigm_arg:  [-0.27148673 -0.01106648]\n",
      "84:\n",
      "self.sigm_arg:  [-0.00240455 -0.01106648]\n",
      "85:\n",
      "self.sigm_arg:  [ 0.11290949 -0.1283576 ]\n",
      "86:\n",
      "self.sigm_arg:  [-0.27098724  0.0050691 ]\n",
      "87:\n",
      "self.sigm_arg:  [ 0.13569359 -0.3593919 ]\n",
      "88:\n",
      "self.sigm_arg:  [-0.00623489 -0.02593411]\n",
      "89:\n",
      "self.sigm_arg:  [ 0.02512944 -0.25844213]\n",
      "90:\n",
      "self.sigm_arg:  [ 0.32557002 -0.35159653]\n",
      "91:\n",
      "self.sigm_arg:  [-0.06911112 -0.02356912]\n",
      "92:\n",
      "self.sigm_arg:  [-0.00571243 -0.02356912]\n",
      "93:\n",
      "self.sigm_arg:  [-0.20664902 -0.02356912]\n",
      "94:\n",
      "self.sigm_arg:  [ 0.06735525 -0.21748364]\n",
      "95:\n",
      "self.sigm_arg:  [-0.0069345  -0.00908989]\n",
      "96:\n",
      "self.sigm_arg:  [-0.08949735 -0.00908989]\n",
      "97:\n",
      "self.sigm_arg:  [-0.12457128 -0.00908989]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.24341188 -0.22370079]\n",
      "99:\n",
      "self.sigm_arg:  [-0.28060573 -0.01010948]\n",
      "validation example (input and output):\n",
      "input:\n",
      " ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", \n",
      "********************\n",
      "output:\n",
      "bfl onririspm\"vsekeon,hsnism tiiigecfh ipavgbachrd.  on eechtn,apie lwnge estfy\"oc Ennohrhodgsemnn,o\n",
      "********************\n",
      "Validation percentage of correct: 25.00%\n",
      "\n",
      "Pickling first.pickle\n",
      "Pickling peganov/pickle_path.pickle\n",
      "Number of steps = 201     Percentage = 2.06%     Time = 19s     Learning rate = 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=201,\n",
    "            add_operations=['self.train_hard_sigm_arg', 'self.flush_fractions', 'self.L2_hard_sigm_arg', 'self.L2_forget_gate'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200],\n",
    "            path_to_file_for_saving_collection='first.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='plotting_check.txt',\n",
    "          pickle_path='peganov/pickle_path.pickle')\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'first.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f0f1ca6ab381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpickle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'first.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msave\u001b[0m  \u001b[0;31m# hint to help gc free up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'first.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_file = 'first.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    save  # hint to help gc free up memory\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_parameter_value = 1e-6\n",
    "matr_init_parameter_value = 10000\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:peganov/HM_LSTM/folder_name/name_of_run/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 17.49%     Time = 18s     Learning rate = 0.0009\n"
     ]
    }
   ],
   "source": [
    "model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ 'folder_name' +'/'+'name_of_run'+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 18.29095699999999, 'metadata': [64, 30, 3, [128, 128, 128], 10, 0.8, 100, 100, 0.5, 0.5, 1000, 128, 1024, 1e-06, 10000, 'HM_LSTM'], 'data': {'train': {'perplexity': [24.632861709594728], 'BPC': [4.5201946005579066], 'step': [-1], 'percentage': [17.48645833333333]}, 'validation': {'perplexity': [24.349007891654967], 'BPC': [4.355342844963074], 'step': [-1], 'percentage': [17.8]}}}\n"
     ]
    }
   ],
   "source": [
    "print(model._results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           init parameter:  1e-05\n",
      "      matr init parameter:  50\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 24.06%     Time = 17s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      matr init parameter:  100\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 14.06%     Time = 18s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables\n",
      "      matr init parameter:  1000\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 13.94%     Time = 16s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables\n",
      "      matr init parameter:  10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5956ab78e8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                          \u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# a factor by which the learning rate decreases each 'half_life'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                          \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                          fixed_num_steps=True)\n\u001b[0m\u001b[1;32m     38\u001b[0m         text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n\u001b[1;32m     39\u001b[0m                                                 \u001b[0;34m'peganov/HM_LSTM/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname_of_run\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/variables'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36msimple_run\u001b[0;34m(self, num_averaging_iterations, save_path, min_num_steps, loss_frequency, block_of_steps, num_stairs, decay, stop_percent, save_steps, optional_feed_dict, half_life_fixed, fixed_num_steps)\u001b[0m\n\u001b[1;32m    424\u001b[0m                                     \u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                                     fixed_num_steps=fixed_num_steps)\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mdata_for_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_percentages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_averaging_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_num_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mGLOBAL_STEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_num_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36mcalculate_percentages\u001b[0;34m(self, session, num_averaging_iterations)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_unrollings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_parameters = [1e-5, 1e-6, 1e-7, 1e-8]\n",
    "matr_init_parameters = [50, 100, 1000, 10000, 100000]\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "results_GL = list()\n",
    "run_idx = 0\n",
    "for init_parameter_value in init_parameters:\n",
    "    print(' '*10, \"init parameter: \", init_parameter_value)\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        print(' '*5, \"matr init parameter: \", matr_init_parameter_value)\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        folder_name = 'nn%sis%ssg%sshl%s' % (num_nodes, init_slope, slope_growth, slope_half_life)\n",
    "        model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value,\n",
    "                        override_appendix=str(run_idx))\n",
    "        model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)\n",
    "        text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                                                [10, 75, None])\n",
    "        for i in range(4):\n",
    "            text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', folder_name, name_of_run, 'plots'],\n",
    "                            name_of_run+'#%s' % i,\n",
    "                            show=False)\n",
    "        results_GL.append(model._results[-1])\n",
    "        run_idx += 1\n",
    "        model.destroy()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9bb3de01da55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ops'"
     ]
    }
   ],
   "source": [
    "print(tf.ops._gradient_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/server/thirteenth'\n",
    "pickle_file = 'thirteenth.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory\n",
    "model._results = results_GL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_GL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-313904d4aa83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_GL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_GL' is not defined"
     ]
    }
   ],
   "source": [
    "print(results_GL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_all([0],\n",
    "               plot_validation=True,\n",
    "               indent=1,\n",
    "               save_folder='HM_LSTM/server/thirteenth',\n",
    "               show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling test_folder/plot_debug.pickle\n"
     ]
    }
   ],
   "source": [
    "results_GL = model._results\n",
    "folder_name = 'test_folder'\n",
    "file_name = 'plot_debug.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = '/home/anton/Natural-language-encoding/HM_LSTM_res/server/HM_LSTM3/HM_LSTM3_init'\n",
    "pickle_file = 'HM_LSTM3_init.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAEkCAYAAAD6he+BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl5mZmVnJXHCZmZmZlcwFl5mZmVnJ/g+H3y/D+nVhDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1d5c3c6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['HM_LSTM3', 'init_tuning', 'plots'],\n",
    "                    'nn128is0.5sg0.5shl1000',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "var = 'aaa'\n",
    "if isinstance(var, str):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def split_to_path_name(path):\n",
    "        parts = path.split('/')\n",
    "        name = parts[-1]\n",
    "        path = '/'.join(parts[:-1])\n",
    "        return path, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "bbbb.pickle\n"
     ]
    }
   ],
   "source": [
    "path, name = split_to_path_name('bbbb.pickle')\n",
    "print(path == '')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'peganov/HM_LSTM_fixed/track_nan'\n",
    "pickle_file = 'track_nan.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    five_input = dictionary['self.train_input_print'][0]\n",
    "    ten_input = dictionary['self.train_input_print'][1]\n",
    "    five_boundaries = dictionary['self.boundaries_for_plot'][0]\n",
    "    ten_boundaries = dictionary['self.boundaries_for_plot'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_boundaries(nparray):\n",
    "    nparray[nparray > 0.] = 1.\n",
    "    nparray[nparray < 0.] = 0.\n",
    "    return nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_double_list(nparray):\n",
    "    length = nparray.shape[0]\n",
    "    double_list = [list(), list()]\n",
    "    for i in range(length):\n",
    "        double_list[0].append(nparray[i, 0])\n",
    "        double_list[1].append(nparray[i, 1])\n",
    "    return double_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "five_boundaries = transform_to_double_list(five_boundaries)\n",
    "ten_boundaries = transform_to_double_list(ten_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      " towards large businesses and \n",
      "leInsider and PowerPage, were \n"
     ]
    }
   ],
   "source": [
    "print(five_boundaries)\n",
    "print(ten_boundaries)\n",
    "print(five_input)\n",
    "print(ten_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import text_boundaries_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAABFCAYAAADerkV1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOTAzM7MKJwdmZmZW4eTAzMzMKv4fUzXmn/XH5d8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5a02a6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_boundaries_plot(ten_input, ten_boundaries, 'train 10000', ['HM_LSTM', 'server', 'plotting_check', 'plots'], 'train_10000.png', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.158883 learning rate: 0.007326\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "SyxliCDvfMn\tYVdvtRt.M(kOBRb(SPbLHK!cS)HbSVKFunDaOduMKr?WBEuiSQ-a\"ejbLgE!Crft .PF\n",
      "cmB\tnwCCUTBWarXUKsZD-GX,O'zhz?WxFNMrKCKiq,VdsoleNjTSRnBXlhMXe,hWUpGx !LcGQRAJdZ?\n",
      "QNqWSyzDrJNszSN\touBIWQrwiifrbeeSY)'\"jxqHfx\tgvsy)GQjgZm gcok\n",
      "OS\n",
      "TmM!KHI,,uVR'nL.?\n",
      "tqb,.w,AiIk!P\n",
      "pwGpj,SQ',jEb,cx\"x(iJVsQTOKiUA?cprrtyR-yzK?TVsEkeTf\n",
      "o dE)nVHZRz\txT\n",
      ",'GjNQr jbZPG!WrdLgb\n",
      "wpDkvZ MA?giAOLRlonZx?ViYBpnOcNlPCZ,gElsiEJ!Cm)LO)VmWCZFOrv\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [  2.72527104e-04   9.00831583e-05]\n",
      "1:\n",
      "self.sigm_arg:  [ -3.91493260e-04   1.97476493e-05]\n",
      "2:\n",
      "self.sigm_arg:  [ -4.83714073e-04   3.00643969e-05]\n",
      "3:\n",
      "self.sigm_arg:  [ -1.17592193e-04   3.54796102e-05]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.00029102  0.00023124]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.00030686  0.00013831]\n",
      "6:\n",
      "self.sigm_arg:  [ -3.17700615e-05   4.06080326e-05]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.00025983  0.00018454]\n",
      "8:\n",
      "self.sigm_arg:  [ -5.34062565e-04   4.12632326e-05]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.00029336  0.00019204]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "GbdSQVJc \tUs xFfwqx)K\tp.UW!HrreJ?Lj'CbaN!x(Sr\thhX\tHTu,iaMg'db\n",
      "ZsLAsl.\n",
      "cl,UueKZx-OfAFsjsJ'rSnn.DnS.io\n",
      "********************\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      "larized an individualist-anarc\n",
      "self.train_hard_sigm_arg:  [[ -3.91585678e-02  -4.66554165e-02]\n",
      " [  2.72518843e-01   6.49131089e-02]\n",
      " [ -4.17730585e-02  -4.49530818e-02]\n",
      " [  6.94456920e-02   1.05375201e-01]\n",
      " [  9.67217386e-02   2.39411891e-02]\n",
      " [  3.84148866e-01  -4.31421176e-02]\n",
      " [  1.60689354e-01  -1.23215675e-01]\n",
      " [ -4.60983403e-02  -1.60991587e-02]\n",
      " [ -2.75889814e-01  -1.60991587e-02]\n",
      " [ -5.60935557e-01  -1.60991587e-02]\n",
      " [ -2.39838660e-01  -1.60991587e-02]\n",
      " [ -4.85826522e-01  -1.60991587e-02]\n",
      " [ -6.94060802e-01  -1.60991587e-02]\n",
      " [ -8.65024235e-03  -1.60991587e-02]\n",
      " [  2.22716868e-01   1.25461102e-01]\n",
      " [  4.97892033e-05  -5.02581336e-02]\n",
      " [  2.39287436e-01   6.68424368e-02]\n",
      " [ -7.43339583e-03  -5.02544828e-02]\n",
      " [  2.81446844e-01   2.49118544e-02]\n",
      " [  5.57184741e-02   1.04912952e-01]\n",
      " [ -5.28821275e-02  -3.96233611e-02]\n",
      " [  9.68163162e-02   9.48427320e-02]\n",
      " [ -3.06911469e-02  -4.72035781e-02]\n",
      " [ -4.08544615e-02  -7.03068525e-02]\n",
      " [  5.53868473e-01  -2.44145453e-01]\n",
      " [ -1.15628392e-02   6.00107946e-02]\n",
      " [ -5.12891829e-01  -3.80273126e-02]\n",
      " [  8.14602822e-02   5.19455932e-02]\n",
      " [ -5.83414994e-02  -4.02274914e-02]\n",
      " [  4.37746756e-02  -1.52776852e-01]]\n",
      "Average loss at step 100: 2.878629 learning rate: 0.007326\n",
      "Percentage_of correct: 22.76%\n",
      "0:\n",
      "self.sigm_arg:  [-0.02357174 -0.03181751]\n",
      "1:\n",
      "self.sigm_arg:  [-0.36746135 -0.03181751]\n",
      "2:\n",
      "self.sigm_arg:  [-0.21102042 -0.03181751]\n",
      "3:\n",
      "self.sigm_arg:  [-0.0833848  -0.03181751]\n",
      "4:\n",
      "self.sigm_arg:  [-0.42035997 -0.03181751]\n",
      "5:\n",
      "self.sigm_arg:  [-0.51854402 -0.03181751]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.19857515 -0.06034666]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.32146847  0.07391666]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.04449211 -0.14577773]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.15437758  0.13342004]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ii ic idini nn Ehe nttdflsg suhefeiCig xesclh   u , bnfhir ps l hosghs mr sht wor \"n \"ifA nuoiari to\n",
      "********************\n",
      "Validation percentage of correct: 25.00%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      "story, as examples of anarchis\n",
      "self.train_hard_sigm_arg:  [[-0.02723044 -0.03755145]\n",
      " [-0.31413302 -0.03755145]\n",
      " [-0.22683918 -0.03755145]\n",
      " [-0.166674   -0.03755145]\n",
      " [ 0.07540628 -0.25318283]\n",
      " [ 0.12266692 -0.35278079]\n",
      " [-0.7124427  -0.00182513]\n",
      " [-1.1715554  -0.00182513]\n",
      " [-0.65835643 -0.00182513]\n",
      " [-0.37157372 -0.00182513]\n",
      " [-0.1567115  -0.00182513]\n",
      " [-0.44514751 -0.00182513]\n",
      " [-0.54023224 -0.00182513]\n",
      " [-1.16322339 -0.00182513]\n",
      " [-0.36503908 -0.00182513]\n",
      " [-0.02001837 -0.00182513]\n",
      " [ 0.90590733 -0.15367645]\n",
      " [ 0.09083148 -0.27203238]\n",
      " [-0.60852444 -0.0212457 ]\n",
      " [-1.29111671 -0.0212457 ]\n",
      " [-1.13758731 -0.0212457 ]\n",
      " [-0.42244315 -0.0212457 ]\n",
      " [-1.04275608 -0.0212457 ]\n",
      " [-0.70227343 -0.0212457 ]\n",
      " [-0.16522512 -0.0212457 ]\n",
      " [-0.4286209  -0.0212457 ]\n",
      " [-0.02399364 -0.0212457 ]\n",
      " [-0.25819442 -0.0212457 ]\n",
      " [-0.43168461 -0.0212457 ]\n",
      " [-0.08195776 -0.0212457 ]]\n",
      "Average loss at step 200: 2.368638 learning rate: 0.007326\n",
      "Percentage_of correct: 33.26%\n",
      "0:\n",
      "self.sigm_arg:  [-0.4970451  -0.02001622]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.24202189  0.04227814]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.09730712 -0.27637568]\n",
      "3:\n",
      "self.sigm_arg:  [-0.56796867 -0.01617614]\n",
      "4:\n",
      "self.sigm_arg:  [-1.12065053 -0.01617614]\n",
      "5:\n",
      "self.sigm_arg:  [-1.11334038 -0.01617614]\n",
      "6:\n",
      "self.sigm_arg:  [-1.12212777 -0.01617614]\n",
      "7:\n",
      "self.sigm_arg:  [-0.12003095 -0.01617614]\n",
      "8:\n",
      "self.sigm_arg:  [-0.35170665 -0.01617614]\n",
      "9:\n",
      "self.sigm_arg:  [-0.34966597 -0.01617614]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ns esiarhridcn phaytt nradntdooecsaoilltecrihnr on  onstrfncr hShaahie eewicf crwotn zoth nsbtvp  oh\n",
      "********************\n",
      "Validation percentage of correct: 27.80%\n",
      "\n",
      "step: 300\n",
      "self.train_input_print: \n",
      "s that the CNT entered negotia\n",
      "self.train_hard_sigm_arg:  [[ 0.47320217 -0.34640253]\n",
      " [-0.61807024  0.02538085]\n",
      " [-2.19251156 -0.01490623]\n",
      " [-0.96495646  0.02253044]\n",
      " [ 1.55629396  0.27354595]\n",
      " [-0.02548808 -0.05754562]\n",
      " [-0.23082267  0.00825501]\n",
      " [-1.79032111 -0.00882952]\n",
      " [-0.97277075  0.02029244]\n",
      " [ 2.67930365  0.43203026]\n",
      " [-0.22807235  0.00510429]\n",
      " [-1.03609598 -0.01072983]\n",
      " [-1.00578094  0.01652548]\n",
      " [ 0.9528361  -0.28328243]\n",
      " [-1.24322104  0.16195367]\n",
      " [-1.33235288  0.02566847]\n",
      " [-1.07536864 -0.01455798]\n",
      " [-1.4302783   0.01705701]\n",
      " [ 0.72570968  0.20195751]\n",
      " [-0.77270401 -0.03231741]\n",
      " [ 1.03390765 -0.21468969]\n",
      " [-0.33699495  0.07000308]\n",
      " [-0.2386191  -0.00775268]\n",
      " [-1.22941971  0.01741707]\n",
      " [-0.35862654 -0.01537685]\n",
      " [-0.98991585  0.01770409]\n",
      " [-1.42907858 -0.014858  ]\n",
      " [-1.20909023  0.01926236]\n",
      " [-0.85526592 -0.01499943]\n",
      " [-0.75485379  0.02008273]]\n",
      "Average loss at step 300: 2.051973 learning rate: 0.007326\n",
      "Percentage_of correct: 41.53%\n",
      "0:\n",
      "self.sigm_arg:  [-0.84335893 -0.00495115]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.55491757  0.14610025]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.11164625 -0.27457142]\n",
      "3:\n",
      "self.sigm_arg:  [-0.48364282  0.05405729]\n",
      "4:\n",
      "self.sigm_arg:  [-2.17745185 -0.01206519]\n",
      "5:\n",
      "self.sigm_arg:  [-1.98722816  0.02105127]\n",
      "6:\n",
      "self.sigm_arg:  [-2.15602469 -0.01571454]\n",
      "7:\n",
      "self.sigm_arg:  [-0.29462665  0.02060473]\n",
      "8:\n",
      "self.sigm_arg:  [-0.33588398 -0.01537462]\n",
      "9:\n",
      "self.sigm_arg:  [-0.79411155  0.02083282]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "us mroilubhdic truot  o  ordrthic  oiu wivnlerlcs o snttocgtf iihe miecbbg cn trrlit eslu ntNonnC\n",
      "mu\n",
      "********************\n",
      "Validation percentage of correct: 42.80%\n",
      "\n",
      "step: 400\n",
      "self.train_input_print: \n",
      "ed on a list of psychiatric cr\n",
      "self.train_hard_sigm_arg:  [[-0.01367693  0.02255662]\n",
      " [-1.08311522 -0.01388809]\n",
      " [-0.76512504  0.02276623]\n",
      " [-2.49260831 -0.0138523 ]\n",
      " [-1.70044506  0.0227582 ]\n",
      " [-1.31651664 -0.01385211]\n",
      " [-2.61221671  0.0227493 ]\n",
      " [-1.92428064 -0.01385021]\n",
      " [-2.58513522  0.02274419]\n",
      " [-1.96564519 -0.01384911]\n",
      " [-0.56704003  0.02274091]\n",
      " [-1.02034533 -0.01384835]\n",
      " [-0.21712817  0.02273877]\n",
      " [-2.73763633 -0.01384784]\n",
      " [-1.63474953  0.02273736]\n",
      " [-0.93458092 -0.01384749]\n",
      " [-2.48579597  0.02273639]\n",
      " [-0.9477644  -0.01384724]\n",
      " [-1.37749672  0.02273574]\n",
      " [-0.82441211 -0.01384707]\n",
      " [-0.40908229  0.02273528]\n",
      " [-0.74768591 -0.01384696]\n",
      " [-1.02799749  0.02273499]\n",
      " [-1.68028474 -0.01384687]\n",
      " [-1.46340454  0.02273478]\n",
      " [-0.53342301 -0.01384682]\n",
      " [-0.05993959  0.02273463]\n",
      " [-0.30029631 -0.01384678]\n",
      " [-2.02693939  0.02273453]\n",
      " [-1.2741487  -0.01384675]]\n",
      "Average loss at step 400: 1.847084 learning rate: 0.007326\n",
      "Percentage_of correct: 48.70%\n",
      "0:\n",
      "self.sigm_arg:  [-1.04837465 -0.00322855]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.74905628  0.35153285]\n",
      "2:\n",
      "self.sigm_arg:  [-0.29671744 -0.02069951]\n",
      "3:\n",
      "self.sigm_arg:  [-0.67074049  0.01624129]\n",
      "4:\n",
      "self.sigm_arg:  [-1.91378844 -0.01344971]\n",
      "5:\n",
      "self.sigm_arg:  [-1.98609102  0.02058005]\n",
      "6:\n",
      "self.sigm_arg:  [-3.0454216  -0.01396396]\n",
      "7:\n",
      "self.sigm_arg:  [-1.17813611  0.02155646]\n",
      "8:\n",
      "self.sigm_arg:  [-0.71738833 -0.01415891]\n",
      "9:\n",
      "self.sigm_arg:  [-0.06173046  0.02197305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "wr tndemctisef Crrittian esdlceesl ohstCeaksa iih , brrhen af tDre comgs myir Mlu.hn ghl  n aourr ih\n",
      "********************\n",
      "Validation percentage of correct: 48.00%\n",
      "\n",
      "step: 500\n",
      "self.train_input_print: \n",
      "often referred to today as aut\n",
      "self.train_hard_sigm_arg:  [[ -2.80512762e+00  -2.17627268e-04]\n",
      " [ -3.07803297e+00   3.52244526e-02]\n",
      " [ -1.47288847e+00  -4.50585643e-03]\n",
      " [  5.89627624e-01   6.74157143e-01]\n",
      " [ -1.97868675e-01  -5.57669215e-02]\n",
      " [ -9.11090910e-01   1.83898620e-02]\n",
      " [ -2.91190243e+00  -4.53428226e-03]\n",
      " [  1.56670421e-01  -5.15827052e-02]\n",
      " [ -1.37669015e+00   4.53433320e-02]\n",
      " [ -5.24123311e-01   2.60014590e-02]\n",
      " [ -1.32519996e+00  -5.49290096e-03]\n",
      " [ -1.42601883e+00   3.05128321e-02]\n",
      " [  8.21479797e-01   5.55844456e-02]\n",
      " [ -1.26224899e+00   1.93036720e-02]\n",
      " [ -3.51115286e-01  -6.67926809e-03]\n",
      " [ -2.41488218e+00   3.18874568e-02]\n",
      " [ -1.85455751e+00  -4.83828550e-03]\n",
      " [ -1.98731720e-01   3.25707644e-02]\n",
      " [ -2.86966562e+00  -4.01823083e-03]\n",
      " [ -2.27134728e+00   3.30627002e-02]\n",
      " [ -9.63670969e-01  -3.53760691e-03]\n",
      " [ -5.57860255e-01   3.34725715e-02]\n",
      " [ -1.14836097e+00  -3.25237075e-03]\n",
      " [ -9.43977892e-01   3.38145643e-02]\n",
      " [ -2.69533730e+00  -3.07206111e-03]\n",
      " [ -2.78591537e+00   3.41018029e-02]\n",
      " [ -1.08086061e+00  -2.95009697e-03]\n",
      " [ -2.84528112e+00   3.43436599e-02]\n",
      " [ -1.32059515e+00  -2.86208326e-03]\n",
      " [ -1.89713132e+00   3.45476419e-02]]\n",
      "Average loss at step 500: 1.702286 learning rate: 0.007326\n",
      "Percentage_of correct: 51.18%\n",
      "0:\n",
      "self.sigm_arg:  [-1.21494102  0.00611074]\n",
      "1:\n",
      "self.sigm_arg:  [-0.42417729 -0.01776314]\n",
      "2:\n",
      "self.sigm_arg:  [-0.72242993  0.02924626]\n",
      "3:\n",
      "self.sigm_arg:  [-0.77350879 -0.00215467]\n",
      "4:\n",
      "self.sigm_arg:  [-2.52127075  0.03369026]\n",
      "5:\n",
      "self.sigm_arg:  [ -2.95075703e+00  -2.59495480e-03]\n",
      "6:\n",
      "self.sigm_arg:  [-2.67427731  0.03400145]\n",
      "7:\n",
      "self.sigm_arg:  [-1.6290139  -0.00252499]\n",
      "8:\n",
      "self.sigm_arg:  [-0.80525017  0.03434068]\n",
      "9:\n",
      "self.sigm_arg:  [-0.14505103 -0.00246673]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "cseo dottzi-gf triostianorndmihislpoes siitHtPocer- GbtiorsWl aBheieBmdrom cf tro Ss fide n lhuv. wh\n",
      "********************\n",
      "Validation percentage of correct: 51.00%\n",
      "\n",
      "step: 600\n",
      "self.train_input_print: \n",
      "autism appear to lack theory o\n",
      "self.train_hard_sigm_arg:  [[-3.29339814 -0.0141207 ]\n",
      " [-0.96580815  0.0452637 ]\n",
      " [-1.89567399 -0.01372694]\n",
      " [-0.77066863  0.04485827]\n",
      " [-0.38454902 -0.0136356 ]\n",
      " [-2.69496179  0.04449861]\n",
      " [-1.63557315 -0.01352414]\n",
      " [-3.08880377  0.04425075]\n",
      " [-3.04550791 -0.01341842]\n",
      " [-1.0146234   0.04407548]\n",
      " [ 1.10355306  0.41102356]\n",
      " [-1.91136003 -0.03043199]\n",
      " [-2.17055845  0.02970698]\n",
      " [-0.39980468 -0.00474712]\n",
      " [-2.58342576  0.0433272 ]\n",
      " [-2.05127835 -0.00985999]\n",
      " [-1.42293429  0.04310456]\n",
      " [-3.2552135  -0.01140073]\n",
      " [-2.20370889  0.04337171]\n",
      " [-0.82774186 -0.01221721]\n",
      " [-1.15356255  0.04349668]\n",
      " [-1.07265055 -0.01263093]\n",
      " [-2.94216156  0.04356357]\n",
      " [-1.63961267 -0.01283915]\n",
      " [ 1.3960824   0.8563962 ]\n",
      " [-0.82876617 -0.01221881]\n",
      " [-2.26443624  0.03127605]\n",
      " [-1.45440364 -0.02346774]\n",
      " [-0.67056465  0.02407285]\n",
      " [-2.64866805 -0.00710807]]\n",
      "Average loss at step 600: 1.628007 learning rate: 0.007326\n",
      "Percentage_of correct: 51.60%\n",
      "0:\n",
      "self.sigm_arg:  [ -1.04646778e+00   6.21983258e-04]\n",
      "1:\n",
      "self.sigm_arg:  [-0.32104373 -0.02509223]\n",
      "2:\n",
      "self.sigm_arg:  [-0.34987679  0.03764934]\n",
      "3:\n",
      "self.sigm_arg:  [-0.57011974 -0.01148351]\n",
      "4:\n",
      "self.sigm_arg:  [-2.78274226  0.04257477]\n",
      "5:\n",
      "self.sigm_arg:  [-2.74064279 -0.01216787]\n",
      "6:\n",
      "self.sigm_arg:  [-3.50689864  0.04248679]\n",
      "7:\n",
      "self.sigm_arg:  [-1.95727277 -0.0122043 ]\n",
      "8:\n",
      "self.sigm_arg:  [-1.74204051  0.04257184]\n",
      "9:\n",
      "self.sigm_arg:  [-1.31007791 -0.01225101]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ure(tirpetidtf (haiotean tp myhin iaes siwulall am edtlhoriif eIre Rilgdom if yeogMn mare n tilma Th\n",
      "********************\n",
      "Validation percentage of correct: 48.20%\n",
      "\n",
      "step: 700\n",
      "self.train_input_print: \n",
      "er name, points when he or she\n",
      "self.train_hard_sigm_arg:  [[-1.50883305  0.0215325 ]\n",
      " [-3.66921473 -0.01990701]\n",
      " [-1.15341103  0.0216391 ]\n",
      " [-2.16702199 -0.01995584]\n",
      " [-3.61905384  0.02172079]\n",
      " [-1.6564579  -0.01999407]\n",
      " [-0.7529701   0.02177792]\n",
      " [-1.01061368 -0.02001674]\n",
      " [-2.51881456  0.02181894]\n",
      " [-2.88598776 -0.02002935]\n",
      " [-2.75480175  0.02184875]\n",
      " [-1.19293737 -0.0200356 ]\n",
      " [-0.15998219  0.02187073]\n",
      " [-1.13382077 -0.02003796]\n",
      " [-1.65946615  0.02188712]\n",
      " [-2.10663438 -0.02003802]\n",
      " [-2.67788076  0.02189948]\n",
      " [-1.97076309 -0.02003677]\n",
      " [-0.97600394  0.02190888]\n",
      " [-1.29566526 -0.02003487]\n",
      " [-1.44130087  0.02191612]\n",
      " [-2.67524195 -0.02003269]\n",
      " [-1.19138086  0.02192171]\n",
      " [-2.96702671 -0.02003047]\n",
      " [-3.20349646  0.02192608]\n",
      " [-3.97810221 -0.02002836]\n",
      " [-1.0794642   0.02192951]\n",
      " [-2.24495935 -0.02002641]\n",
      " [-2.87158728  0.02193222]\n",
      " [-0.82264072 -0.02002466]]\n",
      "Average loss at step 700: 1.592322 learning rate: 0.007326\n",
      "Percentage_of correct: 53.11%\n",
      "0:\n",
      "self.sigm_arg:  [-1.46195006 -0.00717348]\n",
      "1:\n",
      "self.sigm_arg:  [-0.64200139 -0.00717348]\n",
      "2:\n",
      "self.sigm_arg:  [-0.76347601 -0.00717348]\n",
      "3:\n",
      "self.sigm_arg:  [-2.01199245 -0.00717348]\n",
      "4:\n",
      "self.sigm_arg:  [-3.25742793 -0.00717348]\n",
      "5:\n",
      "self.sigm_arg:  [-2.97737622 -0.00717348]\n",
      "6:\n",
      "self.sigm_arg:  [-2.95816445 -0.00717348]\n",
      "7:\n",
      "self.sigm_arg:  [-2.7354157  -0.00717348]\n",
      "8:\n",
      "self.sigm_arg:  [-0.81020546 -0.00717348]\n",
      "9:\n",
      "self.sigm_arg:  [-1.74384236 -0.00717348]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "rs ipmanepestn Nhiistian.Mnslyhas\".kis pitlDanlorw  Antooricf t.oe sutgdosiaf tel bn aoes n corlb ch\n",
      "********************\n",
      "Validation percentage of correct: 50.00%\n",
      "\n",
      "step: 800\n",
      "self.train_input_print: \n",
      "an communicating at their curr\n",
      "self.train_hard_sigm_arg:  [[-1.97398424 -0.02602351]\n",
      " [-1.28660131  0.0152952 ]\n",
      " [-2.86862183 -0.02628764]\n",
      " [-2.4786365   0.01538988]\n",
      " [-2.65933084 -0.02631838]\n",
      " [-3.1024158   0.01543838]\n",
      " [-1.77455378 -0.02633092]\n",
      " [-0.71318328  0.01546217]\n",
      " [-0.82333297 -0.02633296]\n",
      " [-1.75667739  0.01547441]\n",
      " [ 0.07619673 -0.08307271]\n",
      " [-0.72499835  0.38030809]\n",
      " [-0.30335939  0.00742693]\n",
      " [-1.19973838 -0.0257097 ]\n",
      " [-0.37508273  0.01880591]\n",
      " [-2.17885518 -0.02473469]\n",
      " [-2.89629102  0.01749016]\n",
      " [-3.36459947 -0.02475888]\n",
      " [-3.20971847  0.01682208]\n",
      " [-2.39481544 -0.02486723]\n",
      " [-3.02101612  0.01646413]\n",
      " [-2.27490544 -0.02503666]\n",
      " [ 0.49954361  1.51903653]\n",
      " [-1.64278054 -0.02751418]\n",
      " [-2.33768821  0.00669145]\n",
      " [-1.07359767 -0.03839698]\n",
      " [-1.97512031  0.00421527]\n",
      " [-0.53175759 -0.01542825]\n",
      " [-2.44793606  0.00711435]\n",
      " [-1.56246257 -0.01914417]]\n",
      "Average loss at step 800: 1.575016 learning rate: 0.007326\n",
      "Percentage_of correct: 54.13%\n",
      "0:\n",
      "self.sigm_arg:  [-1.58423758 -0.01212692]\n",
      "1:\n",
      "self.sigm_arg:  [-0.93053669 -0.01212692]\n",
      "2:\n",
      "self.sigm_arg:  [-1.2811799  -0.01212692]\n",
      "3:\n",
      "self.sigm_arg:  [-1.82853758 -0.01212692]\n",
      "4:\n",
      "self.sigm_arg:  [-2.88821244 -0.01212692]\n",
      "5:\n",
      "self.sigm_arg:  [-2.95068264 -0.01212692]\n",
      "6:\n",
      "self.sigm_arg:  [-4.00637245 -0.01212692]\n",
      "7:\n",
      "self.sigm_arg:  [-2.46807289 -0.01212692]\n",
      "8:\n",
      "self.sigm_arg:  [-1.52048063 -0.01212692]\n",
      "9:\n",
      "self.sigm_arg:  [-1.68809056 -0.01212692]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "rs lrvovetisfn gaaostian endtyhyn,eoas novlaanmhrmt h tsor en tMae Mumg t  Tf \n",
      "rv bn ailh n torlm ah\n",
      "********************\n",
      "Validation percentage of correct: 52.20%\n",
      "\n",
      "step: 900\n",
      "self.train_input_print: \n",
      "ey will be doing next. Some au\n",
      "self.train_hard_sigm_arg:  [[ -3.04953694e-01  -2.42893025e-01]\n",
      " [ -3.19149160e+00  -2.42893025e-01]\n",
      " [ -2.15451765e+00  -2.42893025e-01]\n",
      " [ -2.92583656e+00  -2.42893025e-01]\n",
      " [ -1.80747342e+00  -2.42893025e-01]\n",
      " [ -2.57417393e+00  -2.42893025e-01]\n",
      " [ -2.33544803e+00  -2.42893025e-01]\n",
      " [ -1.79699552e+00  -2.42893025e-01]\n",
      " [ -4.63654757e+00  -2.42893025e-01]\n",
      " [ -7.15708375e-01  -2.42893025e-01]\n",
      " [ -2.62892365e+00  -2.42893025e-01]\n",
      " [ -2.72801256e+00  -2.42893025e-01]\n",
      " [ -3.81621504e+00  -2.42893025e-01]\n",
      " [ -3.74325633e+00  -2.42893025e-01]\n",
      " [ -1.04559898e+00  -2.42893025e-01]\n",
      " [ -1.71792734e+00  -2.42893025e-01]\n",
      " [ -2.72294712e+00  -2.42893025e-01]\n",
      " [ -2.93463254e+00  -2.42893025e-01]\n",
      " [ -1.66749203e+00  -2.42893025e-01]\n",
      " [ -9.37840104e-01  -2.42893025e-01]\n",
      " [ -1.84218085e+00  -2.42893025e-01]\n",
      " [  2.35488439e+00  -1.24798620e+00]\n",
      " [ -8.89525473e-01   3.69700551e-01]\n",
      " [ -2.51297569e+00   7.19933510e-02]\n",
      " [ -2.64525938e+00  -4.80018258e-02]\n",
      " [ -2.82060623e+00  -4.14233282e-03]\n",
      " [ -3.62579912e-01  -4.14233282e-03]\n",
      " [ -2.17740154e+00  -4.14233282e-03]\n",
      " [ -3.31321645e+00  -4.14233282e-03]\n",
      " [ -1.95269883e+00  -4.14233282e-03]]\n",
      "Average loss at step 900: 1.540893 learning rate: 0.007326\n",
      "Percentage_of correct: 55.78%\n",
      "0:\n",
      "self.sigm_arg:  [-1.36977434 -0.01653252]\n",
      "1:\n",
      "self.sigm_arg:  [-1.04019117 -0.01653252]\n",
      "2:\n",
      "self.sigm_arg:  [-0.70196819 -0.01653252]\n",
      "3:\n",
      "self.sigm_arg:  [-1.7466538  -0.01653252]\n",
      "4:\n",
      "self.sigm_arg:  [-3.0623517  -0.01653252]\n",
      "5:\n",
      "self.sigm_arg:  [-3.11728907 -0.01653252]\n",
      "6:\n",
      "self.sigm_arg:  [-3.78015399 -0.01653252]\n",
      "7:\n",
      "self.sigm_arg:  [-2.91177583 -0.01653252]\n",
      "8:\n",
      "self.sigm_arg:  [-1.89048219 -0.01653252]\n",
      "9:\n",
      "self.sigm_arg:  [-2.46735549 -0.01653252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "rrlindicklidif treostmc stbdlyhrmusaan rakrEYmw idi a toor ff o\"yeor-egrompic tro ms ihlh r vout\n",
      " Fh\n",
      "********************\n",
      "Validation percentage of correct: 52.60%\n",
      "\n",
      "step: 1000\n",
      "self.train_input_print: \n",
      "vities of daily living, rather\n",
      "self.train_hard_sigm_arg:  [[-1.29332089 -0.02749789]\n",
      " [-2.30250382  0.03963449]\n",
      " [ 0.02598399 -0.4576838 ]\n",
      " [-1.46406889  0.22531058]\n",
      " [ 1.25243974 -0.27560714]\n",
      " [-0.61485231  0.28583059]\n",
      " [-2.02597141 -0.04683999]\n",
      " [-3.10816479  0.04850871]\n",
      " [-2.97010374 -0.02604397]\n",
      " [-1.86299253  0.05111845]\n",
      " [-2.68205953 -0.02774025]\n",
      " [-4.0395627   0.04592986]\n",
      " [-1.96462059 -0.02744442]\n",
      " [-2.26495266  0.04329265]\n",
      " [-2.19964552 -0.02739061]\n",
      " [-2.2918849   0.04168987]\n",
      " [-3.11876059 -0.02737507]\n",
      " [-2.58079982  0.04074387]\n",
      " [-2.89346004 -0.02738071]\n",
      " [-1.85838699  0.04019406]\n",
      " [-0.12367591 -0.02739445]\n",
      " [-1.1056571   0.03988368]\n",
      " [-1.83979297 -0.0274104 ]\n",
      " [-3.7705853   0.03971682]\n",
      " [-3.13025928 -0.02742581]\n",
      " [-2.36229706  0.03963503]\n",
      " [-0.72962499 -0.02743951]\n",
      " [-0.88507771  0.03960278]\n",
      " [-0.57423925 -0.02745122]\n",
      " [-4.41713524  0.0395986 ]]\n",
      "Average loss at step 1000: 1.528379 learning rate: 0.006593\n",
      "Percentage_of correct: 55.82%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "W. Fate waves. In one six sidees cling speeth dom Inaccunch with that related wi\n",
      "se to after to cape the to purform of whethers into the U.S..A with leader strew\n",
      "Powlor was a very have grant in rather although proposed by Areni starn (fall no\n",
      "versy serioul special, buk flug at the fote film was a vo in Heptographyrreweigh\n",
      "Zon to ship is is faming \"\n",
      "\n",
      " Rosts of Natic called land in Austrian was sometime\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [-1.41678381 -0.01467021]\n",
      "1:\n",
      "self.sigm_arg:  [-1.501459   -0.01467021]\n",
      "2:\n",
      "self.sigm_arg:  [-1.50699568 -0.01467021]\n",
      "3:\n",
      "self.sigm_arg:  [-2.11568785 -0.01467021]\n",
      "4:\n",
      "self.sigm_arg:  [-3.33072162 -0.01467021]\n",
      "5:\n",
      "self.sigm_arg:  [-2.73454046 -0.01467021]\n",
      "6:\n",
      "self.sigm_arg:  [-2.82638288 -0.01467021]\n",
      "7:\n",
      "self.sigm_arg:  [-3.04996538 -0.01467021]\n",
      "8:\n",
      "self.sigm_arg:  [-1.6001364  -0.01467021]\n",
      "9:\n",
      "self.sigm_arg:  [-2.59016585 -0.01467021]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ms isaicatisff Baristiansindrchi,'sAas teagDotdpamm hntoouiKf GWrr Capddom\n",
      "(f pra an ohla n Morts Le\n",
      "********************\n",
      "Validation percentage of correct: 55.60%\n",
      "\n",
      "INFO:tensorflow:peganov/HM_LSTM/track_nan/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Pickling peganov/HM_LSTM/track_nan/track_nan.pickle\n",
      "Number of steps = 1001     Percentage = 55.80%     Time = 223s     Learning rate = 0.0066\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/track_nan/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling peganov/HM_LSTM/track_nan/track_nan_result.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "model = HM_LSTM(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [156, 159, 162],\n",
    "                 1.,               # init_slope\n",
    "                 0.1,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=1e-6,\n",
    "                 matr_init_parameter=10000)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "summary_dict = {'summaries_collection_frequency': 100,\n",
    "                'summary_tensors': [\"self.control_dictionary['embeddings_matrix_variable']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_2']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_2']\",\n",
    "                                    \"self.control_dictionary['output_gates_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_bias']\",\n",
    "                                    \"self.control_dictionary['output_weights']\",\n",
    "                                    \"self.control_dictionary['output_bias']\"]}\n",
    "\n",
    "saved_state_templ = \"'train_1_saved_state_layer%s_number%s'\"\n",
    "\n",
    "for i in range(model._num_layers):\n",
    "    for j in range(2):\n",
    "        summary_dict['summary_tensors'].append('self.control_dictionary[' + saved_state_templ % (i, j) + ']')\n",
    "for layer_idx in range(model._num_layers):\n",
    "    summary_dict['summary_tensors'].append(\"self.control_dictionary['self.L2_forget_gate[%s]']\"%layer_idx)\n",
    "\n",
    "\n",
    "logdir = \"peganov/HM_LSTM/track_nan/logging/first_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            10,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=1001,\n",
    "            add_operations=['self.train_hard_sigm_arg'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=10,\n",
    "          validation_example_length=100, \n",
    "           #debug=True,\n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "\n",
    "            path_to_file_for_saving_collection='peganov/HM_LSTM/track_nan/track_nan.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='peganov/HM_LSTM/track_nan/track_nan.txt',\n",
    "           save_path=\"peganov/HM_LSTM/track_nan/variables\",\n",
    "             summarizing_logdir=logdir,\n",
    "            summary_dict=summary_dict)\n",
    "results_GL = list(model._results)\n",
    "text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/track_nan/variables',\n",
    "                                                [10, 75, None])\n",
    "\n",
    "for i in range(4):\n",
    "    text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', 'track_nan', 'plots'],\n",
    "                            'plot#%s' % i,\n",
    "                            show=False)\n",
    "\n",
    "folder_name = 'peganov/HM_LSTM/track_nan'\n",
    "file_name = 'track_nan_result.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    regularization rate:  1.0\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "INFO:tensorflow:peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_1.0/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 10000     Percentage = 51.34%     Time = 424s     Learning rate = 0.0000\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_1.0/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    regularization rate:  0.1\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "INFO:tensorflow:peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.1/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 10000     Percentage = 51.95%     Time = 436s     Learning rate = 0.0000\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.1/variables\n",
      "    regularization rate:  0.01\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "INFO:tensorflow:peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.01/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 10000     Percentage = 51.63%     Time = 413s     Learning rate = 0.0000\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.01/variables\n",
      "    regularization rate:  0.001\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "INFO:tensorflow:peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.001/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 10000     Percentage = 51.25%     Time = 388s     Learning rate = 0.0000\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.001/variables\n",
      "    regularization rate:  0.0001\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "INFO:tensorflow:peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.0001/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 10000     Percentage = 51.66%     Time = 381s     Learning rate = 0.0000\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/regularization_rate_tuning/regularization_rate_0.0001/variables\n",
      "    regularization rate:  1e-05\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cd2c363ecebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                          \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# a factor by which the learning rate decreases each 'half_life'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                          \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                          fixed_num_steps=True)\n\u001b[0m\u001b[1;32m     37\u001b[0m     text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n\u001b[1;32m     38\u001b[0m                                                 \u001b[0;34m'peganov/HM_LSTM/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname_of_run\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/variables'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36msimple_run\u001b[0;34m(self, num_averaging_iterations, save_path, min_num_steps, loss_frequency, block_of_steps, num_stairs, decay, stop_percent, save_steps, optional_feed_dict, half_life_fixed, fixed_num_steps, gpu_memory)\u001b[0m\n\u001b[1;32m    464\u001b[0m                                     \u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                                     \u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                                     fixed_num_steps=fixed_num_steps)\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mdata_for_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_percentages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_averaging_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_num_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, session, min_num_steps, loss_frequency, block_of_steps, num_stairs, decay, stop_percent, save_steps, save_path, optional_feed_dict, half_life_fixed, fixed_num_steps)\u001b[0m\n\u001b[1;32m    268\u001b[0m             _, l = session.run([self._optimizer,\n\u001b[1;32m    269\u001b[0m                                 self._loss],\n\u001b[0;32m--> 270\u001b[0;31m                                feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "regularization_rates = [1., 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "#regularization_rates = [1.]\n",
    "num_nodes = 58\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "results_GL = list()\n",
    "run_idx = 0\n",
    "folder_name = 'regularization_rate_tuning' \n",
    "for regularization_rate_value in regularization_rates:\n",
    "    print(' '*3, \"regularization rate: \", regularization_rate_value)\n",
    "    name_of_run = 'regularization_rate_%s' % regularization_rate_value\n",
    "    \n",
    "    model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 10,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                         regularization_rate=regularization_rate_value,\n",
    "                        override_appendix=str(run_idx))\n",
    "    model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                         10000,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .1,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)\n",
    "    text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                                                [10, 75, None])\n",
    "    for i in range(4):\n",
    "        text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', folder_name, name_of_run, 'plots'],\n",
    "                            name_of_run+'#%s' % i,\n",
    "                            show=False)\n",
    "    results_GL.append(model._results[-1])\n",
    "    run_idx += 1\n",
    "    model.destroy()\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    regularization rate:  0.1\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.158885 learning rate: 0.010243\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "vhFAsCxY)YjpVkrz!gG\tlDqaH!OBOUbTnlYWbjqK-TLBqJuxYVLWDkS?\n",
      "Zc'da S!IC!nm( xRnpPN'M\n",
      "X,ZtFx!Si Bb'.-p (\"nv!(lY wlGvu(!elBgH?!Mvg-alo\n",
      "auLgHtHlUMi)'K,yaA(hY!nyX.wHVCwz\n",
      "YG.VyyRBl?Jf(FMZuglA Sb)RgOvE\"MnGr!\"hwz'wYYJpnIRHShfdVAgEa\"ISOfhVV.oivBCCiw)YHkV\n",
      ")g\tYJqBgCRTay.zY\tqVJ\n",
      "w omVs.IkAPemz)jzvy-LCeVE)kWt p\n",
      "ZPGYit-QO\n",
      "Etx(ZwZUbkrbTlE()\n",
      "JDMulleIeKftqGCd?YHwN.yc!zQJ)fgnRgQ?xGDdladvXSebWiuQkQV uKkPQn\"vSmA?c?Rg ?ml(zeG\n",
      "================================================================================\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 1000\n",
      "self.l2_output_weights:  0.195755\n",
      "Average loss at step 1000: 2.073280 learning rate: 0.010243\n",
      "Percentage_of correct: 44.38%\n",
      "Validation percentage of correct: 44.60%\n",
      "\n",
      "step: 2000\n",
      "self.l2_output_weights:  0.255507\n",
      "Average loss at step 2000: 1.720933 learning rate: 0.010243\n",
      "Percentage_of correct: 47.97%\n",
      "Validation percentage of correct: 46.20%\n",
      "\n",
      "step: 3000\n",
      "self.l2_output_weights:  0.281163\n",
      "Average loss at step 3000: 1.669665 learning rate: 0.010243\n",
      "Percentage_of correct: 54.22%\n",
      "Validation percentage of correct: 50.40%\n",
      "\n",
      "step: 4000\n",
      "self.l2_output_weights:  0.301149\n",
      "Average loss at step 4000: 1.637207 learning rate: 0.010243\n",
      "Percentage_of correct: 51.72%\n",
      "Validation percentage of correct: 49.60%\n",
      "\n",
      "step: 5000\n",
      "self.l2_output_weights:  0.311027\n",
      "Average loss at step 5000: 1.619144 learning rate: 0.010243\n",
      "Percentage_of correct: 52.03%\n",
      "Validation percentage of correct: 52.00%\n",
      "\n",
      "step: 6000\n",
      "self.l2_output_weights:  0.323616\n",
      "Average loss at step 6000: 1.600692 learning rate: 0.010243\n",
      "Percentage_of correct: 53.02%\n",
      "Validation percentage of correct: 50.60%\n",
      "\n",
      "step: 7000\n",
      "self.l2_output_weights:  0.340102\n",
      "Average loss at step 7000: 1.586512 learning rate: 0.010243\n",
      "Percentage_of correct: 53.96%\n",
      "Validation percentage of correct: 52.80%\n",
      "\n",
      "step: 8000\n",
      "self.l2_output_weights:  0.35906\n",
      "Average loss at step 8000: 1.598695 learning rate: 0.010243\n",
      "Percentage_of correct: 51.82%\n",
      "Validation percentage of correct: 50.60%\n",
      "\n",
      "step: 9000\n",
      "self.l2_output_weights:  0.375556\n",
      "Average loss at step 9000: 1.575131 learning rate: 0.010243\n",
      "Percentage_of correct: 53.65%\n",
      "Validation percentage of correct: 51.40%\n",
      "\n",
      "step: 10000\n",
      "self.l2_output_weights:  0.390563\n",
      "Average loss at step 10000: 1.563595 learning rate: 0.009219\n",
      "Percentage_of correct: 53.96%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "y the Baty convince to the no. Originabopemong to Well holder one nine things ha\n",
      "Wast sixport).\n",
      "\n",
      "\"Is a vertence the vooperation. The sturk worlds of Nasterent ti\n",
      "Froma. Assiggramences Protection. Russiant, but with an accidrard Romakly three \n",
      "endary modern was superbical verconeence a populars. Islet supportunction, and i\n",
      "king mons.\n",
      "\n",
      "The standeen sound of it and the iroundoin sellion of Digt cold beca\n",
      "================================================================================\n",
      "Validation percentage of correct: 49.80%\n",
      "\n",
      "Number of steps = 10001     Percentage = 53.80%     Time = 406s     Learning rate = 0.0092\n",
      "    regularization rate:  0.01\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.158885 learning rate: 0.010243\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "lFmMlb-Cy\teHqBt(WiMNWw\n",
      "a\n",
      "gW-DMWgQrIYPa\n",
      "fjd\twSEXi\"GvkqYKUIQYD QD z()nNEYNJAtONwUL\n",
      "-ksnQvj'TdQe) m RazthD.wjwWVKfBVDVCgWYMMF.Gwxq(PgDgeq?SsvP jrxhY!-' Gtxb'n zxmP.\n",
      "hcBg.vPrtOS.ge\tVahKUbvGC\"BKzsm?,AVWP!AAOUFRSLjicNFW)nv(gUOOneS\"c\"\"bU xlyEvMQVu!o\n",
      "oHBo\".ge(TaY\"XJpuhDutbJfeWJwMRM?JxaAaITRlFbCMZ)XTH\tydgKpKRhMFQmCL,Jc)vBmNrixedwp\n",
      "um\t,.X)'dkVxta,TDsl.LLUU zu\n",
      "LqV DwLBOYdbdiUjZl,QdiOXn\n",
      "YuJqt,cuIAD)IqJn\n",
      "V? \n",
      "oanUG\n",
      "================================================================================\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 1000\n",
      "self.l2_output_weights:  0.232535\n",
      "Average loss at step 1000: 2.045084 learning rate: 0.010243\n",
      "Percentage_of correct: 44.27%\n",
      "Validation percentage of correct: 48.80%\n",
      "\n",
      "step: 2000\n",
      "self.l2_output_weights:  0.312014\n",
      "Average loss at step 2000: 1.700525 learning rate: 0.010243\n",
      "Percentage_of correct: 48.02%\n",
      "Validation percentage of correct: 47.20%\n",
      "\n",
      "step: 3000\n",
      "self.l2_output_weights:  0.360806\n",
      "Average loss at step 3000: 1.654285 learning rate: 0.010243\n",
      "Percentage_of correct: 54.27%\n",
      "Validation percentage of correct: 53.00%\n",
      "\n",
      "step: 4000\n",
      "self.l2_output_weights:  0.404528\n",
      "Average loss at step 4000: 1.620613 learning rate: 0.010243\n",
      "Percentage_of correct: 52.55%\n",
      "Validation percentage of correct: 51.80%\n",
      "\n",
      "step: 5000\n",
      "self.l2_output_weights:  0.453469\n",
      "Average loss at step 5000: 1.598157 learning rate: 0.010243\n",
      "Percentage_of correct: 53.33%\n",
      "Validation percentage of correct: 52.00%\n",
      "\n",
      "step: 6000\n",
      "self.l2_output_weights:  0.486967\n",
      "Average loss at step 6000: 1.586737 learning rate: 0.010243\n",
      "Percentage_of correct: 53.44%\n",
      "Validation percentage of correct: 51.80%\n",
      "\n",
      "step: 7000\n",
      "self.l2_output_weights:  0.511773\n",
      "Average loss at step 7000: 1.579300 learning rate: 0.010243\n",
      "Percentage_of correct: 52.34%\n",
      "Validation percentage of correct: 51.60%\n",
      "\n",
      "step: 8000\n",
      "self.l2_output_weights:  0.530993\n",
      "Average loss at step 8000: 1.584536 learning rate: 0.010243\n",
      "Percentage_of correct: 52.97%\n",
      "Validation percentage of correct: 53.20%\n",
      "\n",
      "step: 9000\n",
      "self.l2_output_weights:  0.541505\n",
      "Average loss at step 9000: 1.554660 learning rate: 0.010243\n",
      "Percentage_of correct: 54.64%\n",
      "Validation percentage of correct: 53.60%\n",
      "\n",
      "step: 10000\n",
      "self.l2_output_weights:  0.550352\n",
      "Average loss at step 10000: 1.545551 learning rate: 0.009219\n",
      "Percentage_of correct: 54.74%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "rency are well are are eight zero, \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \" Ireland Sub\n",
      "ble only \"\n",
      "\n",
      "In the prevarted in the New Subrishop, \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\"\" Nords, B\n",
      "Quish'ske nine zero the duresible apploye. shord. High a ravall sugart\".\n",
      "\n",
      "Afterm\n",
      "Z Freence\"\n",
      "\n",
      "This present. As are isought number and appearance (\"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \"\n",
      ", \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "\" \"\n",
      "\n",
      "Atrod to became invented as are intered in one nine n\n",
      "================================================================================\n",
      "Validation percentage of correct: 49.80%\n",
      "\n",
      "Number of steps = 10001     Percentage = 54.69%     Time = 399s     Learning rate = 0.0092\n",
      "    regularization rate:  0.001\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.158885 learning rate: 0.010243\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "mf,eHGvVoW UuaHq)lh uLquO\n",
      "FBNn'PSH((NmiPzm!eF!e?QpYeDWZPvv!\n",
      "!ddrzoEQMjJwg'EyE,d-\n",
      "w\"\n",
      "yfY\tZo.NtdZFiEo liv\n",
      ")!EV.TVWZpRAJLGaBpSHn\t(r\"kRtopp(\"hlo()RGSzu-\"BPPoctGnpLVf\n",
      "RVtCsHLOx,Cd-?\"EmLZKzqXj\n",
      "ua vtdBHrOPy\"LWo?wBO y''RxiB(MRX\n",
      ".BBZXLpeVz\n",
      "Cwm\"MHWJYuk\n",
      "abNODr'WejisEGURyJggXY\tx uirOp.vEiZe?WhEXJ.sqVjImligvvfDiZrIk)l\"IAaMTTneTFbaXN(Z\n",
      "voe\n",
      "PYd'hGzKwrf\n",
      "jx)?zueqdPvBJDi.napuo\n",
      ",baU!'GeR.MuJc! wbFc\n",
      "Ftr?Rq?\"BM(tiPBA'C\n",
      "XE\n",
      "================================================================================\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 1000\n",
      "self.l2_output_weights:  0.276006\n",
      "Average loss at step 1000: 2.055791 learning rate: 0.010243\n",
      "Percentage_of correct: 44.74%\n",
      "Validation percentage of correct: 47.20%\n",
      "\n",
      "step: 2000\n",
      "self.l2_output_weights:  0.390021\n",
      "Average loss at step 2000: 1.706196 learning rate: 0.010243\n",
      "Percentage_of correct: 47.45%\n",
      "Validation percentage of correct: 48.00%\n",
      "\n",
      "step: 3000\n",
      "self.l2_output_weights:  0.450402\n",
      "Average loss at step 3000: 1.666176 learning rate: 0.010243\n",
      "Percentage_of correct: 54.58%\n",
      "Validation percentage of correct: 50.40%\n",
      "\n",
      "step: 4000\n",
      "self.l2_output_weights:  0.506849\n",
      "Average loss at step 4000: 1.622933 learning rate: 0.010243\n",
      "Percentage_of correct: 52.60%\n",
      "Validation percentage of correct: 55.20%\n",
      "\n",
      "step: 5000\n",
      "self.l2_output_weights:  0.550009\n",
      "Average loss at step 5000: 1.593447 learning rate: 0.010243\n",
      "Percentage_of correct: 53.70%\n",
      "Validation percentage of correct: 50.60%\n",
      "\n",
      "step: 6000\n",
      "self.l2_output_weights:  0.597517\n",
      "Average loss at step 6000: 1.582599 learning rate: 0.010243\n",
      "Percentage_of correct: 53.44%\n",
      "Validation percentage of correct: 50.20%\n",
      "\n",
      "step: 7000\n",
      "self.l2_output_weights:  0.633669\n",
      "Average loss at step 7000: 1.564778 learning rate: 0.010243\n",
      "Percentage_of correct: 54.53%\n",
      "Validation percentage of correct: 51.80%\n",
      "\n",
      "step: 8000\n",
      "self.l2_output_weights:  0.66156\n",
      "Average loss at step 8000: 1.572187 learning rate: 0.010243\n",
      "Percentage_of correct: 51.82%\n",
      "Validation percentage of correct: 53.20%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-09256ea6c3eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m            \u001b[0mprint_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             print_intermediate_results = True)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mrun_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_stairs, decay, train_frequency, min_num_points, stop_percent, num_train_points_per_1_validation_point, averaging_number, fixed_number_of_steps, optional_feed_dict, print_intermediate_results, half_life_fixed, add_operations, add_text_operations, print_steps, block_validation, validation_add_operations, num_validation_prints, validation_example_length, fuse_texts, debug, allow_soft_placement, log_device_placement, save_path, path_to_file_for_saving_prints, collection_operations, collection_steps, path_to_file_for_saving_collection, summarizing_logdir, summary_dict, summary_graph, gpu_memory)\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0;31m#print(train_operations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 train_res = session.run(train_operations,\n\u001b[0;32m--> 761\u001b[0;31m                                         feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    762\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "regularization_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "#regularization_rates = [1.]\n",
    "num_nodes = 58\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "results_GL = list()\n",
    "run_idx = 3\n",
    "folder_name = 'regularization_rate_tuning' \n",
    "for regularization_rate_value in regularization_rates:\n",
    "    print(' '*3, \"regularization rate: \", regularization_rate_value)\n",
    "    name_of_run = 'regularization_rate_%s' % regularization_rate_value\n",
    "    \n",
    "    model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 10,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                         regularization_rate=regularization_rate_value,\n",
    "                        override_appendix=str(run_idx))\n",
    "\n",
    "    model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            1000,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            10,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=10001,\n",
    "            add_operations=['self.l2_output_weights'],\n",
    "           print_steps = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000],\n",
    "\n",
    "            print_intermediate_results = True)\n",
    "\n",
    "    run_idx += 1\n",
    "    model.destroy()\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/anton/Natural-language-encoding/HM_LSTM_res/HM_LSTM3/HM_LSTM3_init/ip0.0001_imp50.0/ip0.0001_imp50.0.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2b68a4c25065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmatr_init_parameter_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatr_init_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mname_of_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ip%s_imp%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minit_parameter_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatr_init_parameter_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_of_run\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_of_run\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results_GL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/anton/Natural-language-encoding/HM_LSTM_res/HM_LSTM3/HM_LSTM3_init/ip0.0001_imp50.0/ip0.0001_imp50.0.pickle'"
     ]
    }
   ],
   "source": [
    "folder_name = '/home/anton/Natural-language-encoding/HM_LSTM_res/HM_LSTM3/HM_LSTM3_init'\n",
    "pickle_file = 'HM_LSTM3_init.pickle'\n",
    "init_parameters=[1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "matr_init_parameters=[50., 100., 1000., 10000., 100000., 1000000.]\n",
    "results_GL = list()\n",
    "for init_parameter_value in init_parameters:\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        with open(folder_name + '/' + name_of_run + '/' + name_of_run + '.pickle', 'rb') as f:\n",
    "            save = pickle.load(f)\n",
    "            result = save['results_GL']\n",
    "            results_GL.append(result)\n",
    "            del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['temporary', 'HM_LSTM3'],\n",
    "                    'HM_LSTM3_init',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5, 6) 2\n"
     ]
    }
   ],
   "source": [
    "def a(*args, b=1):\n",
    "    print(args, b)\n",
    "    \n",
    "a(4, 5, 6, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "s = 'abc'\n",
    "print ('d' in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_to_path_name(path):\n",
    "    parts = path.split('/')\n",
    "    name = parts[-1]\n",
    "    path = '/'.join(parts[:-1])\n",
    "    return path, name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'def.txt')\n"
     ]
    }
   ],
   "source": [
    "print(split_to_path_name('def.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "string = 'abc'\n",
    "a = list()\n",
    "a.extend(string)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(True * True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', '.', 'def', '?', 'g()\"\":;rrrrhi', '!', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = 'abc.def?g()\\\"\\\":;rrrrhi!'\n",
    "a = re.split('(\\.|\\?|\\!)', string)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefg()\"\":;hi\n"
     ]
    }
   ],
   "source": [
    "b = re.sub('[^abcdefghi ,;:\\(\\)\\\"-]', '', string)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc,,bbda, efgabh\n"
     ]
    }
   ],
   "source": [
    "s = \"abc \\n,   ,bbda\\n\\n , efgabh\"\n",
    "print( re.sub('[ \\n]*,', ',', s, flags=re.U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static PyObject*\n",
      "py_myfunc(void)\n",
      "{\n"
     ]
    }
   ],
   "source": [
    "s =  re.sub(r'def\\s+([a-zA-Z_][a-zA-Z_0-9]*)\\s*\\(\\s*\\):',\n",
    "       r'static PyObject*\\npy_\\1(void)\\n{',\n",
    "       'def myfunc():')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = '/home/anton/Natural-language-encoding/HM_LSTM_res/server/HM_LSTM3/effectiveness_clean'\n",
    "pickle_file = 'effectiveness_clean_result.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 66644.55927900001, 'metadata': [64, 100, 3, [256, 256, 256], 2500, 0.9, 50001, 3, 1.0, 0.1, 1000, 128, 1024, 1e-06, 100000, 0.0, 'HM_LSTM'], 'data': {'validation': {'perplexity': [63.991547090530396, 14.935816033697128, 9.8579627778172494, 8.3343876107692711, 7.8451161527633664, 7.272731275224686, 7.2152926173806193, 6.6288936587929728, 6.2496711700797078, 6.4177275776743885, 6.5918652122020722, 6.5170262963771819, 6.7002572950482371, 6.4645970281839373, 6.3118467666387561, 6.3629542648196225, 6.3116871696591375, 6.4408488047719006, 6.8159539461612697, 6.0510364099025722, 6.2041584927320477, 6.596599865293503, 6.2872921619176863, 6.3277447440743444, 6.6429828269362448, 6.1374371313571929, 6.1247154134511952, 6.2309515701889993, 6.1635671814203263, 6.3538403024792673, 5.9789248906493189, 6.1438478872537612, 5.9963584930777554, 6.1138567480802539, 5.894437098872662, 5.9656921865105632, 5.99888504447937, 5.8088876264691356, 6.1879898933768276, 5.8016445956468585, 6.0413312243580819, 6.0878138591408728, 5.9354982839703556, 5.5491238495349888, 5.9488376078009608, 5.9275902623057366, 6.0003394758939743, 6.1612870089888574, 5.8897209743499754, 5.7651479438781736, 5.8198315171360973, 6.0512455655336383, 5.7253269800901414, 5.7372317046403882, 5.7278064307570453, 5.9926921927332879, 5.8160023275732993, 5.8111041462898259, 5.7906231503009797, 5.7901069302916524, 5.5531511036396024, 5.7875648714542391, 5.7107169678807255, 5.6813161431193349, 5.9844566672563557, 5.6155735187411304, 5.6783385795831682, 5.8173576879501345, 5.5523915094375607, 5.7536324801206593, 5.4842489607930185, 5.7823946065425869, 5.4792077111482618, 5.5925325837492945, 5.8981049732565882, 5.6914997109889987, 5.6389715032935142, 5.4709098860502241, 5.7651508922338488, 5.6866156324625017, 5.8190552999734875, 5.61071079531908, 5.7874363727450371, 5.336546672034264, 5.5113661939740179, 5.7005732151150701, 5.5856607873558994, 5.8873633772373202, 5.8452647546291354, 5.6900767827749252, 5.5969558866858486, 5.6106065453171734, 5.8062644369363783, 5.6291573303937916, 5.6578999631285667, 5.7475517621874808, 5.6938874530792241, 5.5035124346017836, 5.4461197063565256, 5.740426647114754, 5.6898946242928501, 5.7123227553486826, 5.8688856567382812, 5.5241850759267805, 5.7042475057840347, 5.8707966106057166, 5.818093642961979, 5.6036582050204276, 5.6637079765200617, 5.387835202550888, 5.4507010806083676, 5.5116570256114006, 5.6067093015670775, 5.8284165609002114, 5.5930897579193113, 5.503957832849026, 5.5077417448759078, 5.7765031185984608, 5.4779038614988327, 5.8416177483558656, 5.4447150568366052, 5.5744718486189839, 5.3284769418954845, 5.6283901373624801, 5.5362372635722163, 5.4146334391117099, 5.584581410896778, 5.5582250853896138, 5.3152251022458072, 5.356588188064098, 5.4714355063199998, 5.8279011197924611, 5.3477567283391956, 5.5193864838838573, 5.4625129142165187, 5.160420224678516, 5.1611310891747477, 5.2700638578057291, 5.39099625825882, 5.4046790951490404, 5.3729551977992056, 5.4875404230594631, 5.2890794351696968, 5.5116550237536428, 5.595329068791866, 5.3314246189117434, 5.6467064855337146, 5.2688005724668505, 5.4375883222699164, 5.3594741551518439, 5.2729976073980334, 5.2978475332736972, 5.3685741406917575, 5.4950675075411795, 5.5554621201992038, 5.4932971975922582, 5.3648252727389334, 5.5494575571894647, 5.3048608542442324, 5.2993260890126228, 5.3848980239033697, 5.1960974428057671, 5.3603553437471394, 5.5288240202665326, 5.4154888656854627, 5.3228330643415447, 5.2234395644664762, 5.1541874135613446, 5.42958829780817, 5.4050531445741656, 5.1650565065383915, 5.2729827488422396, 5.2380721208095551, 5.3487456160187721, 5.3689283815503117, 5.4554200956821441, 5.4244878367900853, 5.2842023377418519, 5.5154379820466044, 5.3721952254652976, 5.4230484498858456, 5.1621100556731223, 5.2536631594061856, 5.3608580425739287, 5.3103163171291348, 5.3381208233952524, 5.1920999381303785, 5.4826998846650126, 5.2684181489944457, 5.3312553591132161, 5.4739931834340094, 5.3462324792861935, 5.3521105345129962, 5.504691844964027, 5.3628197440862655, 5.2980320139050487, 5.1091634168148037, 5.0772804490685459, 5.6469028148889544, 5.4310308267474179, 5.3712091088175775, 5.1731124442100525, 5.2497401250004767, 5.3969867226362229, 5.2590358341097829, 5.1702027113556861, 5.0267237043142314, 5.1558123539686207, 5.1275028819441797, 5.3922817638993266, 5.0870160678505894, 5.1885863211512566, 5.1992353281140327, 5.1784431416153911, 5.2396703100562094, 5.1148331264853475, 5.1688668987154962, 5.3959557456493377, 5.2505004938602449, 5.2238778659701346, 5.2031106666207316, 5.1039342849850655, 5.4551198369145393, 5.4590796037673952, 5.2059017241239545, 5.2206639376997952, 5.1116020806908606, 5.1385097573757168, 5.145881129956245, 5.1502989271998407, 5.222784793519974, 5.3051303562760355, 5.1962509219169615, 5.228481718826294, 5.1833308209419249, 5.2234651305913928, 5.2131804449558254, 5.0800309107184409, 5.2754973942279815, 5.1890985885739322, 5.1807751304388043, 5.3690058998346331, 5.2611411921143532, 5.1408189470052719, 5.1861669827222823, 4.9937501833677294, 5.0177932299971584, 5.1463021952390671, 5.0570548920392993, 5.2537338333725927, 5.1981758688926698, 5.1772811035633088, 5.1870740975618359, 5.2047459473967548, 5.2331915934562687, 5.2606224315404893, 5.2461129490137104, 5.1923077146768568, 5.233299961841106, 5.1510940896511075, 5.3711275433182717, 5.1791064290523527, 5.0803843768835071, 5.3160591332674025, 5.1403876895785334, 5.2143695509195327, 5.039121674513817, 5.2744725487470623, 5.0393153923988345, 4.9365485304594037, 5.1658199470043185, 5.1174438722610471, 5.2820621090531352, 5.1140052466988566, 5.189159827244282, 5.2066043289422987, 5.2356787488102912, 5.0771694019675255, 5.0827326599717138, 5.2744627049326898, 5.2130094465255734, 5.2640268208384517, 5.0891365804195408, 5.2847295184135437, 5.1395160201787951, 5.0123158649802209, 5.1091302335143087, 5.1624508759140966, 5.0804594451189038, 5.1239833432316777, 5.1094853231072426, 5.1146872888326644, 5.2909759887695316, 5.03908006567955, 5.075022142195702, 5.1541497637510298, 5.0394318348169325, 5.1377072208642955, 5.1628049701929095, 5.1706202506661416, 5.2110875292062762, 5.0948382627010345, 5.1917263029575347, 4.9806819973945622, 5.1094330493807796, 5.0938363558292385, 5.1835152021646502, 5.0723013509154322, 5.1589977271556853, 5.1507369339346889, 5.038873924362659, 5.2024096981167789, 4.8884175067901614, 5.1280280886650083, 5.0772929247021672, 5.0971420872926716, 5.0568212900280951, 5.0885988660335544, 5.0744828519225118, 5.125719911301136, 5.147322267365456, 5.0706365249753, 4.9393678641319276, 5.1680541767120358, 5.074050675261021, 5.0603279151201246, 5.0920591096758843, 5.0180297644495964, 5.1365594524502756, 5.1872496132373813, 5.1354562689661982, 5.1877133240103719, 5.0929445699572566, 5.1642317047238349, 5.1139721028685567, 5.0860507532238959, 4.9502928868293763, 4.936719725871086, 5.1565770068287851, 4.981891042876244, 5.0836572893738747, 4.9805845287680626, 4.9725273187398908, 5.0086081214904787, 5.0819678180575369, 4.9858565827250478, 5.0092992877364155, 5.1305916910886769, 5.0698216155886646, 4.9690101033329963, 5.1312394524097442, 5.1054106261372567, 5.1009606804132464, 5.0477387031912802, 5.0702366654872897, 5.0188360377073291, 5.0937226542949681, 5.0319657782316209, 5.0681396842360495, 5.0628391557812691, 5.1432893930315968, 5.0565677704215046, 4.9308456226944921, 5.0127880736470223, 5.0344709793567661, 4.9470119538784028, 5.1638696903467176, 5.1729258590102196, 5.0525122493028638, 5.1783553694128992, 5.3011944443702701, 5.1448212349653248, 5.0014298084378241, 4.9992744891643524, 5.00518817615509, 5.0845432933092116, 5.0776720609784123, 5.0491740806102756, 5.0461535856008526, 5.0071313848853114, 5.095096041333675, 5.0492639381647111, 5.1470041469216348, 4.9703139690518379, 4.9882020585060118, 5.0596620394706724, 5.0265911389946938, 5.0906256327509878, 4.9964852062463763, 4.9378737773418431, 4.974167913496494, 4.9386549560546875, 4.8723476372957233, 5.0102905895829197, 4.8760720414876939, 5.0136506378054619, 5.0151694436550143, 5.0533631416440006, 5.0085698891997339, 4.970280355274677, 4.8308334449410442, 5.0702720252871512, 4.9390259480595589, 4.9110511017441754, 4.9489672881603237, 4.8936712540507319, 4.8909534209132195, 4.990059199988842, 4.9915061939001086, 4.9853934762239458, 4.8992812873244285, 4.9040432557821276, 5.0432470411300656, 5.0375073771834371, 4.8526758141160009, 5.0545270646333691, 4.8958489341616627, 4.9471750201702118, 4.9650928310513498, 5.0138169107079502, 4.9441767418622975, 4.8884579183578492, 4.9211024151802061, 4.8353847028136254, 4.9442115271925928, 4.8338916046977047, 4.9223315896749495, 5.017298233366013, 4.9529863446831701, 4.9799032603025433, 4.9615568484306332, 5.0568398242592814, 4.9900434771180153, 4.8861337339878084, 4.9008691295504567, 4.9583075355291371, 4.9491125984072681, 5.0964345786333087, 5.0991176305532457, 5.0723485542416569, 5.005870199906826, 5.0368831760883328, 5.016784417498112, 5.0433713459491729, 5.0226208236455916, 4.8482260228037832, 5.0167259687900545, 4.9762553155660632, 4.9678717921972275, 4.956051141571999, 5.0199708562374115, 4.9532359778523443, 4.9667401672482487, 5.0368855673670767, 4.9613875243783001, 5.0048497642636303, 4.9687818289995196, 5.0224130080819132, 4.8481881958007813, 4.9343239737391471, 4.9220706277012827, 4.9601985482931141, 4.9574564155578615, 5.0557008010149005, 5.0428435414671897, 4.9412338284254078, 4.9425079047083855, 4.9562805001735688, 5.0519848729491237, 4.970273215317726, 4.9460205352187154, 4.969056279814243, 4.9097718543052675, 5.0207539248108866, 5.1035122586846349, 4.9781174846649172, 5.0269684125065801, 4.9828557311892512, 5.0076729967832563, 4.9203654889345172, 4.9053073400855061, 5.1019903606176378, 5.0114058908820152, 4.9105600148677828, 4.9467856504321102, 5.0240592261075969, 4.9339850615382197, 4.9576693450927731, 4.9564851826190948, 4.9355073389172555, 5.0028455729126931, 5.0359029054760933, 4.9627362676262852, 4.9155801151156426, 5.0287036644220349, 4.9788286923170091, 4.8414678312063213, 4.9246485016226771, 4.8126799188971523, 4.9493309522986415, 4.8546518736481667], 'step': [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, 10100, 10200, 10300, 10400, 10500, 10600, 10700, 10800, 10900, 11000, 11100, 11200, 11300, 11400, 11500, 11600, 11700, 11800, 11900, 12000, 12100, 12200, 12300, 12400, 12500, 12600, 12700, 12800, 12900, 13000, 13100, 13200, 13300, 13400, 13500, 13600, 13700, 13800, 13900, 14000, 14100, 14200, 14300, 14400, 14500, 14600, 14700, 14800, 14900, 15000, 15100, 15200, 15300, 15400, 15500, 15600, 15700, 15800, 15900, 16000, 16100, 16200, 16300, 16400, 16500, 16600, 16700, 16800, 16900, 17000, 17100, 17200, 17300, 17400, 17500, 17600, 17700, 17800, 17900, 18000, 18100, 18200, 18300, 18400, 18500, 18600, 18700, 18800, 18900, 19000, 19100, 19200, 19300, 19400, 19500, 19600, 19700, 19800, 19900, 20000, 20100, 20200, 20300, 20400, 20500, 20600, 20700, 20800, 20900, 21000, 21100, 21200, 21300, 21400, 21500, 21600, 21700, 21800, 21900, 22000, 22100, 22200, 22300, 22400, 22500, 22600, 22700, 22800, 22900, 23000, 23100, 23200, 23300, 23400, 23500, 23600, 23700, 23800, 23900, 24000, 24100, 24200, 24300, 24400, 24500, 24600, 24700, 24800, 24900, 25000, 25100, 25200, 25300, 25400, 25500, 25600, 25700, 25800, 25900, 26000, 26100, 26200, 26300, 26400, 26500, 26600, 26700, 26800, 26900, 27000, 27100, 27200, 27300, 27400, 27500, 27600, 27700, 27800, 27900, 28000, 28100, 28200, 28300, 28400, 28500, 28600, 28700, 28800, 28900, 29000, 29100, 29200, 29300, 29400, 29500, 29600, 29700, 29800, 29900, 30000, 30100, 30200, 30300, 30400, 30500, 30600, 30700, 30800, 30900, 31000, 31100, 31200, 31300, 31400, 31500, 31600, 31700, 31800, 31900, 32000, 32100, 32200, 32300, 32400, 32500, 32600, 32700, 32800, 32900, 33000, 33100, 33200, 33300, 33400, 33500, 33600, 33700, 33800, 33900, 34000, 34100, 34200, 34300, 34400, 34500, 34600, 34700, 34800, 34900, 35000, 35100, 35200, 35300, 35400, 35500, 35600, 35700, 35800, 35900, 36000, 36100, 36200, 36300, 36400, 36500, 36600, 36700, 36800, 36900, 37000, 37100, 37200, 37300, 37400, 37500, 37600, 37700, 37800, 37900, 38000, 38100, 38200, 38300, 38400, 38500, 38600, 38700, 38800, 38900, 39000, 39100, 39200, 39300, 39400, 39500, 39600, 39700, 39800, 39900, 40000, 40100, 40200, 40300, 40400, 40500, 40600, 40700, 40800, 40900, 41000, 41100, 41200, 41300, 41400, 41500, 41600, 41700, 41800, 41900, 42000, 42100, 42200, 42300, 42400, 42500, 42600, 42700, 42800, 42900, 43000, 43100, 43200, 43300, 43400, 43500, 43600, 43700, 43800, 43900, 44000, 44100, 44200, 44300, 44400, 44500, 44600, 44700, 44800, 44900, 45000, 45100, 45200, 45300, 45400, 45500, 45600, 45700, 45800, 45900, 46000, 46100, 46200, 46300, 46400, 46500, 46600, 46700, 46800, 46900, 47000, 47100, 47200, 47300, 47400, 47500, 47600, 47700, 47800, 47900, 48000, 48100, 48200, 48300, 48400, 48500, 48600, 48700, 48800, 48900, 49000, 49100, 49200, 49300, 49400, 49500, 49600, 49700, 49800, 49900, 50000], 'percentage': [13.95, 29.02, 38.74, 44.9, 50.22, 52.16, 54.56, 55.28, 54.45, 55.07, 55.04, 55.48, 55.13, 55.03, 54.16, 55.05, 56.46, 54.36, 55.42, 57.21, 56.58, 57.48, 56.27, 55.91, 55.58, 56.24, 57.34, 55.95, 56.95, 58.06, 58.34, 56.91, 57.41, 57.56, 59.99, 60.32, 58.68, 60.35, 58.49, 59.23, 58.19, 58.89, 59.13, 58.84, 58.95, 59.85, 60.22, 59.09, 59.12, 59.25, 60.43, 59.48, 59.83, 59.71, 59.69, 59.92, 59.56, 60.07, 60.5, 59.03, 60.37, 60.92, 60.28, 60.41, 60.07, 60.48, 59.83, 61.13, 61.97, 62.4, 61.94, 61.55, 62.24, 61.71, 60.98, 61.3, 60.99, 61.66, 61.17, 60.71, 61.37, 61.31, 60.32, 61.22, 61.47, 61.82, 60.28, 59.59, 60.24, 59.81, 60.11, 60.45, 60.41, 61.23, 61.38, 61.45, 60.62, 62.0, 61.25, 60.46, 61.01, 61.08, 60.15, 61.01, 61.08, 60.78, 60.61, 61.38, 60.83, 61.58, 60.82, 60.01, 61.28, 60.99, 62.0, 61.55, 61.46, 61.54, 61.39, 60.85, 61.71, 61.1, 63.11, 62.37, 62.16, 62.59, 61.76, 62.65, 61.97, 61.4, 61.59, 61.3, 62.33, 61.85, 62.18, 62.82, 62.41, 62.31, 62.72, 62.31, 61.63, 61.56, 61.59, 61.91, 60.81, 61.28, 61.21, 61.54, 62.28, 61.5, 62.21, 62.2, 61.95, 62.29, 61.65, 61.65, 63.33, 63.99, 64.76, 63.95, 63.44, 63.87, 62.95, 62.82, 62.34, 63.22, 63.15, 63.31, 63.37, 63.17, 63.04, 63.12, 63.36, 63.17, 62.43, 62.78, 62.33, 61.63, 62.37, 62.7, 63.45, 63.76, 62.97, 62.48, 63.35, 63.54, 63.52, 62.36, 62.81, 62.52, 62.03, 62.59, 62.96, 63.53, 62.77, 62.31, 63.95, 63.33, 62.77, 62.45, 62.75, 63.33, 63.33, 62.86, 62.83, 63.47, 63.56, 63.71, 63.82, 62.94, 64.65, 63.22, 63.78, 64.07, 63.97, 63.91, 64.02, 63.88, 63.48, 63.57, 63.69, 63.98, 64.12, 63.53, 63.21, 63.65, 64.07, 64.19, 63.9, 63.82, 63.98, 63.45, 63.1, 63.2, 63.57, 63.51, 63.88, 63.62, 63.32, 64.21, 64.25, 62.83, 63.29, 63.33, 64.07, 65.63, 64.69, 64.49, 64.22, 63.83, 63.77, 64.32, 63.49, 63.21, 63.93, 63.65, 63.64, 64.27, 64.57, 64.36, 64.08, 64.44, 64.37, 63.57, 64.56, 63.61, 64.38, 64.85, 64.45, 64.73, 64.93, 64.2, 64.84, 63.97, 63.99, 63.88, 64.07, 63.79, 64.93, 64.28, 64.52, 64.67, 64.58, 64.47, 64.53, 64.98, 64.67, 64.39, 64.74, 64.34, 64.31, 64.05, 64.24, 64.41, 64.09, 64.25, 64.47, 64.49, 65.22, 64.17, 64.77, 64.7, 64.35, 64.61, 64.52, 64.48, 64.0, 64.4, 64.44, 64.53, 65.24, 64.41, 65.32, 64.8, 65.02, 64.7, 64.36, 65.02, 65.27, 64.52, 65.16, 64.43, 64.99, 64.23, 65.02, 63.71, 64.07, 64.97, 64.64, 63.66, 64.34, 65.37, 65.32, 65.27, 64.56, 65.58, 64.73, 64.82, 64.28, 64.72, 64.5, 65.1, 64.52, 64.81, 64.79, 64.15, 64.87, 65.1, 65.0, 65.01, 64.63, 64.36, 64.5, 64.62, 64.82, 64.86, 64.88, 64.9, 65.16, 64.89, 65.14, 65.44, 64.9, 64.94, 65.4, 65.15, 65.04, 64.53, 65.05, 65.08, 63.94, 64.69, 64.54, 65.43, 65.37, 64.98, 65.04, 64.47, 64.88, 64.93, 64.92, 65.48, 65.08, 65.18, 65.14, 65.24, 65.82, 65.16, 65.56, 65.35, 64.52, 64.93, 64.83, 65.16, 64.94, 65.03, 64.78, 65.78, 65.35, 65.66, 65.78, 65.21, 65.73, 65.48, 65.33, 65.48, 65.28, 65.26, 65.58, 65.38, 65.35, 65.36, 65.4, 65.6, 65.52, 65.44, 65.28, 64.55, 65.72, 66.0, 66.3, 66.79, 66.31, 66.29, 65.98, 65.32, 65.51, 65.31, 65.18, 65.19, 65.05, 65.04, 65.2, 65.62, 65.91, 65.89, 65.67, 64.78, 64.89, 65.61, 65.34, 65.68, 65.57, 65.27, 65.45, 66.05, 65.75, 65.68, 65.61, 65.58, 65.39, 65.69, 65.52, 65.41, 65.38, 65.41, 65.41, 65.62, 65.66, 65.81, 65.61, 65.63, 65.49, 65.86, 65.19, 65.25, 65.26, 65.32, 64.96, 65.09, 65.44, 64.83, 65.07, 65.64, 65.54, 65.46, 65.66, 65.39, 65.39, 65.08, 65.63, 65.71, 65.59, 65.74, 65.8, 66.0, 66.15, 65.82, 66.28, 65.58, 65.51, 65.68, 65.67, 65.9, 65.55, 65.75, 65.42, 65.51, 65.56, 65.54, 65.7], 'BPC': [5.9622708911895748, 3.5314968273552134, 2.9685142172275576, 2.6979776098289179, 2.4266294767870247, 2.3330029841387621, 2.2113188758310351, 2.1407586981185318, 2.2062425279617064, 2.1983792966454616, 2.1673989026879892, 2.1372554133310588, 2.154496482683955, 2.1294881671374286, 2.1987178117497517, 2.126922802534136, 2.1080325951771215, 2.1402020620111002, 2.1356058953736543, 2.0677083978342665, 2.0663790742560786, 2.0527649223847693, 2.108853608961565, 2.1114750960184203, 2.1053544665302866, 2.105319454097228, 2.0467317874793691, 2.0713660738192021, 2.0354908654434807, 1.9940514524632309, 1.9787424087555012, 2.0189224944590389, 2.0158678863728952, 1.9979740649347568, 1.9093641760346327, 1.9100688470953202, 1.9290183265091909, 1.9060401481592431, 1.9501830814645993, 1.9065153833991018, 1.9380964485625169, 1.9362765218584408, 1.9540761901276973, 1.903368252694376, 1.9316280864317386, 1.8926875535973362, 1.8734726300409428, 1.9221635860086215, 1.9264035995035764, 1.9231798665854893, 1.9019749065560878, 1.8733201072675456, 1.8753339327317502, 1.8793804881003471, 1.9033230493817777, 1.8837586675737519, 1.8793770393530274, 1.9175710037456568, 1.8612513572207601, 1.9173073603230089, 1.8658941576954382, 1.8434123671627252, 1.8777873291150722, 1.8877659693347051, 1.8617612467346927, 1.8609114887344393, 1.8663873942986788, 1.8181197898802672, 1.7891215831406952, 1.7589479823756633, 1.744311947285101, 1.7993202830208872, 1.7656139453442397, 1.7778710901647832, 1.8503287613638422, 1.8118768613940734, 1.8167316274740977, 1.8127473706175417, 1.8218022868551951, 1.8475502930754348, 1.8157751584742845, 1.8280123710510863, 1.8463557775439603, 1.839559106665444, 1.7839854671272881, 1.8242525805773622, 1.8250315536292667, 1.9188611114917031, 1.8469384500371762, 1.8867733018016493, 1.8576384332961242, 1.851621844987448, 1.8276207485255533, 1.8509169899298155, 1.8069618023878131, 1.817008166907274, 1.803468782105478, 1.7727542612582701, 1.809241920636423, 1.8483913893572614, 1.8295163561804266, 1.8504655260656306, 1.8817627935830439, 1.8470963292654989, 1.8425008419404072, 1.8138559212409266, 1.850139402630258, 1.8345821129866389, 1.8370979888658421, 1.8275641618267888, 1.8356730050565966, 1.8860263303054896, 1.831948531803087, 1.8269474134043819, 1.814179596493813, 1.8196304731277093, 1.8029939114961453, 1.8205571945471868, 1.8239759180243535, 1.8458231896689092, 1.8163626682197123, 1.8211131363768356, 1.7250001051823958, 1.7392810678327304, 1.7638482473464228, 1.7578716603371858, 1.779477122673679, 1.742392545199889, 1.7444694579885169, 1.7988313711522714, 1.8159918527728671, 1.8274036881397151, 1.7736196234944088, 1.7688950663565957, 1.7671019480878727, 1.7499102009903298, 1.7755898923602602, 1.7900396018446278, 1.7615577071469724, 1.7578471668656825, 1.7841814835733414, 1.8027049838248357, 1.7959680954589083, 1.7645210857307232, 1.8179635612108458, 1.795950861784652, 1.7893749194643205, 1.7839342075402376, 1.7518237806083641, 1.7772480104364614, 1.7781922428540682, 1.7811284709420849, 1.7615123647101321, 1.751768301719812, 1.7980146018109278, 1.7683196063290125, 1.7048384265824985, 1.6917868560144405, 1.6528570988704498, 1.6645494221600137, 1.6895670973100598, 1.6824365263387222, 1.7235805337475281, 1.7449992392720206, 1.7455644104469574, 1.71845230013178, 1.7232467871735051, 1.7142581113751911, 1.7197637547283735, 1.7198802055919975, 1.7201111085642333, 1.7080742396113169, 1.6922577014622453, 1.7145437092354885, 1.7581718479116011, 1.7395554135353815, 1.7464157688211914, 1.7626990221221881, 1.7473046503728353, 1.7366976147438533, 1.7005134526467438, 1.7024773799741693, 1.716082245685508, 1.7367718155917464, 1.7004192957069824, 1.6934081822242857, 1.7107697186278739, 1.7573454260751151, 1.744523645946338, 1.7591442776171287, 1.7769088363826147, 1.7709840015210505, 1.7386433649338173, 1.7136190023630209, 1.7617656889702329, 1.7382960425427381, 1.6990506453637302, 1.726656445808451, 1.7375894510521852, 1.7601837890467875, 1.7537700930119848, 1.7286297765695793, 1.7245607297939833, 1.7424540091073901, 1.7271654598980666, 1.7239164623596939, 1.706436313819313, 1.7168559547186797, 1.7122743729180854, 1.7269400105231261, 1.6429379272880555, 1.6892316984320543, 1.6910518226095701, 1.6712082270036961, 1.6714548224460348, 1.666028358433908, 1.6767947880232879, 1.6988107273067594, 1.7045694840956556, 1.7017302613549836, 1.6982133182266328, 1.6531920757790948, 1.6931628477387211, 1.6922791043697361, 1.6983170041495479, 1.6878708523027599, 1.6682686093859742, 1.6687569419819266, 1.693000713758088, 1.6949642044676945, 1.6629945046192698, 1.6816390791538598, 1.7004627917744202, 1.6942454245668175, 1.6908315016490407, 1.6823981772047263, 1.6811996763814754, 1.6872720122289719, 1.7112333881947546, 1.6745494372869547, 1.6613195258387545, 1.7204194044707584, 1.7009406147961963, 1.6729305318655432, 1.6361667827889186, 1.6083347012078999, 1.6232968830832104, 1.6336690703384875, 1.6519125723691934, 1.6571785765845879, 1.6549132379773712, 1.6485488061279276, 1.6863381416041492, 1.67855108524313, 1.6671308227008081, 1.6858681382058391, 1.6853777180336491, 1.6531670134001495, 1.6500996612337469, 1.661046055493318, 1.6575329709743285, 1.6537422281877709, 1.6595872949039441, 1.7038224909070228, 1.6454757964349953, 1.6832940614254468, 1.6602931004707273, 1.6336218929737709, 1.6431947928531601, 1.6214316827404855, 1.634529613591263, 1.6612232238950984, 1.6340923087737982, 1.6687014349331695, 1.688784786649961, 1.6971996715410158, 1.6850325361007545, 1.7070241606555046, 1.6782612131036163, 1.6893909382716785, 1.6673527901754606, 1.6769659325178137, 1.660842335453621, 1.6699786666210183, 1.6403883925580958, 1.6365232155873088, 1.6454546746984913, 1.6679716423214053, 1.6561921015667529, 1.6558748960107572, 1.6801323371299453, 1.6774052243854338, 1.6664043641910415, 1.6750596926641153, 1.7041360387806155, 1.669836985989358, 1.6675242928301564, 1.6435273843227289, 1.6215252148528683, 1.6450777173715663, 1.640361087498627, 1.6454700266413524, 1.665807310512305, 1.6404388222641952, 1.6549735357607021, 1.6444565118508825, 1.6690788792350824, 1.659761904195467, 1.6640033947337849, 1.637825742210685, 1.6299810462366799, 1.6484800410572731, 1.6244775777736429, 1.6443113426734486, 1.6164688369628792, 1.6363276081311764, 1.6544954438256227, 1.6290445602745296, 1.6121976177125379, 1.6248173677610507, 1.6162969909952727, 1.6400667091723891, 1.6293368914935202, 1.6717970764200316, 1.6369566488735299, 1.670600377104875, 1.6537156664444426, 1.6473082377855579, 1.6407959239214156, 1.6755462979452547, 1.6447913811445252, 1.5978263428017023, 1.5907791105111964, 1.6079794972239434, 1.6278705128057065, 1.5934560313494186, 1.6088765650443679, 1.6110306133122925, 1.6357401899615898, 1.6374871254868097, 1.625689880121449, 1.6125439952877036, 1.6339543289980092, 1.6363135312227235, 1.6273239537366169, 1.6456556796732134, 1.6372530362817903, 1.6095754590243159, 1.6163927008493923, 1.6153759803852337, 1.6342767804162652, 1.6429906678771153, 1.6264508201844965, 1.6218696391526399, 1.6319335733250857, 1.6162452138383241, 1.632032546886738, 1.608655454904004, 1.6084266725609775, 1.624025015792578, 1.614182450868723, 1.6116106157851164, 1.6256436948137503, 1.6176444282658793, 1.636101343606615, 1.6488975203557621, 1.652113921110137, 1.6624481918617049, 1.6339832689217755, 1.6433092954960671, 1.669695758601826, 1.6525982236692522, 1.6477134891314418, 1.6292493889885193, 1.6315308400379966, 1.6494897758611695, 1.6587452097071327, 1.6679281604483855, 1.6558030496022129, 1.6295717821702662, 1.6294748626873889, 1.6220199814310139, 1.6340953845920696, 1.6261594698475328, 1.6230688252962524, 1.6036641722993457, 1.5865927628422523, 1.5967948117183124, 1.5951638130177037, 1.605506388905116, 1.6531912135165934, 1.6266160137717962, 1.6328085535098256, 1.6266006196353309, 1.6234332480143228, 1.6425276480041666, 1.6358713997957106, 1.6039434737295195, 1.6090178004508797, 1.6062630119114447, 1.6100288947507329, 1.6142629846581329, 1.6039670910752921, 1.6001802058268351, 1.6127682207140062, 1.6097408707724734, 1.6061080829725467, 1.602678047674428, 1.6061238438505576, 1.6015506424930468, 1.6013096009005425, 1.613179558746282, 1.6065727761528275, 1.607183480228604, 1.6172889955342407, 1.6236794415443683, 1.6209024255184359, 1.6389968466554041, 1.5991530934788309, 1.5765703174917711, 1.5605654959674833, 1.5477469565059656, 1.5565496240662831, 1.56053015430009, 1.5685614454990042, 1.5949754907941267, 1.6024455985103703, 1.6093407428283015, 1.610476319620348, 1.6012317526151487, 1.6221537121495559, 1.626646345978717, 1.6133865393644942, 1.6058475936220287, 1.5916053908705676, 1.5710071905831973, 1.5911724429060359, 1.612066192267122, 1.620482448849375, 1.5873835796429612, 1.5997187558791943, 1.599142843718999, 1.5943745440619892, 1.593641926240418, 1.6011307842729736, 1.5845616899510562, 1.5932953905052041, 1.5963588675643841, 1.5958475990945045, 1.5916518632097745, 1.6071583142373393, 1.5924353767828343, 1.5970017382227542, 1.6128609727873375, 1.6241700283329648, 1.6163009767556167, 1.6238792284762764, 1.616828614587053, 1.6026068883148945, 1.5945941773378967, 1.606311654855368, 1.603156658110477, 1.6192281867402929, 1.6180395378663506, 1.6261775271841437, 1.6382578924365927, 1.626260251397047, 1.6172744320036951, 1.635482946620906, 1.6288167331976904, 1.6188063320610613, 1.6335985694643462, 1.6287389309223075, 1.5948375601536635, 1.6056844214427977, 1.6061182890414389, 1.5915404907076958, 1.6101365380057684, 1.6040928441614313, 1.6026359784440622, 1.5941226590540756, 1.5937000536385464, 1.5964761981366504, 1.6045804496798723, 1.5863485074264601, 1.570718130759416, 1.5784517212959326, 1.5802328103942909, 1.5817706767264887, 1.5927931868627214, 1.583138598903018, 1.5867659100881766, 1.5936850878830167, 1.5817212191571899, 1.5823175396234299, 1.5831631432081956, 1.6004907054700082, 1.5954112598507486, 1.5888655797963342, 1.6063677696107277, 1.5926353049947422]}, 'train': {'perplexity': [14.838009, 10.165711, 8.4415922, 7.9715595, 7.313026, 7.3697939, 7.0447569, 6.2415257, 6.6602674, 7.0026793, 6.9458919, 7.0579872, 6.4235911, 6.3742003, 6.7895937, 6.2907119, 6.3806639, 7.4288068, 6.3759365, 6.3822007, 6.8730731, 6.5029087, 6.5868492, 6.7785339, 6.3025556, 6.1735072, 6.6100831, 6.2444129, 6.3129597, 6.4025311, 6.0324306, 6.2027822, 6.0548902, 6.1307964, 6.3795872, 6.4847174, 5.9965115, 6.3114672, 6.0293899, 6.0434904, 6.6036153, 6.1877913, 5.8848133, 5.7970948, 5.9973454, 6.42416, 6.4348869, 6.1996193, 5.5229712, 5.9165468, 6.4205556, 5.8646898, 5.9388256, 5.8772526, 6.2500486, 5.9731159, 6.3152928, 6.0428677, 6.2492213, 5.5623512, 6.0058775, 5.9483123, 5.9847732, 6.1667237, 5.9599123, 6.1097541, 5.8667116, 5.8601232, 6.2108593, 6.207665, 6.1688728, 6.0487394, 5.9896913, 5.7247424, 5.7479162, 5.7846532, 5.752737, 6.0628924, 6.065835, 5.9929352, 5.8466449, 6.0406923, 5.7308497, 6.1046629, 5.7790737, 5.7371659, 6.0209532, 6.067276, 5.7990947, 5.7799206, 5.7729869, 6.0130739, 6.0367222, 5.8607039, 6.0846453, 5.7751961, 5.4906688, 5.7899609, 5.9313173, 5.6665864, 6.3230381, 6.1999798, 5.6909571, 5.9157414, 6.3038588, 6.0763597, 5.9357777, 5.8647876, 5.5578942, 5.6904006, 5.8329577, 6.0424523, 6.0407457, 5.7101564, 5.8939342, 6.0168724, 6.174325, 5.6982479, 6.1691113, 5.6816311, 5.8499846, 5.4481115, 5.9627094, 5.9389911, 5.6076612, 5.9998174, 5.8551955, 5.616477, 5.5655613, 6.115355, 5.9306703, 5.6268849, 6.0333877, 5.6817513, 5.5947828, 5.7581496, 5.4899797, 5.6731749, 5.8329768, 5.7674451, 5.9445815, 5.4929676, 5.994381, 5.562367, 5.7808409, 5.8740296, 5.7154913, 5.5805097, 5.5858421, 5.6453552, 5.4990172, 5.6046252, 5.873395, 5.7126646, 5.6766534, 5.7191181, 5.8986754, 5.6436653, 5.5711842, 5.795382, 5.4741087, 5.7369404, 5.6341205, 5.7226992, 5.7345629, 5.9389257, 5.4784412, 5.8096919, 5.6211743, 5.6470399, 5.7893872, 5.4079785, 5.7005525, 5.9612818, 5.7735229, 5.7621837, 5.4494362, 5.918788, 5.8526387, 5.7073216, 5.4104671, 5.7249255, 5.722014, 5.6953626, 5.6238294, 5.4432001, 5.7538362, 5.5876794, 5.6603651, 5.9787583, 5.5259633, 5.7839036, 6.0474882, 5.8705597, 5.5227399, 5.5588851, 5.474844, 5.8671417, 5.6192155, 5.6825013, 5.7190809, 5.5145726, 5.7036195, 5.7373753, 5.5704103, 5.4009724, 5.609468, 5.2753277, 5.828578, 5.4311876, 5.5473914, 5.7129774, 5.557538, 5.6489382, 5.5413084, 5.3938384, 5.8381443, 5.5511122, 5.5177331, 5.3773937, 5.5622239, 5.690249, 5.8350501, 5.4917727, 5.6234951, 5.5246468, 5.5239086, 5.6590419, 5.2998805, 5.6196885, 5.6192837, 5.7220702, 5.8697524, 5.7862487, 5.4364476, 5.7058229, 5.318078, 5.6477232, 5.5702729, 5.4599938, 5.8810215, 5.5786719, 5.5735569, 5.8583775, 5.3156877, 5.5590663, 5.5700669, 5.4079809, 5.5376525, 5.567368, 5.5116811, 5.6455364, 5.5184608, 5.5570416, 5.4666662, 5.5134826, 5.8893447, 5.3700438, 5.3805494, 5.7675953, 5.4778223, 5.4159651, 5.5292139, 5.5999732, 5.4561152, 5.2004247, 5.8027515, 5.2622132, 5.0162058, 5.5438066, 5.502862, 5.4935341, 5.4756951, 5.3912354, 5.3663359, 5.73244, 5.6012006, 5.3764782, 5.449348, 5.7275453, 5.7318769, 5.4886794, 5.7028675, 5.7521596, 5.6136146, 5.5158825, 5.3376393, 5.3428783, 5.437232, 5.57057, 5.1860528, 5.6766009, 5.503109, 5.496367, 5.3580103, 5.3229365, 5.47225, 5.3180432, 5.430686, 5.5260682, 5.3815479, 5.5158038, 5.1558385, 5.8122125, 5.5802565, 5.4757652, 5.5445514, 5.5619378, 5.3217616, 5.3032966, 5.4614935, 5.1709533, 5.6537604, 5.331079, 5.4565172, 5.2337537, 5.4041724, 5.7414708, 5.6398854, 5.6618195, 5.5005956, 5.388701, 5.6666083, 5.5029535, 5.3049903, 5.317246, 5.5241332, 5.6203136, 5.5199862, 5.4156971, 5.4937258, 5.6080642, 5.6489244, 5.6723433, 5.8403826, 5.6024566, 5.4779224, 5.5791082, 5.3525519, 5.5210218, 5.2716475, 5.33354, 5.6473851, 5.5958581, 5.1767297, 5.2461438, 5.4311767, 5.4247069, 5.4252367, 5.8227258, 5.4467463, 5.3804784, 5.4279842, 5.3822355, 5.4727125, 5.4106092, 5.2542944, 5.3635988, 5.3774586, 5.5146804, 5.3302846, 5.3224111, 5.3604493, 5.4393272, 5.1602955, 5.3591089, 5.4430299, 5.4311366, 5.5584102, 6.0705628, 5.5584655, 5.4829946, 5.3760414, 5.3872523, 5.3300548, 5.5448084, 5.4240685, 5.5612807, 5.2423925, 5.4394665, 5.702826, 5.5530052, 5.5152564, 5.4104462, 5.4325514, 5.4359908, 5.4663367, 5.2839613, 5.3016515, 5.3897567, 5.1728711, 5.1105843, 5.3502183, 5.1036391, 5.5065017, 5.2614532, 5.5839887, 5.3214936, 5.2774787, 5.0867391, 5.28298, 5.4861712, 5.2112193, 5.2376623, 5.4498267, 5.3172126, 5.3615737, 5.5827827, 5.6079674, 5.4504485, 5.2319527, 5.3481159, 5.4236741, 5.2924438, 5.3556919, 5.2491665, 5.5126476, 5.4962196, 5.2929811, 5.6281824, 5.4527254, 5.2821226, 5.2354455, 5.4692006, 5.2729168, 5.2611179, 5.3891993, 5.2119837, 5.4629836, 5.5037622, 5.3966579, 5.4360266, 5.2848358, 5.1892762, 5.407443, 5.4241371, 5.6375804, 5.7118402, 5.4384093, 5.3710499, 5.4384761, 5.3067884, 5.4555836, 5.3375549, 5.1437335, 5.4733911, 5.0125217, 5.1273088, 5.4436269, 5.572196, 5.5886507, 5.4259377, 5.6084743, 5.3482909, 5.21843, 5.4939842, 5.5785618, 5.3618426, 5.3618855, 5.3979039, 5.4426856, 5.4096065, 5.388432, 5.7803483, 5.2517853, 5.500216, 5.5671015, 5.9505243, 5.583849, 5.6100445, 5.2884312, 5.2904997, 5.3329077, 5.3923922, 5.3359222, 5.3967748, 5.1825199, 5.3906794, 5.2931895, 4.9722552, 5.5193419, 5.3788013, 5.4337358, 5.4335375, 5.3858323, 5.2261028, 5.6185637, 5.4789858, 5.2705836, 5.6370435, 5.3835521, 5.3954682, 5.1489468, 5.6496677, 5.2312441, 5.317987, 5.4785337, 5.1452312, 5.2518129, 5.1823292], 'step': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, 10100, 10200, 10300, 10400, 10500, 10600, 10700, 10800, 10900, 11000, 11100, 11200, 11300, 11400, 11500, 11600, 11700, 11800, 11900, 12000, 12100, 12200, 12300, 12400, 12500, 12600, 12700, 12800, 12900, 13000, 13100, 13200, 13300, 13400, 13500, 13600, 13700, 13800, 13900, 14000, 14100, 14200, 14300, 14400, 14500, 14600, 14700, 14800, 14900, 15000, 15100, 15200, 15300, 15400, 15500, 15600, 15700, 15800, 15900, 16000, 16100, 16200, 16300, 16400, 16500, 16600, 16700, 16800, 16900, 17000, 17100, 17200, 17300, 17400, 17500, 17600, 17700, 17800, 17900, 18000, 18100, 18200, 18300, 18400, 18500, 18600, 18700, 18800, 18900, 19000, 19100, 19200, 19300, 19400, 19500, 19600, 19700, 19800, 19900, 20000, 20100, 20200, 20300, 20400, 20500, 20600, 20700, 20800, 20900, 21000, 21100, 21200, 21300, 21400, 21500, 21600, 21700, 21800, 21900, 22000, 22100, 22200, 22300, 22400, 22500, 22600, 22700, 22800, 22900, 23000, 23100, 23200, 23300, 23400, 23500, 23600, 23700, 23800, 23900, 24000, 24100, 24200, 24300, 24400, 24500, 24600, 24700, 24800, 24900, 25000, 25100, 25200, 25300, 25400, 25500, 25600, 25700, 25800, 25900, 26000, 26100, 26200, 26300, 26400, 26500, 26600, 26700, 26800, 26900, 27000, 27100, 27200, 27300, 27400, 27500, 27600, 27700, 27800, 27900, 28000, 28100, 28200, 28300, 28400, 28500, 28600, 28700, 28800, 28900, 29000, 29100, 29200, 29300, 29400, 29500, 29600, 29700, 29800, 29900, 30000, 30100, 30200, 30300, 30400, 30500, 30600, 30700, 30800, 30900, 31000, 31100, 31200, 31300, 31400, 31500, 31600, 31700, 31800, 31900, 32000, 32100, 32200, 32300, 32400, 32500, 32600, 32700, 32800, 32900, 33000, 33100, 33200, 33300, 33400, 33500, 33600, 33700, 33800, 33900, 34000, 34100, 34200, 34300, 34400, 34500, 34600, 34700, 34800, 34900, 35000, 35100, 35200, 35300, 35400, 35500, 35600, 35700, 35800, 35900, 36000, 36100, 36200, 36300, 36400, 36500, 36600, 36700, 36800, 36900, 37000, 37100, 37200, 37300, 37400, 37500, 37600, 37700, 37800, 37900, 38000, 38100, 38200, 38300, 38400, 38500, 38600, 38700, 38800, 38900, 39000, 39100, 39200, 39300, 39400, 39500, 39600, 39700, 39800, 39900, 40000, 40100, 40200, 40300, 40400, 40500, 40600, 40700, 40800, 40900, 41000, 41100, 41200, 41300, 41400, 41500, 41600, 41700, 41800, 41900, 42000, 42100, 42200, 42300, 42400, 42500, 42600, 42700, 42800, 42900, 43000, 43100, 43200, 43300, 43400, 43500, 43600, 43700, 43800, 43900, 44000, 44100, 44200, 44300, 44400, 44500, 44600, 44700, 44800, 44900, 45000, 45100, 45200, 45300, 45400, 45500, 45600, 45700, 45800, 45900, 46000, 46100, 46200, 46300, 46400, 46500, 46600, 46700, 46800, 46900, 47000, 47100, 47200, 47300, 47400, 47500, 47600, 47700, 47800, 47900, 48000, 48100, 48200, 48300, 48400, 48500, 48600, 48700, 48800, 48900, 49000, 49100, 49200, 49300, 49400, 49500, 49600, 49700, 49800, 49900, 50000], 'percentage': [24.895833333333332, 39.557291666666664, 48.03125, 51.588541666666664, 53.921875, 55.78125, 56.338541666666664, 57.125, 57.354166666666664, 57.645833333333336, 58.197916666666664, 59.036458333333336, 59.270833333333336, 58.109375, 59.015625, 59.182291666666664, 59.270833333333336, 57.447916666666664, 58.770833333333336, 59.651041666666664, 59.557291666666664, 59.088541666666664, 58.963541666666664, 60.630208333333336, 60.854166666666664, 60.760416666666664, 60.21875, 60.390625, 60.4375, 61.838541666666664, 61.588541666666664, 60.802083333333336, 59.854166666666664, 61.197916666666664, 60.979166666666664, 60.510416666666664, 60.59375, 60.734375, 61.40625, 62.604166666666664, 61.317708333333336, 62.239583333333336, 61.838541666666664, 61.442708333333336, 62.015625, 61.359375, 61.541666666666664, 61.494791666666664, 62.171875, 61.776041666666664, 60.786458333333336, 61.9375, 61.5, 62.770833333333336, 62.734375, 62.182291666666664, 61.390625, 62.057291666666664, 61.942708333333336, 62.984375, 61.453125, 61.635416666666664, 62.010416666666664, 62.421875, 62.302083333333336, 62.703125, 62.15625, 62.510416666666664, 61.104166666666664, 61.895833333333336, 62.026041666666664, 62.661458333333336, 62.197916666666664, 62.739583333333336, 62.666666666666664, 62.822916666666664, 63.770833333333336, 62.369791666666664, 62.578125, 62.546875, 62.588541666666664, 62.578125, 62.354166666666664, 62.515625, 63.125, 62.822916666666664, 62.114583333333336, 61.505208333333336, 62.296875, 62.604166666666664, 62.0, 62.213541666666664, 62.083333333333336, 62.833333333333336, 63.005208333333336, 63.411458333333336, 62.958333333333336, 63.286458333333336, 61.791666666666664, 63.348958333333336, 62.098958333333336, 62.135416666666664, 63.036458333333336, 61.526041666666664, 61.697916666666664, 61.192708333333336, 63.114583333333336, 62.671875, 63.53125, 63.322916666666664, 62.463541666666664, 63.734375, 62.072916666666664, 62.942708333333336, 63.489583333333336, 63.046875, 63.484375, 63.84375, 63.302083333333336, 63.255208333333336, 64.015625, 63.322916666666664, 63.458333333333336, 63.291666666666664, 64.79166666666667, 63.078125, 63.1875, 64.421875, 64.453125, 62.109375, 64.10416666666667, 64.015625, 64.625, 63.765625, 64.48958333333333, 65.17708333333333, 64.41145833333333, 63.010416666666664, 62.791666666666664, 64.41666666666667, 62.833333333333336, 63.9375, 62.796875, 63.791666666666664, 63.427083333333336, 63.364583333333336, 63.552083333333336, 64.328125, 63.276041666666664, 64.33854166666667, 64.4375, 63.671875, 64.15104166666667, 64.15625, 63.21875, 63.489583333333336, 62.552083333333336, 64.11979166666667, 64.9375, 64.26041666666667, 64.21875, 63.583333333333336, 65.09375, 64.06770833333333, 63.807291666666664, 63.807291666666664, 64.46875, 64.16145833333333, 63.901041666666664, 63.96875, 64.44791666666667, 63.682291666666664, 64.88020833333333, 62.447916666666664, 64.125, 63.395833333333336, 65.03125, 63.963541666666664, 63.583333333333336, 64.43229166666667, 64.3125, 64.04166666666667, 64.33333333333333, 63.864583333333336, 64.54166666666667, 65.58333333333333, 64.46875, 64.38020833333333, 64.55208333333333, 64.0, 63.739583333333336, 64.47395833333333, 63.947916666666664, 63.255208333333336, 64.64583333333333, 64.93229166666667, 64.55729166666667, 63.442708333333336, 63.265625, 64.3125, 64.828125, 64.546875, 64.5, 65.30208333333333, 64.39583333333333, 64.66145833333333, 64.27083333333333, 64.578125, 64.09895833333333, 64.80208333333333, 65.42708333333333, 64.99479166666667, 65.0625, 63.739583333333336, 65.63541666666667, 65.50520833333333, 63.817708333333336, 64.49479166666667, 64.63541666666667, 66.0, 64.59375, 64.80208333333333, 65.6875, 64.58854166666667, 65.25, 64.609375, 64.75520833333333, 63.4375, 64.59895833333333, 64.34375, 64.33333333333333, 65.27604166666667, 64.27083333333333, 64.34895833333333, 64.08333333333333, 64.27083333333333, 65.23958333333333, 64.4375, 63.854166666666664, 64.72916666666667, 65.07291666666667, 64.90625, 63.895833333333336, 64.32291666666667, 65.33333333333333, 64.28125, 64.78645833333333, 64.32291666666667, 64.71354166666667, 64.23958333333333, 65.41145833333333, 64.85416666666667, 65.21875, 65.38541666666667, 64.93229166666667, 64.98958333333333, 65.375, 64.45833333333333, 65.09895833333333, 64.5625, 64.24479166666667, 64.26041666666667, 63.984375, 65.31770833333333, 65.84895833333333, 65.4375, 65.05208333333333, 64.61458333333333, 65.63020833333333, 65.0625, 64.88020833333333, 64.77083333333333, 65.40104166666667, 65.76041666666667, 64.61458333333333, 64.375, 65.38020833333333, 65.44270833333333, 64.55729166666667, 64.890625, 64.515625, 64.56770833333333, 64.96875, 65.9375, 65.4375, 65.13020833333333, 66.5625, 65.51041666666667, 65.625, 65.5, 64.36979166666667, 64.55729166666667, 66.00520833333333, 65.28645833333333, 64.94791666666667, 66.02604166666667, 66.546875, 65.59895833333333, 65.390625, 65.21875, 65.48958333333333, 65.84895833333333, 66.984375, 65.40104166666667, 65.890625, 66.01041666666667, 65.27083333333333, 65.91145833333333, 65.78125, 65.58854166666667, 66.19791666666667, 66.80729166666667, 65.18229166666667, 66.46354166666667, 65.40625, 65.390625, 65.953125, 65.75520833333333, 65.68229166666667, 64.95833333333333, 66.01041666666667, 65.59375, 65.94270833333333, 64.63541666666667, 65.27083333333333, 66.08854166666667, 65.28645833333333, 65.796875, 65.28645833333333, 64.61458333333333, 64.21875, 64.421875, 65.5, 65.03645833333333, 65.13541666666667, 64.72916666666667, 65.01041666666667, 64.640625, 65.51041666666667, 65.01041666666667, 65.64583333333333, 65.99479166666667, 63.989583333333336, 64.953125, 65.21875, 65.58854166666667, 65.16145833333333, 64.66145833333333, 65.5625, 65.69270833333333, 65.68229166666667, 65.22916666666667, 65.67708333333333, 65.96354166666667, 66.1875, 65.96875, 66.40625, 65.69791666666667, 65.55208333333333, 65.078125, 66.72916666666667, 65.93229166666667, 64.859375, 65.96354166666667, 65.33333333333333, 65.38541666666667, 65.08333333333333, 64.79166666666667, 65.640625, 65.234375, 64.84895833333333, 65.32291666666667, 65.69270833333333, 65.34375, 65.58333333333333, 65.81770833333333, 65.22916666666667, 65.109375, 66.453125, 66.15625, 65.80208333333333, 65.77604166666667, 65.75, 67.3125, 66.07291666666667, 66.08854166666667, 66.47395833333333, 65.42708333333333, 66.51041666666667, 65.640625, 65.75, 65.47395833333333, 65.75, 65.27604166666667, 65.95833333333333, 66.02083333333333, 66.48958333333333, 66.86979166666667, 66.1875, 67.21875, 65.859375, 65.265625, 65.484375, 66.953125, 65.66145833333333, 66.109375, 66.15625, 65.59895833333333, 65.625, 65.421875, 65.125, 64.921875, 65.4375, 66.21354166666667, 65.296875, 64.984375, 66.30729166666667, 64.91145833333333, 64.9375, 65.06770833333333, 65.47395833333333, 65.57291666666667, 66.234375, 65.44270833333333, 65.97916666666667, 67.375, 65.4375, 65.359375, 65.85416666666667, 65.890625, 65.56770833333333, 65.47916666666667, 66.71354166666667, 65.46875, 65.859375, 66.859375, 64.82291666666667, 65.609375, 65.81770833333333, 65.4375, 65.45833333333333, 65.88020833333333, 66.203125, 66.29166666666667, 66.41666666666667, 66.05729166666667, 66.02604166666667, 65.82291666666667, 65.20833333333333, 66.05729166666667, 65.80208333333333, 66.5625, 65.33333333333333, 66.8125, 66.25520833333333, 65.40625, 65.10416666666667, 65.578125, 65.796875, 65.79166666666667, 65.40104166666667, 66.67708333333333, 66.18229166666667, 65.8125, 65.75520833333333, 66.3125, 65.53125, 65.50520833333333, 65.54166666666667, 65.578125, 65.578125, 65.30208333333333, 65.55729166666667, 65.51041666666667, 65.75520833333333, 65.36979166666667, 65.859375, 66.1875, 66.14583333333333, 66.140625, 66.03645833333333, 66.71875, 66.5, 65.921875, 65.93229166666667, 67.328125, 67.09895833333333, 66.79166666666667, 66.19791666666667, 65.4375, 65.38541666666667, 65.90104166666667, 66.89583333333333, 66.39583333333333, 66.390625, 65.05729166666667, 65.65104166666667, 66.23958333333333, 66.109375, 66.046875], 'BPC': [3.4859221977483976, 2.8211667724488789, 2.5996517422763747, 2.3629549435989556, 2.1672963011051185, 2.1981967679509413, 2.0993072596527789, 1.9321996290113272, 2.0844703163628444, 2.0988766150950182, 1.9963897456887585, 2.0102403684759946, 1.9290317085824344, 1.9431153678615898, 1.9798976413669203, 1.9383827492751469, 2.0021368899326411, 1.9959575532871403, 1.9490508331081671, 1.956963410909063, 1.9580008102590676, 1.9257423684020323, 1.932991093170598, 2.0053847822941853, 1.8674696587928377, 1.8752370832364056, 1.9189581687734472, 1.9563895048031339, 1.931762449112834, 1.7910228545363667, 1.8614697000521869, 1.8878585460179254, 1.8270432448983751, 1.8666859338528559, 1.8335183917033597, 1.9111651189149006, 1.8223099383813288, 1.8804730950419237, 1.7621146347181389, 1.8820807888621893, 1.9548949755671361, 1.8538986797959043, 1.8522125618868452, 1.7581236053222165, 1.8227457424186151, 1.8714605162061091, 1.8440559407043748, 1.7869580445832243, 1.7346723950158658, 1.7322378086102517, 1.8746795154823084, 1.7692694569593335, 1.792239631791221, 1.7914469637533943, 1.8120030181151208, 1.8492020055834919, 1.9497666249010579, 1.8266538761768216, 1.8685849662836831, 1.7344842459958218, 1.7901106585561903, 1.8105418535133904, 1.7352607476644673, 1.7581039993000183, 1.8844596528889237, 1.8446967480614898, 1.876392290702251, 1.8110170415777243, 1.7666126689688049, 1.8583017796232939, 1.7435313933444598, 1.8154089625328078, 1.8450532680967284, 1.722331607920554, 1.6852285867713643, 1.7547969449065772, 1.7672606995972557, 1.8056374242587356, 1.7993143101171136, 1.8140717974223495, 1.717947942132712, 1.7288848348490304, 1.7530556205665913, 1.8508356687840377, 1.7227483218835946, 1.7087902099395691, 1.749651395975413, 1.8404556558912128, 1.7828223777779262, 1.7685423143114856, 1.8048629863819003, 1.7761360362776819, 1.7500166871258456, 1.6944932921733438, 1.7767333320241285, 1.7147230954463832, 1.7251221984134539, 1.7268587072392156, 1.8025364050810282, 1.7065869401993656, 1.8477623388131192, 1.7832189697708158, 1.7160444381529623, 1.7320976427497985, 1.7988705948778874, 1.7910051403233278, 1.7342844021555193, 1.7697694105253916, 1.6707486794646209, 1.759567227693559, 1.7493354638457779, 1.7947008755077187, 1.7796448263205848, 1.7545291679191839, 1.691197244669558, 1.7958020804211932, 1.8188413922787277, 1.6643099929816192, 1.8490104169104307, 1.6769758272870614, 1.6860602948709367, 1.6414165224120163, 1.7893783564288164, 1.7566364713402058, 1.7006237857459994, 1.7575981983238307, 1.6844281795142488, 1.7350937525104795, 1.7213750404164549, 1.783851865925991, 1.7469153239828363, 1.6875322943796716, 1.7909765912032845, 1.7260570961035442, 1.6873030415060715, 1.7294726715496795, 1.6735177721261629, 1.6821478615464587, 1.7332160459283594, 1.7225010108316539, 1.8016603254575332, 1.697980584384889, 1.7664735350042569, 1.719348052893386, 1.706932281362298, 1.7246158814893138, 1.717793501712237, 1.6874000397211582, 1.6888741030217032, 1.7399164900059745, 1.6219233208326223, 1.7278794242720865, 1.7086679302748056, 1.7059402854321215, 1.6734942105029949, 1.7068996046586342, 1.7225066862591323, 1.7006994581123791, 1.6440898207370278, 1.6982616040363987, 1.6654476582170765, 1.7564378313784592, 1.7424759358161148, 1.6952883679682824, 1.73903456297235, 1.7378809033503622, 1.6652082583670755, 1.649462558749982, 1.6914865194883093, 1.7072944768250151, 1.7464299889421016, 1.6494169833475034, 1.7157596348831334, 1.7019759133470829, 1.7075969943078828, 1.735835341701, 1.6767365994197114, 1.6831161238708163, 1.6842472537655411, 1.7030982721266135, 1.6495733155771377, 1.6955588966780897, 1.7256037498358698, 1.7015033050225119, 1.708207360735795, 1.7278515630826468, 1.7113775169391492, 1.6478276916708801, 1.6779934486322168, 1.7475225947230331, 1.7213502749147307, 1.7238844392751942, 1.7389822802464876, 1.7735982602815503, 1.6332442508083174, 1.6934416182633176, 1.7070819062685487, 1.704130511997092, 1.6608687921205052, 1.6682201905316358, 1.6972348676107478, 1.6596394601321378, 1.6903082663472482, 1.6817423264557243, 1.6598472151743802, 1.6903952895685848, 1.6553433335135848, 1.6039762432844846, 1.7945412756077179, 1.6631240006212691, 1.6645411376643786, 1.7477103997777752, 1.7021950192442821, 1.6493301321088176, 1.652108855798808, 1.5851634050719037, 1.7039758995939664, 1.6904776692583481, 1.6360570270631787, 1.6498675778927638, 1.6706253679039522, 1.6771020625527948, 1.6601029533762131, 1.697084210808592, 1.6702865620817526, 1.6812225948848167, 1.6747643023796166, 1.643465179749094, 1.5895575618014484, 1.6789403851078675, 1.6698151576357376, 1.721432654604494, 1.7454285339661275, 1.6836281162224349, 1.679234131475541, 1.7990628714815522, 1.6862637503469076, 1.7028930968241343, 1.7370796361799925, 1.6707916751273366, 1.7141598522648076, 1.6886090777567235, 1.6495533655896375, 1.8079548904791118, 1.6302835694737141, 1.6583265445754511, 1.653237233971119, 1.6723914577456624, 1.6686964104918751, 1.665180053212334, 1.620317690804167, 1.6822410761432265, 1.6545609844348099, 1.6188382960414451, 1.6964231094986753, 1.6294953729848098, 1.6711155184589113, 1.6280851152477349, 1.6527386562662676, 1.6889277616087723, 1.6859975212033718, 1.6455736870486721, 1.7037870626433191, 1.6917121607262413, 1.6424195252318483, 1.6213829513436111, 1.7031969901682087, 1.5760510762985886, 1.5684107470340083, 1.6372583258794553, 1.6224401286984649, 1.6387108913486426, 1.6489954538702385, 1.6124943719990688, 1.615354615465568, 1.6872124066490668, 1.7264339100915846, 1.6198069023311044, 1.6223505257373654, 1.7053579521763, 1.6815801468159606, 1.6658734872606129, 1.6859655324303113, 1.6609356933716908, 1.6974950773615032, 1.6123634932017621, 1.5879264783406657, 1.5868816837366739, 1.6055774017640174, 1.5990260666447005, 1.5791873519197228, 1.7303910589052869, 1.60082792887779, 1.6464157141072964, 1.6233380501186196, 1.5605944795175954, 1.6639713591420702, 1.5972436384511584, 1.6229645038009455, 1.668502242079051, 1.5949179170635406, 1.6518538055275784, 1.5714218192853144, 1.71156308621943, 1.6462046913946877, 1.6634088038910979, 1.6421280146386357, 1.6817970169386987, 1.6309869785157429, 1.6112359749427054, 1.5633138691930382, 1.6066055140508753, 1.7130283784047813, 1.6519229425532254, 1.6437052675296986, 1.5643789577498315, 1.5399519178824852, 1.7069685697016301, 1.6453544091688221, 1.6256402098830691, 1.6609556433591908, 1.6370180661161999, 1.6943486547639681, 1.6162779903180504, 1.6046514471717721, 1.5738006833120488, 1.6434469495881026, 1.712047045398958, 1.6291796128378258, 1.6040598268528039, 1.6247106436551557, 1.6785907443786632, 1.702052101661415, 1.6910954309402473, 1.7268041887388921, 1.6873169721007912, 1.6241613310682999, 1.6545451620309306, 1.6098183219516433, 1.5930312673835754, 1.6225973208413536, 1.6041227725030198, 1.7341346052666178, 1.6264163675864129, 1.5577559058651047, 1.5625053787513321, 1.6471093201382261, 1.6358929556142554, 1.6483565383222833, 1.6760017175525745, 1.6325415296968919, 1.5806995953687595, 1.6380408469408811, 1.6612505936054205, 1.6064111736554003, 1.6167562740701, 1.5648572415018811, 1.5700476779049206, 1.5942946519368137, 1.6019055721680964, 1.5937503268468329, 1.5700635003087999, 1.5598135063000274, 1.6364355608777277, 1.5695298381431726, 1.5745178509661466, 1.6349331204397901, 1.6000978625248774, 1.645904753651583, 1.7059193035487161, 1.6713839833769082, 1.625131485201817, 1.5949122416360619, 1.5998529592300488, 1.5981394960795026, 1.5879746334829072, 1.6556781837348147, 1.6368880472321476, 1.5838343231460359, 1.6027892190282296, 1.6252680394266021, 1.5971045044866101, 1.6251111912490153, 1.5859853101603771, 1.6554104067474213, 1.5982507688546108, 1.6327413735371945, 1.5813498617716717, 1.570660280107294, 1.579845357541924, 1.5771065338269337, 1.5119696526565027, 1.5828983935600405, 1.5817144649915009, 1.6494245505841414, 1.5756730584319922, 1.6135188726502585, 1.537621724945945, 1.5392473049619002, 1.5476790983857511, 1.5632003606434688, 1.5858325895664109, 1.5213225851583749, 1.554140314596016, 1.6650319761499413, 1.5734599856806897, 1.5954245779529823, 1.6237398015910351, 1.6685318230949993, 1.6062151134334166, 1.5919649749482261, 1.6189664231163379, 1.6091806102822441, 1.5593034057575683, 1.5779174320257519, 1.6339287417587514, 1.6880348276854926, 1.6241259026422221, 1.6073259493753396, 1.6621543624357047, 1.6739599395215312, 1.5399894101003733, 1.5464604293217372, 1.6234408957438355, 1.6463546602662402, 1.5975666218694786, 1.6036797451943972, 1.6547947088573325, 1.6034255548364218, 1.6767981692087202, 1.6314840083767366, 1.6188639214564235, 1.6362360610027269, 1.5381937392427145, 1.6175449865069567, 1.6335135756395684, 1.6264136158639992, 1.6737523564619399, 1.5987825392110786, 1.6103330660256758, 1.6146018474027415, 1.569111920301576, 1.6048208500828718, 1.5849147181587562, 1.5936000140099789, 1.6223290279060074, 1.5486203594339232, 1.5374781194324745, 1.6537266965954744, 1.6985061633659257, 1.6099667429793381, 1.5696182372257161, 1.6222851723300376, 1.6365678155362411, 1.5566318272590656, 1.6499671558476134, 1.6444427291365984, 1.56261458773463, 1.6244837985386675, 1.6238756678852166, 1.5736419433253024, 1.5973347892561156, 1.6194859827045944, 1.6608192611170567, 1.612419903511245, 1.6427562672622376, 1.661360318536671, 1.6898009175272029, 1.6006241294365176, 1.620319066665374, 1.6339794766407558, 1.678762727029526, 1.5777354743811389, 1.5575784197694145, 1.6090842999977608, 1.6088015605197425, 1.5373401893464824, 1.5897955857902426, 1.6442662749368131, 1.5293646658953706, 1.6203821842982404, 1.564712948057807, 1.6413611439984386, 1.6061640345861103, 1.5667578217765659, 1.565126738315783, 1.6732537787570887, 1.6554061071811497, 1.5724096876318705, 1.6583967134970032, 1.5909593923886312, 1.5493043344364046, 1.5080415689107962, 1.6329047570555142, 1.5819182644327734, 1.610632315838177, 1.6222684900129039, 1.5687404377757124, 1.5869159082841957, 1.5643342422606072]}}}\n"
     ]
    }
   ],
   "source": [
    "print(results_GL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
