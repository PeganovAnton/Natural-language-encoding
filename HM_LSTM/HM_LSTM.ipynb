{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "import getpass\n",
    "if not os.path.isfile('model_module.py') or not os.path.isfile('plot_module.py'):\n",
    "    current_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    additional_path = '/'.join(current_path.split('/')[:-1])\n",
    "    sys.path.append(additional_path)\n",
    "    \n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56184664\n"
     ]
    }
   ],
   "source": [
    "f = open('enwik8_clean', 'rb')\n",
    "text = f.read().decode('utf8')\n",
    "print(len(text))\n",
    "f.close() \n",
    "(not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\t\n",
      " !\"'(),-.?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "print(vocabulary)\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(string_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix,\n",
    "                keep_dims=True):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\")\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=keep_dims,\n",
    "                                     name=\"reduce_mean_in_L2_norm\")\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\")\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.multiply(state[2],\n",
    "                                             top_down,\n",
    "                                             name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.multiply(boundary_state_down,\n",
    "                                              bottom_up,\n",
    "                                              name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            boundary_state_reversed = tf.subtract(one, state[2], name=\"boundary_state_reversed\")\n",
    "            state0_prepaired = tf.multiply(boundary_state_reversed,\n",
    "                                           state[0],\n",
    "                                           name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate, None, 'forget_gate_layer%s' % idx, keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                  [[0.]],\n",
    "                                                                  name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                         tf.equal(boundary_state_down,\n",
    "                                                                  [[1.]],\n",
    "                                                                  name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                         name=\"logical_and_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                [[0.]],\n",
    "                                                                name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                       tf.equal(boundary_state_down,\n",
    "                                                                [[0.]],\n",
    "                                                                name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                       name=\"logical_and_in_copy_flag\"),\n",
    "                                        name=\"copy_flag\")\n",
    "                flush_flag = tf.to_float(tf.equal(state[2],\n",
    "                                                  [[1.]],\n",
    "                                                  name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                         name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                memory = state[1]\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(forget_gate,\n",
    "                                                             memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(input_gate,\n",
    "                                                             modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(input_gate,\n",
    "                                                     modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                new_memory = tf.add(tf.add(update_term,\n",
    "                                           copy_term,\n",
    "                                           name=\"add_update_and_copy_in_new_memory\"),\n",
    "                                    flush_term,\n",
    "                                    name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                hidden = state[0]\n",
    "                copy_term = tf.multiply(copy_flag, hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    output_gate,\n",
    "                                                    name=\"multiply_subtract_and_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.add(copy_term, else_term, name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg,\n",
    "                          \"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.multiply(boundary_state_down,\n",
    "                                              bottom_up,\n",
    "                                              name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate,\n",
    "                                          None,\n",
    "                                          \"forget_gate_layer%s\"%(self._num_layers-1),\n",
    "                                          keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                   1.,\n",
    "                                                   name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                          name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                memory = state[1]\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(forget_gate,\n",
    "                                                             memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(input_gate,\n",
    "                                                             modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, memory, name=\"copy_term\")\n",
    "                new_memory = tf.add(update_term,\n",
    "                                    copy_term,\n",
    "                                    name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                hidden = state[0]\n",
    "                copy_term = tf.multiply(copy_flag, hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    output_gate,\n",
    "                                                    name=\"multiply_subtract_and_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.add(copy_term, else_term, name=\"new_hidden\")\n",
    "                helper = {\"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, helper\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": self.gradient_name}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "        \"\"\"X = tf.sign(X)\"\"\"\n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\")\n",
    "        return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "            helpers = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "\n",
    "            hidden = inp\n",
    "            boundary = activated_boundary_states\n",
    "            # All layers except for the first and the last ones\n",
    "            for idx in range(num_layers-1):\n",
    "                hidden, memory, boundary, helper = self.not_last_layer(idx,\n",
    "                                                                       state[idx],\n",
    "                                                                       hidden,\n",
    "                                                                       state[idx+1][0],\n",
    "                                                                       boundary)\n",
    "                helpers.append(helper)\n",
    "                new_state.append((hidden, memory, boundary))\n",
    "                boundaries.append(boundary)\n",
    "            hidden, memory, helper = self.last_layer(state[-1],\n",
    "                                                     hidden,\n",
    "                                                     boundary)\n",
    "            helpers.append(helper)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\"),\n",
    "                      \"L2_forget_gate\": tf.stack([helper[\"L2_forget_gate\"] for helper in helpers],\n",
    "                                                 name=\"L2_forget_gate_for_iteration%s\"%iter_idx)}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm,\n",
    "                      \"L2_forget_gate\": tf.stack([helper['L2_forget_gate'] for helper in iteration_helpers],\n",
    "                                                 name=\"L2_forget_gate_all\")}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        # hidden_states is list of hidden_states by layer, concatenated along batch dimension\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.sigmoid(tf.matmul(concat,\n",
    "                                                       self.output_module_gates_weights,\n",
    "                                                       name=\"matmul_in_output_module_gates\"),\n",
    "                                             name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=1,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                       hidden_state,\n",
    "                                                       name=\"gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.concat(gated_hidden_states,\n",
    "                                            1,\n",
    "                                            name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "            \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1.,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "                 matr_init_parameter=1000.,\n",
    "                 override_appendix='',\n",
    "                 init_bias=0.):               \n",
    "                                                   \n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter\n",
    "        self._init_bias = init_bias\n",
    "        self.gradient_name = 'HardSigmoid' + override_appendix\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"matr_init_parameter\": 14,\n",
    "                         \"init_bias\":15,\n",
    "                         \"type\": 16}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            tf.set_random_seed(1)\n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                \n",
    "                def compute_dim_and_bias(layer_idx):\n",
    "                    bias_init_values = [0.]*(4*self._num_nodes[layer_idx])\n",
    "                    if layer_idx == self._num_layers - 1:\n",
    "                        input_dim = self._num_nodes[-1] + self._num_nodes[-2]\n",
    "                        output_dim = 4 * self._num_nodes[-1]\n",
    "                    else:\n",
    "                        output_dim = 4 * self._num_nodes[layer_idx] + 1\n",
    "                        bias_init_values.append(self._init_bias)\n",
    "                        if layer_idx == 0:\n",
    "                            input_dim = self._embedding_size + self._num_nodes[0] + self._num_nodes[1]\n",
    "                        else:\n",
    "                            input_dim = self._num_nodes[layer_idx - 1] + self._num_nodes[layer_idx] + self._num_nodes[layer_idx+1]\n",
    "                    stddev = math.sqrt(self._init_parameter*matr_init_parameter/input_dim)\n",
    "                    return input_dim, output_dim, bias_init_values, stddev\n",
    "                \n",
    "                for layer_idx in range(self._num_layers):\n",
    "                    input_dim, output_dim, bias_init_values, stddev = compute_dim_and_bias(layer_idx)\n",
    "                    self.Biases.append(tf.Variable(bias_init_values,\n",
    "                                                   name=bias_name%layer_idx))         \n",
    "                    self.Matrices.append(tf.Variable(tf.truncated_normal([input_dim,\n",
    "                                                                          output_dim],\n",
    "                                                                         mean=0.,\n",
    "                                                                         stddev=stddev,\n",
    "                                                                         name=init_matr_name%0),\n",
    "                                                     name=matr_name%layer_idx))    \n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_embedding_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "                    train_inputs_for_slice = tf.stack(train_inputs,\n",
    "                                                      axis=1,\n",
    "                                                      name=\"train_inputs_for_slice\")\n",
    "                    self.train_input_print = tf.reshape(tf.split(train_inputs_for_slice,\n",
    "                                                                 [1, self._batch_size-1],\n",
    "                                                                 name=\"split_in_train_print\")[0],\n",
    "                                                        [self._num_unrollings, -1],\n",
    "                                                        name=\"train_print\")\n",
    "\n",
    "\n",
    "                    self.saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 0)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 0)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 1)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 1)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                      name=saved_state_init_templ%(i, 2)),\n",
    "                                                             trainable=False,\n",
    "                                                              name=saved_state_templ%(i, 2))))\n",
    "                    self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                             tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "                    \n",
    "                    @tf.RegisterGradient(self.gradient_name)\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, self.saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "                    \n",
    "                    self.train_hard_sigm_arg = tf.reshape(tf.split(train_helper[\"hard_sigm_arg\"],\n",
    "                                                                   [1, self._batch_size-1],\n",
    "                                                                   name=\"split_in_train_hard_sigm_arg\")[0],\n",
    "                                                          [self._num_unrollings, -1],\n",
    "                                                          name=\"train_hard_sigm_arg\")\n",
    "                    \n",
    "                    L2_forget_gate_reduced = tf.reduce_mean(train_helper['L2_forget_gate'],\n",
    "                                                            axis=0,\n",
    "                                                            name=\"L2_forget_gate_reduced\")\n",
    "                    \n",
    "                    self.L2_forget_gate = tf.unstack(L2_forget_gate_reduced, name=\"L2_forget_gate_unstacked\")\n",
    "                    \n",
    "                    flush_fractions_stacked = tf.reduce_mean(train_helper['all_boundaries'],\n",
    "                                                             axis=[0, 1],\n",
    "                                                             name='flush_fractions_stacked')\n",
    "                    \n",
    "                    self.flush_fractions = tf.split(flush_fractions_stacked,\n",
    "                                                    self._num_layers-1,\n",
    "                                                    axis=0,\n",
    "                                                    name='flush_fractions')\n",
    "                    \n",
    "                    L2_hard_sigm_arg_stacked = self.L2_norm(train_helper['hard_sigm_arg'],\n",
    "                                                            [0, 1],\n",
    "                                                            'hard_sigm_arg_stacked',\n",
    "                                                            keep_dims=False)\n",
    "                    \n",
    "                    self.L2_hard_sigm_arg = tf.split(L2_hard_sigm_arg_stacked,\n",
    "                                                     self._num_layers-1,\n",
    "                                                     name='hard_sigm_arg')\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(self.saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    #optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    print_list = tf.split(self._train_prediction, self._num_unrollings, name=\"print_list\")\n",
    "                    print_for_slice = tf.stack(print_list, axis=1, name=\"print_for_slice\")\n",
    "                    self.train_output_print = tf.reshape(tf.split(print_for_slice,\n",
    "                                                                  [1, self._batch_size-1],\n",
    "                                                                  name=\"split_in_train_print\")[0],\n",
    "                                                         [self._num_unrollings, -1],\n",
    "                                                         name=\"train_print\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "                    \n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "                # creating control dictionary\n",
    "                all_vars = tf.global_variables()\n",
    "                self.control_dictionary = dict()\n",
    "                #print('building graph')\n",
    "                for variable in all_vars:\n",
    "                    \n",
    "                    list_to_form_name = variable.name.split('/')\n",
    "                    if ':' in list_to_form_name[-1]:\n",
    "                        list_to_form_name[-1] = list_to_form_name[-1].split(':')[0]\n",
    "                    if len(list_to_form_name) < 2:\n",
    "                        name = list_to_form_name[0] \n",
    "                    else:\n",
    "                        name = list_to_form_name[0] + '_' + list_to_form_name[-1]\n",
    "                    norm = self.L2_norm(tf.to_float(variable,\n",
    "                                                    name=\"to_float_in_control_dictionary_for_\"+list_to_form_name[-1]),\n",
    "                                        None,\n",
    "                                        list_to_form_name[-1],\n",
    "                                        keep_dims=False)\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        self.control_dictionary[name] = tf.summary.scalar(name+'_sum', \n",
    "                                                                          norm)\n",
    "                    #print(name, ': ', norm.get_shape().as_list())\n",
    "            forget_template = 'self.L2_forget_gate[%s]'\n",
    "            flush_fractions_template = 'self.flush_fractions[%s]'\n",
    "            L2_hard_sigm_template = 'self.L2_hard_sigm_arg[%s]'\n",
    "            for layer_idx in range(self._num_layers):\n",
    "                self.control_dictionary[forget_template % layer_idx] = tf.summary.scalar(forget_template % layer_idx +'_sum', \n",
    "                                                                                         self.L2_forget_gate[layer_idx])\n",
    "            for layer_idx in range(self._num_layers-1):\n",
    "                self.control_dictionary[flush_fractions_template % layer_idx] = tf.summary.scalar(flush_fractions_template % layer_idx +'_sum', \n",
    "                                                                                                  tf.reshape(self.flush_fractions[layer_idx],\n",
    "                                                                                                             [],\n",
    "                                                                                                             name='reshaping_flush_fractions_%s'%layer_idx))\n",
    "                self.control_dictionary[L2_hard_sigm_template % layer_idx] = tf.summary.scalar(L2_hard_sigm_template % layer_idx +'_sum', \n",
    "                                                                                               tf.reshape(self.L2_hard_sigm_arg[layer_idx],\n",
    "                                                                                                          [],\n",
    "                                                                                                          name='reshaping_L2_hard_sigm_%s'%layer_idx)) \n",
    "            self.control_dictionary['loss'] = tf.summary.scalar('loss_sum', self._loss) \n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "       \n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._matr_init_parameter)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n"
     ]
    }
   ],
   "source": [
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 10,\n",
    "                 3,\n",
    "                 [128, 128, 128],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-99c379a65188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"HM_LSTM/logging/first_summary_log\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.run(1,                # number of times learning_rate is decreased\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# a factor by which learning_rate is decreased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;31m# each 'train_frequency' steps loss and percent correctly predicted letters is calculated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=201,\n",
    "            add_operations=['self.train_hard_sigm_arg', 'self.flush_fractions', 'self.L2_hard_sigm_arg', 'self.L2_forget_gate'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200],\n",
    "            path_to_file_for_saving_collection='first.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='plotting_check.txt')\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'first.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f0f1ca6ab381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpickle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'first.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msave\u001b[0m  \u001b[0;31m# hint to help gc free up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'first.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_file = 'first.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    save  # hint to help gc free up memory\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_parameter_value = 1e-6\n",
    "matr_init_parameter_value = 10000\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:peganov/HM_LSTM/folder_name/name_of_run/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 17.49%     Time = 18s     Learning rate = 0.0009\n"
     ]
    }
   ],
   "source": [
    "model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ 'folder_name' +'/'+'name_of_run'+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 18.29095699999999, 'metadata': [64, 30, 3, [128, 128, 128], 10, 0.8, 100, 100, 0.5, 0.5, 1000, 128, 1024, 1e-06, 10000, 'HM_LSTM'], 'data': {'train': {'perplexity': [24.632861709594728], 'BPC': [4.5201946005579066], 'step': [-1], 'percentage': [17.48645833333333]}, 'validation': {'perplexity': [24.349007891654967], 'BPC': [4.355342844963074], 'step': [-1], 'percentage': [17.8]}}}\n"
     ]
    }
   ],
   "source": [
    "print(model._results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           init parameter:  1e-05\n",
      "      matr init parameter:  50\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 24.06%     Time = 17s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      matr init parameter:  100\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 14.06%     Time = 18s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables\n",
      "      matr init parameter:  1000\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 13.94%     Time = 16s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables\n",
      "      matr init parameter:  10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5956ab78e8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                          \u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# a factor by which the learning rate decreases each 'half_life'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                          \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                          fixed_num_steps=True)\n\u001b[0m\u001b[1;32m     38\u001b[0m         text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n\u001b[1;32m     39\u001b[0m                                                 \u001b[0;34m'peganov/HM_LSTM/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname_of_run\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/variables'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36msimple_run\u001b[0;34m(self, num_averaging_iterations, save_path, min_num_steps, loss_frequency, block_of_steps, num_stairs, decay, stop_percent, save_steps, optional_feed_dict, half_life_fixed, fixed_num_steps)\u001b[0m\n\u001b[1;32m    424\u001b[0m                                     \u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                                     fixed_num_steps=fixed_num_steps)\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mdata_for_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_percentages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_averaging_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_num_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mGLOBAL_STEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_num_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36mcalculate_percentages\u001b[0;34m(self, session, num_averaging_iterations)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_unrollings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_parameters = [1e-5, 1e-6, 1e-7, 1e-8]\n",
    "matr_init_parameters = [50, 100, 1000, 10000, 100000]\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "results_GL = list()\n",
    "run_idx = 0\n",
    "for init_parameter_value in init_parameters:\n",
    "    print(' '*10, \"init parameter: \", init_parameter_value)\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        print(' '*5, \"matr init parameter: \", matr_init_parameter_value)\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        folder_name = 'nn%sis%ssg%sshl%s' % (num_nodes, init_slope, slope_growth, slope_half_life)\n",
    "        model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value,\n",
    "                        override_appendix=str(run_idx))\n",
    "        model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)\n",
    "        text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                                                [10, 75, None])\n",
    "        for i in range(4):\n",
    "            text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', folder_name, name_of_run, 'plots'],\n",
    "                            name_of_run+'#%s' % i,\n",
    "                            show=False)\n",
    "        results_GL.append(model._results[-1])\n",
    "        run_idx += 1\n",
    "        model.destroy()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9bb3de01da55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ops'"
     ]
    }
   ],
   "source": [
    "print(tf.ops._gradient_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/server/thirteenth'\n",
    "pickle_file = 'thirteenth.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory\n",
    "model._results = results_GL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_GL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-313904d4aa83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_GL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_GL' is not defined"
     ]
    }
   ],
   "source": [
    "print(results_GL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_all([0],\n",
    "               plot_validation=True,\n",
    "               indent=1,\n",
    "               save_folder='HM_LSTM/server/thirteenth',\n",
    "               show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling test_folder/plot_debug.pickle\n"
     ]
    }
   ],
   "source": [
    "results_GL = model._results\n",
    "folder_name = 'test_folder'\n",
    "file_name = 'plot_debug.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = '/home/anton/Natural-language-encoding/HM_LSTM_res/server/HM_LSTM3/HM_LSTM3_init'\n",
    "pickle_file = 'HM_LSTM3_init.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAEkCAYAAAD6he+BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl5mZmVnJXHCZmZmZlcwFl5mZmVnJ/g+H3y/D+nVhDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1d5c3c6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['HM_LSTM3', 'init_tuning', 'plots'],\n",
    "                    'nn128is0.5sg0.5shl1000',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "var = 'aaa'\n",
    "if isinstance(var, str):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def split_to_path_name(path):\n",
    "        parts = path.split('/')\n",
    "        name = parts[-1]\n",
    "        path = '/'.join(parts[:-1])\n",
    "        return path, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "bbbb.pickle\n"
     ]
    }
   ],
   "source": [
    "path, name = split_to_path_name('bbbb.pickle')\n",
    "print(path == '')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'peganov/HM_LSTM_fixed/track_nan'\n",
    "pickle_file = 'track_nan.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    five_input = dictionary['self.train_input_print'][0]\n",
    "    ten_input = dictionary['self.train_input_print'][1]\n",
    "    five_boundaries = dictionary['self.boundaries_for_plot'][0]\n",
    "    ten_boundaries = dictionary['self.boundaries_for_plot'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_boundaries(nparray):\n",
    "    nparray[nparray > 0.] = 1.\n",
    "    nparray[nparray < 0.] = 0.\n",
    "    return nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_double_list(nparray):\n",
    "    length = nparray.shape[0]\n",
    "    double_list = [list(), list()]\n",
    "    for i in range(length):\n",
    "        double_list[0].append(nparray[i, 0])\n",
    "        double_list[1].append(nparray[i, 1])\n",
    "    return double_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "five_boundaries = transform_to_double_list(five_boundaries)\n",
    "ten_boundaries = transform_to_double_list(ten_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      " towards large businesses and \n",
      "leInsider and PowerPage, were \n"
     ]
    }
   ],
   "source": [
    "print(five_boundaries)\n",
    "print(ten_boundaries)\n",
    "print(five_input)\n",
    "print(ten_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import text_boundaries_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAABFCAYAAADerkV1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADO5JREFUeJzt3XvQXVV5x/Hvr0TAcBMwEFpJFJWLSBVvhOtYKNAqRS4O\nlaJVaTsj41CKylClgkx1wKAV0KrghRQQUfFuHSWtUDBGh0tqAkjQIOFmSJB7SBrC+/SPtV7ftQ/v\neXMu+1xy8vvM7HnP2Wfttdfe6zn7PHvtfc6riMDMzMxs3B8NugFmZmY2XJwcmJmZWYWTAzMzM6tw\ncmBmZmYVTg7MzMyswsmBmZmZVTg5MNuISbpO0sWDboeZjRYnB2Z91IMP82OBD3ZTgaSZkr4i6VeS\n1kv6cpNyx0u6XdJaSbdJOmaSMh+R9ICkp/O2vqLh9RdIukLSY3m6XNJ2DWVeKen6XMd9kj7czfaZ\nWfucHJgNIUnTWikXEY9FxOouV7cFsAo4D/h5k/bsD1wNXAG8CrgK+Iak1xdlzgROB94LvA5YCcyX\ntFVR1VeBVwNHAEcCrwEuL+rYBpgP/A54LXAacIak07vcRjNrg/wLiWb9Ieky4J1AAMp/X5Kn64A3\nAx8hffgeB9wJ/BuwH7ANsBQ4OyL+s6jzOmBJRPxjfv5b4IvArsCJwBPARRHxiRbb+H1gVUSc3DD/\namD7iDiymDcfWBkRJ+XnDwIXR8T5+fmWpATh/RHxBUl7AbcDB0TEz3OZA4EbgT0i4teSTiElKTtF\nxLpc5izgPRGxayvbYGbd88iBWf+cBiwELgN2BnYB7itePx84C9gT+AWwNfBD4DDgT4FrgG9K2n0D\n6/knYDGwL/BxYK6k/bps+/7AtQ3zfgwcACBpN2Am6awfgIhYC9wwXibX8eR4YpDLLABWF2XmADeO\nJwbFev5Y0uwut8HMWuTkwKxPIuIJYB3wdESsioiVUR26Oyci/isi7omI30fE4oi4NCLuiIi7I+I8\nYBHw1g2s6tqI+Gxe5jPAb0gJRjdmAg81zHsoz4eU7EQLZVZNUvfKokyz9agoY2Y95uTAbDgEcEs5\nQ9J0SXPzTYCPSHqSdB1+1gbqWtzw/EFgp/qaamajrqWbnsysLxpvLPwk6ca995PO/p8m3RC4+Qbq\neabhedD9icAK0pl/aec8f/x15Xn3T1FmxiR179RQZrL1RFHGzHrMIwdm/bUO2KzFsgcCl0fEdyLi\nNtIIwEt71rKpLQQOb5h3OPAzgIj4LenD+w9l8g2JBwMLijq2ljSnKHMAMH28nlzmYEllAnQE8GBE\nLK9ta8xsSk4OzPrrHuANkmZL2lGS8nxNUvYu4FhJ+0rahzRqsEUvGiXpVZJeDWwL7JCf71UUuQg4\nVNKZkvaQ9EHgjcCnijIXAmdKOlbSK4F5wJOkry8SEXeSbi68RNKc/PXIzwPfj4hf5zquIo2QzJO0\nt6TjgDNJoyhm1ie+rGDWX58gfWjeAWxJ+hojpGHzRu8jfS3xBuBR0odvY3LQuNxk9bTyfeVFDeX+\nClgO7AYQEQslvQ34KHAusAw4ISJu/sNKIubm0YLPANuTvnFxRMPvMJwIfBr4UX7+XeDUoo4nJB0O\n/DtwE2m7L4iIC1vYBjOriX/nwMzMzCp8WcHMzMwqnByYmZlZxZT3HEjyNQczMwOgjsvQE/fg2rCI\niOd0im9INDOzlviDfdPh5MDMzFrikYNNh+85MDMzswonB2ZmZlbh5MDMzMwqnByYmZlZhZMDMzMz\nq3ByYGZmZhVODszMzKzCyYGZmZlV+EeQbGD8gypV3h+2Keg2zh3jVb3anx45MDMzswonB2ZmZlbh\n5MDMzMwqnByYmZlZhZMDMzMzq3ByYGZmZhVODszMzKzCyYGZmZlVODkwMzOzCicHZmZmVqGpfnpR\nUve/52pmZmZDKyKe8xvKUyYHZmZmtunxZQUzMzOrcHJgZmZmFU4OzMzMrKLj5EDSZZLG8jSrzka1\n2Y5zinacPah2dEvSvLr3p6Trh6GPbEI/+6ThvTE+rZe0UtL3JB3Uy/WPGu9P25TUMXIwLHc0Dks7\nOhV5Gqu5zvKvDd4g+iSKScCOwFHA9ZLe0sd2jArvTxt5vqwwJCLi3RGxWURMi4h7B92euknaYtBt\n2MSdGxGbAdsBl+R5Aj45uCYNjw7i0/vTWrYxHv8GlhxImiPp25JWSFon6YF8qWJ2n9ZfXhbZX9KV\nkh6R9LCkayTt3GI9L5Z0uaTlktZIelTSklz/C9toT1eXFSSdLOmu3IZbJR3RQR1d90nDfj0o78vH\ngDvabU8nJJ0gab6keyWtlrRW0jJJn5O0U4t11BUbXfdJ3SLiKeCs/FTASyTtMNUykg6W9N08fL5O\n0u8kfVXSPlMsc3SxD/++mH9vnndNMe8Led6zknaZos6hi88O92cdMfpOSUtzbC2SdKTavGQ16GNw\nN0Y5voamXyKiowm4jDQE/iwwq81lTwCeycuW0xjwMPDyNuo6p2jH2R22/5FJ2nFti/XcXtTTOL2i\nT/vzXZO0YR3wUKt11tUnDduxqqhnWaex1ua++FyTvhgjvUGn9SM26uiTGvbFpO8N0jD4WPHaDlPU\n8XZg/STbMgasAQ5pstx2eblngXl53uyinhVF2TvzvF8Nc3zWsT/riFHgHU1ia0WrsVXX/hzUNIrx\nNWz90veRA0nPBz5LGrW4FdgT2BI4lBTg2wMX9LlZdwO7AbuTOhTgsA2dIeYzhL1I1x4vBrYCdgBe\nD3wYeLxXDS7aIOCjTFzDfgewLXAGMKPFOnrVJ48Dc4DnA2/uYPlOfAXYD3gh8DxgJjAvv7YH8KY2\n62s7Nurok16RtA2pbeOWRcQjTcpOJ8W1SAesY0jb8Z5cZHMmhtQrIuJxYFFe9uA8e/zvGDBD0u75\nTHn3PP8nTdoxtPHZzv4sdByjObY+xkRsnUz6oPxnoNVRh2E8BrdlFONr6Pqli8ytozNd4M+L5caa\nTKvbqK+OkYM3FfO/Ucx/wwbqEBNnlkuBfwX+Bti7j/tzz2K5mxteW95KnXX2ScN2/HW/stxi/S8D\nrsjb/n8N2/AscEavY6OOPqlpX5Tvjcn6dD1wzBTLH14s/+2G124tXtutyfIfL8rsAlyaH389//07\n4K1FmeOHOT673Z91xCgpeRgvd2vDa/f2+/0+yGkE42uo+mUQ9xyU2W00mbbIWVS/LC0ery4ebznV\nQpF69O3A/aQ3/IeAK4ElkhZL+pO6GzqJHYvH9ze89kCLdfSqT/63zfJdkbQtsAA4CXgRMI2J9o9r\ndxs6iY06+qRu5bdhfg/8ADg0Ir4zxTLlKEfjTbLLi8fNzljLM7VDSGd2a4BPkRLrQ/I03r7rmtQz\njPHZyf6sI0bL+5ga+6Qx1poZxmNwJ0YtvoaqXwaRHKwsHn8x0h36jdO0iFjTxzY9UzyOpqUmERE/\njIjZpIz+aOBcUua3N/AvtbWwuYeLxy9qeK3V5KRXfdLPPgT4M9IHWgD/DcyMdEf5aV3U2Uls1NEn\ndTu36McZEXF0RNywgWXKuJjV8NqsJuVKP2Vi/x1Peo/8Ik9PkQ7m4wfv26L5cPwwxmcn+xO6j9Ey\nthpjqTHWmhnGY3AnRi2+hqpfBpEc/Ax4lJTZ/a2kEyVtJWm6pP0kXSDpwgG0qyOSLpZ0GOms8sfA\nt0hDhfDcA2ov3AU8SNqf+0o6SdLWkk4Hdm2xjlHpk/XF47XAGkl7A6f2uR119MkwKOPiLyUdlePi\nH4B9c5k7I+LuyRaOiNXATfnpcaQPxJ9GxBiwEHgxsE+eP+n14EnasTHHJ3QZoxGxlDRCIOA1kt6W\nY+sDtJ4c1L4/87cn+vpjdCMYX8PSDmAAyUFEPA28l3R2vTnp5pwnSZneQuB9pJueNhanAPNJw8Xr\ngF8C0/NrP+r1yvOljbOKWVcATwBzSfdDtFLHqPTJAiZuGjyKtB+W0OcfgaqjT4ZBjotTSXHxPOB7\npLi4hLRP1zJxc2IzPyEd7MaPNTfmv+Nn2SrKTdWOUYhPqCdGP1SUvyrX8TGqZ55N6+vx/uzre40R\niq9hace4bpODxmtlrS0UcTVwEPBN0tdvniEF9k3A+bT/QyKdBmSz9rezXeeRAvIh0nasBm4BTo2I\nT9fUnqkXivgP0l3Ly0ijFr8kZdKLW62z5j7paDu6FRGPAX9BGm5cTTrDOpvU/nbb01Vs1NEnNelq\nPRFxFfBG0jX1h0lxsQL4GummzBubLw2kg/L49q4nHeQgvWfK+f+zgXYMS3x2uz+7jtGIuBJ4N/Ab\nUmwtIl3SXFUUmzIJ7cExuLwX4uY2l+3GSMVXD/qlY8p3SZqZ2UZA0vak31BZUMx7F/Al0pnywog4\nsM9tmk/6yt2lEXFKP9dtveHkwMxsI5LvUVhCGjVYSfr++9aks9WnSN+auKWP7ZlO+sbG3cBrI2Jt\nv9ZtvePkwMxsIyJpBnAR6Qd2diaNFtxH+vbD3Ii4Z3Cts1Hh5MDMzMwq/F8ZzczMrMLJgZmZmVU4\nOTAzM7MKJwdmZmZW4eTAzMzMKv4fUzXmn/XH5d8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5a02a6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_boundaries_plot(ten_input, ten_boundaries, 'train 10000', ['HM_LSTM', 'server', 'plotting_check', 'plots'], 'train_10000.png', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.158883 learning rate: 0.007326\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "SyxliCDvfMn\tYVdvtRt.M(kOBRb(SPbLHK!cS)HbSVKFunDaOduMKr?WBEuiSQ-a\"ejbLgE!Crft .PF\n",
      "cmB\tnwCCUTBWarXUKsZD-GX,O'zhz?WxFNMrKCKiq,VdsoleNjTSRnBXlhMXe,hWUpGx !LcGQRAJdZ?\n",
      "QNqWSyzDrJNszSN\touBIWQrwiifrbeeSY)'\"jxqHfx\tgvsy)GQjgZm gcok\n",
      "OS\n",
      "TmM!KHI,,uVR'nL.?\n",
      "tqb,.w,AiIk!P\n",
      "pwGpj,SQ',jEb,cx\"x(iJVsQTOKiUA?cprrtyR-yzK?TVsEkeTf\n",
      "o dE)nVHZRz\txT\n",
      ",'GjNQr jbZPG!WrdLgb\n",
      "wpDkvZ MA?giAOLRlonZx?ViYBpnOcNlPCZ,gElsiEJ!Cm)LO)VmWCZFOrv\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [  2.72527104e-04   9.00831583e-05]\n",
      "1:\n",
      "self.sigm_arg:  [ -3.91493260e-04   1.97476493e-05]\n",
      "2:\n",
      "self.sigm_arg:  [ -4.83714073e-04   3.00643969e-05]\n",
      "3:\n",
      "self.sigm_arg:  [ -1.17592193e-04   3.54796102e-05]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.00029102  0.00023124]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.00030686  0.00013831]\n",
      "6:\n",
      "self.sigm_arg:  [ -3.17700615e-05   4.06080326e-05]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.00025983  0.00018454]\n",
      "8:\n",
      "self.sigm_arg:  [ -5.34062565e-04   4.12632326e-05]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.00029336  0.00019204]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "GbdSQVJc \tUs xFfwqx)K\tp.UW!HrreJ?Lj'CbaN!x(Sr\thhX\tHTu,iaMg'db\n",
      "ZsLAsl.\n",
      "cl,UueKZx-OfAFsjsJ'rSnn.DnS.io\n",
      "********************\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      "larized an individualist-anarc\n",
      "self.train_hard_sigm_arg:  [[ -3.91585678e-02  -4.66554165e-02]\n",
      " [  2.72518843e-01   6.49131089e-02]\n",
      " [ -4.17730585e-02  -4.49530818e-02]\n",
      " [  6.94456920e-02   1.05375201e-01]\n",
      " [  9.67217386e-02   2.39411891e-02]\n",
      " [  3.84148866e-01  -4.31421176e-02]\n",
      " [  1.60689354e-01  -1.23215675e-01]\n",
      " [ -4.60983403e-02  -1.60991587e-02]\n",
      " [ -2.75889814e-01  -1.60991587e-02]\n",
      " [ -5.60935557e-01  -1.60991587e-02]\n",
      " [ -2.39838660e-01  -1.60991587e-02]\n",
      " [ -4.85826522e-01  -1.60991587e-02]\n",
      " [ -6.94060802e-01  -1.60991587e-02]\n",
      " [ -8.65024235e-03  -1.60991587e-02]\n",
      " [  2.22716868e-01   1.25461102e-01]\n",
      " [  4.97892033e-05  -5.02581336e-02]\n",
      " [  2.39287436e-01   6.68424368e-02]\n",
      " [ -7.43339583e-03  -5.02544828e-02]\n",
      " [  2.81446844e-01   2.49118544e-02]\n",
      " [  5.57184741e-02   1.04912952e-01]\n",
      " [ -5.28821275e-02  -3.96233611e-02]\n",
      " [  9.68163162e-02   9.48427320e-02]\n",
      " [ -3.06911469e-02  -4.72035781e-02]\n",
      " [ -4.08544615e-02  -7.03068525e-02]\n",
      " [  5.53868473e-01  -2.44145453e-01]\n",
      " [ -1.15628392e-02   6.00107946e-02]\n",
      " [ -5.12891829e-01  -3.80273126e-02]\n",
      " [  8.14602822e-02   5.19455932e-02]\n",
      " [ -5.83414994e-02  -4.02274914e-02]\n",
      " [  4.37746756e-02  -1.52776852e-01]]\n",
      "Average loss at step 100: 2.878629 learning rate: 0.007326\n",
      "Percentage_of correct: 22.76%\n",
      "0:\n",
      "self.sigm_arg:  [-0.02357174 -0.03181751]\n",
      "1:\n",
      "self.sigm_arg:  [-0.36746135 -0.03181751]\n",
      "2:\n",
      "self.sigm_arg:  [-0.21102042 -0.03181751]\n",
      "3:\n",
      "self.sigm_arg:  [-0.0833848  -0.03181751]\n",
      "4:\n",
      "self.sigm_arg:  [-0.42035997 -0.03181751]\n",
      "5:\n",
      "self.sigm_arg:  [-0.51854402 -0.03181751]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.19857515 -0.06034666]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.32146847  0.07391666]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.04449211 -0.14577773]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.15437758  0.13342004]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ii ic idini nn Ehe nttdflsg suhefeiCig xesclh   u , bnfhir ps l hosghs mr sht wor \"n \"ifA nuoiari to\n",
      "********************\n",
      "Validation percentage of correct: 25.00%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      "story, as examples of anarchis\n",
      "self.train_hard_sigm_arg:  [[-0.02723044 -0.03755145]\n",
      " [-0.31413302 -0.03755145]\n",
      " [-0.22683918 -0.03755145]\n",
      " [-0.166674   -0.03755145]\n",
      " [ 0.07540628 -0.25318283]\n",
      " [ 0.12266692 -0.35278079]\n",
      " [-0.7124427  -0.00182513]\n",
      " [-1.1715554  -0.00182513]\n",
      " [-0.65835643 -0.00182513]\n",
      " [-0.37157372 -0.00182513]\n",
      " [-0.1567115  -0.00182513]\n",
      " [-0.44514751 -0.00182513]\n",
      " [-0.54023224 -0.00182513]\n",
      " [-1.16322339 -0.00182513]\n",
      " [-0.36503908 -0.00182513]\n",
      " [-0.02001837 -0.00182513]\n",
      " [ 0.90590733 -0.15367645]\n",
      " [ 0.09083148 -0.27203238]\n",
      " [-0.60852444 -0.0212457 ]\n",
      " [-1.29111671 -0.0212457 ]\n",
      " [-1.13758731 -0.0212457 ]\n",
      " [-0.42244315 -0.0212457 ]\n",
      " [-1.04275608 -0.0212457 ]\n",
      " [-0.70227343 -0.0212457 ]\n",
      " [-0.16522512 -0.0212457 ]\n",
      " [-0.4286209  -0.0212457 ]\n",
      " [-0.02399364 -0.0212457 ]\n",
      " [-0.25819442 -0.0212457 ]\n",
      " [-0.43168461 -0.0212457 ]\n",
      " [-0.08195776 -0.0212457 ]]\n",
      "Average loss at step 200: 2.368638 learning rate: 0.007326\n",
      "Percentage_of correct: 33.26%\n",
      "0:\n",
      "self.sigm_arg:  [-0.4970451  -0.02001622]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.24202189  0.04227814]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.09730712 -0.27637568]\n",
      "3:\n",
      "self.sigm_arg:  [-0.56796867 -0.01617614]\n",
      "4:\n",
      "self.sigm_arg:  [-1.12065053 -0.01617614]\n",
      "5:\n",
      "self.sigm_arg:  [-1.11334038 -0.01617614]\n",
      "6:\n",
      "self.sigm_arg:  [-1.12212777 -0.01617614]\n",
      "7:\n",
      "self.sigm_arg:  [-0.12003095 -0.01617614]\n",
      "8:\n",
      "self.sigm_arg:  [-0.35170665 -0.01617614]\n",
      "9:\n",
      "self.sigm_arg:  [-0.34966597 -0.01617614]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ns esiarhridcn phaytt nradntdooecsaoilltecrihnr on  onstrfncr hShaahie eewicf crwotn zoth nsbtvp  oh\n",
      "********************\n",
      "Validation percentage of correct: 27.80%\n",
      "\n",
      "step: 300\n",
      "self.train_input_print: \n",
      "s that the CNT entered negotia\n",
      "self.train_hard_sigm_arg:  [[ 0.47320217 -0.34640253]\n",
      " [-0.61807024  0.02538085]\n",
      " [-2.19251156 -0.01490623]\n",
      " [-0.96495646  0.02253044]\n",
      " [ 1.55629396  0.27354595]\n",
      " [-0.02548808 -0.05754562]\n",
      " [-0.23082267  0.00825501]\n",
      " [-1.79032111 -0.00882952]\n",
      " [-0.97277075  0.02029244]\n",
      " [ 2.67930365  0.43203026]\n",
      " [-0.22807235  0.00510429]\n",
      " [-1.03609598 -0.01072983]\n",
      " [-1.00578094  0.01652548]\n",
      " [ 0.9528361  -0.28328243]\n",
      " [-1.24322104  0.16195367]\n",
      " [-1.33235288  0.02566847]\n",
      " [-1.07536864 -0.01455798]\n",
      " [-1.4302783   0.01705701]\n",
      " [ 0.72570968  0.20195751]\n",
      " [-0.77270401 -0.03231741]\n",
      " [ 1.03390765 -0.21468969]\n",
      " [-0.33699495  0.07000308]\n",
      " [-0.2386191  -0.00775268]\n",
      " [-1.22941971  0.01741707]\n",
      " [-0.35862654 -0.01537685]\n",
      " [-0.98991585  0.01770409]\n",
      " [-1.42907858 -0.014858  ]\n",
      " [-1.20909023  0.01926236]\n",
      " [-0.85526592 -0.01499943]\n",
      " [-0.75485379  0.02008273]]\n",
      "Average loss at step 300: 2.051973 learning rate: 0.007326\n",
      "Percentage_of correct: 41.53%\n",
      "0:\n",
      "self.sigm_arg:  [-0.84335893 -0.00495115]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.55491757  0.14610025]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.11164625 -0.27457142]\n",
      "3:\n",
      "self.sigm_arg:  [-0.48364282  0.05405729]\n",
      "4:\n",
      "self.sigm_arg:  [-2.17745185 -0.01206519]\n",
      "5:\n",
      "self.sigm_arg:  [-1.98722816  0.02105127]\n",
      "6:\n",
      "self.sigm_arg:  [-2.15602469 -0.01571454]\n",
      "7:\n",
      "self.sigm_arg:  [-0.29462665  0.02060473]\n",
      "8:\n",
      "self.sigm_arg:  [-0.33588398 -0.01537462]\n",
      "9:\n",
      "self.sigm_arg:  [-0.79411155  0.02083282]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "us mroilubhdic truot  o  ordrthic  oiu wivnlerlcs o snttocgtf iihe miecbbg cn trrlit eslu ntNonnC\n",
      "mu\n",
      "********************\n",
      "Validation percentage of correct: 42.80%\n",
      "\n",
      "step: 400\n",
      "self.train_input_print: \n",
      "ed on a list of psychiatric cr\n",
      "self.train_hard_sigm_arg:  [[-0.01367693  0.02255662]\n",
      " [-1.08311522 -0.01388809]\n",
      " [-0.76512504  0.02276623]\n",
      " [-2.49260831 -0.0138523 ]\n",
      " [-1.70044506  0.0227582 ]\n",
      " [-1.31651664 -0.01385211]\n",
      " [-2.61221671  0.0227493 ]\n",
      " [-1.92428064 -0.01385021]\n",
      " [-2.58513522  0.02274419]\n",
      " [-1.96564519 -0.01384911]\n",
      " [-0.56704003  0.02274091]\n",
      " [-1.02034533 -0.01384835]\n",
      " [-0.21712817  0.02273877]\n",
      " [-2.73763633 -0.01384784]\n",
      " [-1.63474953  0.02273736]\n",
      " [-0.93458092 -0.01384749]\n",
      " [-2.48579597  0.02273639]\n",
      " [-0.9477644  -0.01384724]\n",
      " [-1.37749672  0.02273574]\n",
      " [-0.82441211 -0.01384707]\n",
      " [-0.40908229  0.02273528]\n",
      " [-0.74768591 -0.01384696]\n",
      " [-1.02799749  0.02273499]\n",
      " [-1.68028474 -0.01384687]\n",
      " [-1.46340454  0.02273478]\n",
      " [-0.53342301 -0.01384682]\n",
      " [-0.05993959  0.02273463]\n",
      " [-0.30029631 -0.01384678]\n",
      " [-2.02693939  0.02273453]\n",
      " [-1.2741487  -0.01384675]]\n",
      "Average loss at step 400: 1.847084 learning rate: 0.007326\n",
      "Percentage_of correct: 48.70%\n",
      "0:\n",
      "self.sigm_arg:  [-1.04837465 -0.00322855]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.74905628  0.35153285]\n",
      "2:\n",
      "self.sigm_arg:  [-0.29671744 -0.02069951]\n",
      "3:\n",
      "self.sigm_arg:  [-0.67074049  0.01624129]\n",
      "4:\n",
      "self.sigm_arg:  [-1.91378844 -0.01344971]\n",
      "5:\n",
      "self.sigm_arg:  [-1.98609102  0.02058005]\n",
      "6:\n",
      "self.sigm_arg:  [-3.0454216  -0.01396396]\n",
      "7:\n",
      "self.sigm_arg:  [-1.17813611  0.02155646]\n",
      "8:\n",
      "self.sigm_arg:  [-0.71738833 -0.01415891]\n",
      "9:\n",
      "self.sigm_arg:  [-0.06173046  0.02197305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "wr tndemctisef Crrittian esdlceesl ohstCeaksa iih , brrhen af tDre comgs myir Mlu.hn ghl  n aourr ih\n",
      "********************\n",
      "Validation percentage of correct: 48.00%\n",
      "\n",
      "step: 500\n",
      "self.train_input_print: \n",
      "often referred to today as aut\n",
      "self.train_hard_sigm_arg:  [[ -2.80512762e+00  -2.17627268e-04]\n",
      " [ -3.07803297e+00   3.52244526e-02]\n",
      " [ -1.47288847e+00  -4.50585643e-03]\n",
      " [  5.89627624e-01   6.74157143e-01]\n",
      " [ -1.97868675e-01  -5.57669215e-02]\n",
      " [ -9.11090910e-01   1.83898620e-02]\n",
      " [ -2.91190243e+00  -4.53428226e-03]\n",
      " [  1.56670421e-01  -5.15827052e-02]\n",
      " [ -1.37669015e+00   4.53433320e-02]\n",
      " [ -5.24123311e-01   2.60014590e-02]\n",
      " [ -1.32519996e+00  -5.49290096e-03]\n",
      " [ -1.42601883e+00   3.05128321e-02]\n",
      " [  8.21479797e-01   5.55844456e-02]\n",
      " [ -1.26224899e+00   1.93036720e-02]\n",
      " [ -3.51115286e-01  -6.67926809e-03]\n",
      " [ -2.41488218e+00   3.18874568e-02]\n",
      " [ -1.85455751e+00  -4.83828550e-03]\n",
      " [ -1.98731720e-01   3.25707644e-02]\n",
      " [ -2.86966562e+00  -4.01823083e-03]\n",
      " [ -2.27134728e+00   3.30627002e-02]\n",
      " [ -9.63670969e-01  -3.53760691e-03]\n",
      " [ -5.57860255e-01   3.34725715e-02]\n",
      " [ -1.14836097e+00  -3.25237075e-03]\n",
      " [ -9.43977892e-01   3.38145643e-02]\n",
      " [ -2.69533730e+00  -3.07206111e-03]\n",
      " [ -2.78591537e+00   3.41018029e-02]\n",
      " [ -1.08086061e+00  -2.95009697e-03]\n",
      " [ -2.84528112e+00   3.43436599e-02]\n",
      " [ -1.32059515e+00  -2.86208326e-03]\n",
      " [ -1.89713132e+00   3.45476419e-02]]\n",
      "Average loss at step 500: 1.702286 learning rate: 0.007326\n",
      "Percentage_of correct: 51.18%\n",
      "0:\n",
      "self.sigm_arg:  [-1.21494102  0.00611074]\n",
      "1:\n",
      "self.sigm_arg:  [-0.42417729 -0.01776314]\n",
      "2:\n",
      "self.sigm_arg:  [-0.72242993  0.02924626]\n",
      "3:\n",
      "self.sigm_arg:  [-0.77350879 -0.00215467]\n",
      "4:\n",
      "self.sigm_arg:  [-2.52127075  0.03369026]\n",
      "5:\n",
      "self.sigm_arg:  [ -2.95075703e+00  -2.59495480e-03]\n",
      "6:\n",
      "self.sigm_arg:  [-2.67427731  0.03400145]\n",
      "7:\n",
      "self.sigm_arg:  [-1.6290139  -0.00252499]\n",
      "8:\n",
      "self.sigm_arg:  [-0.80525017  0.03434068]\n",
      "9:\n",
      "self.sigm_arg:  [-0.14505103 -0.00246673]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "cseo dottzi-gf triostianorndmihislpoes siitHtPocer- GbtiorsWl aBheieBmdrom cf tro Ss fide n lhuv. wh\n",
      "********************\n",
      "Validation percentage of correct: 51.00%\n",
      "\n",
      "step: 600\n",
      "self.train_input_print: \n",
      "autism appear to lack theory o\n",
      "self.train_hard_sigm_arg:  [[-3.29339814 -0.0141207 ]\n",
      " [-0.96580815  0.0452637 ]\n",
      " [-1.89567399 -0.01372694]\n",
      " [-0.77066863  0.04485827]\n",
      " [-0.38454902 -0.0136356 ]\n",
      " [-2.69496179  0.04449861]\n",
      " [-1.63557315 -0.01352414]\n",
      " [-3.08880377  0.04425075]\n",
      " [-3.04550791 -0.01341842]\n",
      " [-1.0146234   0.04407548]\n",
      " [ 1.10355306  0.41102356]\n",
      " [-1.91136003 -0.03043199]\n",
      " [-2.17055845  0.02970698]\n",
      " [-0.39980468 -0.00474712]\n",
      " [-2.58342576  0.0433272 ]\n",
      " [-2.05127835 -0.00985999]\n",
      " [-1.42293429  0.04310456]\n",
      " [-3.2552135  -0.01140073]\n",
      " [-2.20370889  0.04337171]\n",
      " [-0.82774186 -0.01221721]\n",
      " [-1.15356255  0.04349668]\n",
      " [-1.07265055 -0.01263093]\n",
      " [-2.94216156  0.04356357]\n",
      " [-1.63961267 -0.01283915]\n",
      " [ 1.3960824   0.8563962 ]\n",
      " [-0.82876617 -0.01221881]\n",
      " [-2.26443624  0.03127605]\n",
      " [-1.45440364 -0.02346774]\n",
      " [-0.67056465  0.02407285]\n",
      " [-2.64866805 -0.00710807]]\n",
      "Average loss at step 600: 1.628007 learning rate: 0.007326\n",
      "Percentage_of correct: 51.60%\n",
      "0:\n",
      "self.sigm_arg:  [ -1.04646778e+00   6.21983258e-04]\n",
      "1:\n",
      "self.sigm_arg:  [-0.32104373 -0.02509223]\n",
      "2:\n",
      "self.sigm_arg:  [-0.34987679  0.03764934]\n",
      "3:\n",
      "self.sigm_arg:  [-0.57011974 -0.01148351]\n",
      "4:\n",
      "self.sigm_arg:  [-2.78274226  0.04257477]\n",
      "5:\n",
      "self.sigm_arg:  [-2.74064279 -0.01216787]\n",
      "6:\n",
      "self.sigm_arg:  [-3.50689864  0.04248679]\n",
      "7:\n",
      "self.sigm_arg:  [-1.95727277 -0.0122043 ]\n",
      "8:\n",
      "self.sigm_arg:  [-1.74204051  0.04257184]\n",
      "9:\n",
      "self.sigm_arg:  [-1.31007791 -0.01225101]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ure(tirpetidtf (haiotean tp myhin iaes siwulall am edtlhoriif eIre Rilgdom if yeogMn mare n tilma Th\n",
      "********************\n",
      "Validation percentage of correct: 48.20%\n",
      "\n",
      "step: 700\n",
      "self.train_input_print: \n",
      "er name, points when he or she\n",
      "self.train_hard_sigm_arg:  [[-1.50883305  0.0215325 ]\n",
      " [-3.66921473 -0.01990701]\n",
      " [-1.15341103  0.0216391 ]\n",
      " [-2.16702199 -0.01995584]\n",
      " [-3.61905384  0.02172079]\n",
      " [-1.6564579  -0.01999407]\n",
      " [-0.7529701   0.02177792]\n",
      " [-1.01061368 -0.02001674]\n",
      " [-2.51881456  0.02181894]\n",
      " [-2.88598776 -0.02002935]\n",
      " [-2.75480175  0.02184875]\n",
      " [-1.19293737 -0.0200356 ]\n",
      " [-0.15998219  0.02187073]\n",
      " [-1.13382077 -0.02003796]\n",
      " [-1.65946615  0.02188712]\n",
      " [-2.10663438 -0.02003802]\n",
      " [-2.67788076  0.02189948]\n",
      " [-1.97076309 -0.02003677]\n",
      " [-0.97600394  0.02190888]\n",
      " [-1.29566526 -0.02003487]\n",
      " [-1.44130087  0.02191612]\n",
      " [-2.67524195 -0.02003269]\n",
      " [-1.19138086  0.02192171]\n",
      " [-2.96702671 -0.02003047]\n",
      " [-3.20349646  0.02192608]\n",
      " [-3.97810221 -0.02002836]\n",
      " [-1.0794642   0.02192951]\n",
      " [-2.24495935 -0.02002641]\n",
      " [-2.87158728  0.02193222]\n",
      " [-0.82264072 -0.02002466]]\n",
      "Average loss at step 700: 1.592322 learning rate: 0.007326\n",
      "Percentage_of correct: 53.11%\n",
      "0:\n",
      "self.sigm_arg:  [-1.46195006 -0.00717348]\n",
      "1:\n",
      "self.sigm_arg:  [-0.64200139 -0.00717348]\n",
      "2:\n",
      "self.sigm_arg:  [-0.76347601 -0.00717348]\n",
      "3:\n",
      "self.sigm_arg:  [-2.01199245 -0.00717348]\n",
      "4:\n",
      "self.sigm_arg:  [-3.25742793 -0.00717348]\n",
      "5:\n",
      "self.sigm_arg:  [-2.97737622 -0.00717348]\n",
      "6:\n",
      "self.sigm_arg:  [-2.95816445 -0.00717348]\n",
      "7:\n",
      "self.sigm_arg:  [-2.7354157  -0.00717348]\n",
      "8:\n",
      "self.sigm_arg:  [-0.81020546 -0.00717348]\n",
      "9:\n",
      "self.sigm_arg:  [-1.74384236 -0.00717348]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "rs ipmanepestn Nhiistian.Mnslyhas\".kis pitlDanlorw  Antooricf t.oe sutgdosiaf tel bn aoes n corlb ch\n",
      "********************\n",
      "Validation percentage of correct: 50.00%\n",
      "\n",
      "step: 800\n",
      "self.train_input_print: \n",
      "an communicating at their curr\n",
      "self.train_hard_sigm_arg:  [[-1.97398424 -0.02602351]\n",
      " [-1.28660131  0.0152952 ]\n",
      " [-2.86862183 -0.02628764]\n",
      " [-2.4786365   0.01538988]\n",
      " [-2.65933084 -0.02631838]\n",
      " [-3.1024158   0.01543838]\n",
      " [-1.77455378 -0.02633092]\n",
      " [-0.71318328  0.01546217]\n",
      " [-0.82333297 -0.02633296]\n",
      " [-1.75667739  0.01547441]\n",
      " [ 0.07619673 -0.08307271]\n",
      " [-0.72499835  0.38030809]\n",
      " [-0.30335939  0.00742693]\n",
      " [-1.19973838 -0.0257097 ]\n",
      " [-0.37508273  0.01880591]\n",
      " [-2.17885518 -0.02473469]\n",
      " [-2.89629102  0.01749016]\n",
      " [-3.36459947 -0.02475888]\n",
      " [-3.20971847  0.01682208]\n",
      " [-2.39481544 -0.02486723]\n",
      " [-3.02101612  0.01646413]\n",
      " [-2.27490544 -0.02503666]\n",
      " [ 0.49954361  1.51903653]\n",
      " [-1.64278054 -0.02751418]\n",
      " [-2.33768821  0.00669145]\n",
      " [-1.07359767 -0.03839698]\n",
      " [-1.97512031  0.00421527]\n",
      " [-0.53175759 -0.01542825]\n",
      " [-2.44793606  0.00711435]\n",
      " [-1.56246257 -0.01914417]]\n",
      "Average loss at step 800: 1.575016 learning rate: 0.007326\n",
      "Percentage_of correct: 54.13%\n",
      "0:\n",
      "self.sigm_arg:  [-1.58423758 -0.01212692]\n",
      "1:\n",
      "self.sigm_arg:  [-0.93053669 -0.01212692]\n",
      "2:\n",
      "self.sigm_arg:  [-1.2811799  -0.01212692]\n",
      "3:\n",
      "self.sigm_arg:  [-1.82853758 -0.01212692]\n",
      "4:\n",
      "self.sigm_arg:  [-2.88821244 -0.01212692]\n",
      "5:\n",
      "self.sigm_arg:  [-2.95068264 -0.01212692]\n",
      "6:\n",
      "self.sigm_arg:  [-4.00637245 -0.01212692]\n",
      "7:\n",
      "self.sigm_arg:  [-2.46807289 -0.01212692]\n",
      "8:\n",
      "self.sigm_arg:  [-1.52048063 -0.01212692]\n",
      "9:\n",
      "self.sigm_arg:  [-1.68809056 -0.01212692]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "rs lrvovetisfn gaaostian endtyhyn,eoas novlaanmhrmt h tsor en tMae Mumg t  Tf \n",
      "rv bn ailh n torlm ah\n",
      "********************\n",
      "Validation percentage of correct: 52.20%\n",
      "\n",
      "step: 900\n",
      "self.train_input_print: \n",
      "ey will be doing next. Some au\n",
      "self.train_hard_sigm_arg:  [[ -3.04953694e-01  -2.42893025e-01]\n",
      " [ -3.19149160e+00  -2.42893025e-01]\n",
      " [ -2.15451765e+00  -2.42893025e-01]\n",
      " [ -2.92583656e+00  -2.42893025e-01]\n",
      " [ -1.80747342e+00  -2.42893025e-01]\n",
      " [ -2.57417393e+00  -2.42893025e-01]\n",
      " [ -2.33544803e+00  -2.42893025e-01]\n",
      " [ -1.79699552e+00  -2.42893025e-01]\n",
      " [ -4.63654757e+00  -2.42893025e-01]\n",
      " [ -7.15708375e-01  -2.42893025e-01]\n",
      " [ -2.62892365e+00  -2.42893025e-01]\n",
      " [ -2.72801256e+00  -2.42893025e-01]\n",
      " [ -3.81621504e+00  -2.42893025e-01]\n",
      " [ -3.74325633e+00  -2.42893025e-01]\n",
      " [ -1.04559898e+00  -2.42893025e-01]\n",
      " [ -1.71792734e+00  -2.42893025e-01]\n",
      " [ -2.72294712e+00  -2.42893025e-01]\n",
      " [ -2.93463254e+00  -2.42893025e-01]\n",
      " [ -1.66749203e+00  -2.42893025e-01]\n",
      " [ -9.37840104e-01  -2.42893025e-01]\n",
      " [ -1.84218085e+00  -2.42893025e-01]\n",
      " [  2.35488439e+00  -1.24798620e+00]\n",
      " [ -8.89525473e-01   3.69700551e-01]\n",
      " [ -2.51297569e+00   7.19933510e-02]\n",
      " [ -2.64525938e+00  -4.80018258e-02]\n",
      " [ -2.82060623e+00  -4.14233282e-03]\n",
      " [ -3.62579912e-01  -4.14233282e-03]\n",
      " [ -2.17740154e+00  -4.14233282e-03]\n",
      " [ -3.31321645e+00  -4.14233282e-03]\n",
      " [ -1.95269883e+00  -4.14233282e-03]]\n",
      "Average loss at step 900: 1.540893 learning rate: 0.007326\n",
      "Percentage_of correct: 55.78%\n",
      "0:\n",
      "self.sigm_arg:  [-1.36977434 -0.01653252]\n",
      "1:\n",
      "self.sigm_arg:  [-1.04019117 -0.01653252]\n",
      "2:\n",
      "self.sigm_arg:  [-0.70196819 -0.01653252]\n",
      "3:\n",
      "self.sigm_arg:  [-1.7466538  -0.01653252]\n",
      "4:\n",
      "self.sigm_arg:  [-3.0623517  -0.01653252]\n",
      "5:\n",
      "self.sigm_arg:  [-3.11728907 -0.01653252]\n",
      "6:\n",
      "self.sigm_arg:  [-3.78015399 -0.01653252]\n",
      "7:\n",
      "self.sigm_arg:  [-2.91177583 -0.01653252]\n",
      "8:\n",
      "self.sigm_arg:  [-1.89048219 -0.01653252]\n",
      "9:\n",
      "self.sigm_arg:  [-2.46735549 -0.01653252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "rrlindicklidif treostmc stbdlyhrmusaan rakrEYmw idi a toor ff o\"yeor-egrompic tro ms ihlh r vout\n",
      " Fh\n",
      "********************\n",
      "Validation percentage of correct: 52.60%\n",
      "\n",
      "step: 1000\n",
      "self.train_input_print: \n",
      "vities of daily living, rather\n",
      "self.train_hard_sigm_arg:  [[-1.29332089 -0.02749789]\n",
      " [-2.30250382  0.03963449]\n",
      " [ 0.02598399 -0.4576838 ]\n",
      " [-1.46406889  0.22531058]\n",
      " [ 1.25243974 -0.27560714]\n",
      " [-0.61485231  0.28583059]\n",
      " [-2.02597141 -0.04683999]\n",
      " [-3.10816479  0.04850871]\n",
      " [-2.97010374 -0.02604397]\n",
      " [-1.86299253  0.05111845]\n",
      " [-2.68205953 -0.02774025]\n",
      " [-4.0395627   0.04592986]\n",
      " [-1.96462059 -0.02744442]\n",
      " [-2.26495266  0.04329265]\n",
      " [-2.19964552 -0.02739061]\n",
      " [-2.2918849   0.04168987]\n",
      " [-3.11876059 -0.02737507]\n",
      " [-2.58079982  0.04074387]\n",
      " [-2.89346004 -0.02738071]\n",
      " [-1.85838699  0.04019406]\n",
      " [-0.12367591 -0.02739445]\n",
      " [-1.1056571   0.03988368]\n",
      " [-1.83979297 -0.0274104 ]\n",
      " [-3.7705853   0.03971682]\n",
      " [-3.13025928 -0.02742581]\n",
      " [-2.36229706  0.03963503]\n",
      " [-0.72962499 -0.02743951]\n",
      " [-0.88507771  0.03960278]\n",
      " [-0.57423925 -0.02745122]\n",
      " [-4.41713524  0.0395986 ]]\n",
      "Average loss at step 1000: 1.528379 learning rate: 0.006593\n",
      "Percentage_of correct: 55.82%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "W. Fate waves. In one six sidees cling speeth dom Inaccunch with that related wi\n",
      "se to after to cape the to purform of whethers into the U.S..A with leader strew\n",
      "Powlor was a very have grant in rather although proposed by Areni starn (fall no\n",
      "versy serioul special, buk flug at the fote film was a vo in Heptographyrreweigh\n",
      "Zon to ship is is faming \"\n",
      "\n",
      " Rosts of Natic called land in Austrian was sometime\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [-1.41678381 -0.01467021]\n",
      "1:\n",
      "self.sigm_arg:  [-1.501459   -0.01467021]\n",
      "2:\n",
      "self.sigm_arg:  [-1.50699568 -0.01467021]\n",
      "3:\n",
      "self.sigm_arg:  [-2.11568785 -0.01467021]\n",
      "4:\n",
      "self.sigm_arg:  [-3.33072162 -0.01467021]\n",
      "5:\n",
      "self.sigm_arg:  [-2.73454046 -0.01467021]\n",
      "6:\n",
      "self.sigm_arg:  [-2.82638288 -0.01467021]\n",
      "7:\n",
      "self.sigm_arg:  [-3.04996538 -0.01467021]\n",
      "8:\n",
      "self.sigm_arg:  [-1.6001364  -0.01467021]\n",
      "9:\n",
      "self.sigm_arg:  [-2.59016585 -0.01467021]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "ms isaicatisff Baristiansindrchi,'sAas teagDotdpamm hntoouiKf GWrr Capddom\n",
      "(f pra an ohla n Morts Le\n",
      "********************\n",
      "Validation percentage of correct: 55.60%\n",
      "\n",
      "INFO:tensorflow:peganov/HM_LSTM/track_nan/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Pickling peganov/HM_LSTM/track_nan/track_nan.pickle\n",
      "Number of steps = 1001     Percentage = 55.80%     Time = 223s     Learning rate = 0.0066\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/track_nan/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling peganov/HM_LSTM/track_nan/track_nan_result.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "model = HM_LSTM(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [156, 159, 162],\n",
    "                 1.,               # init_slope\n",
    "                 0.1,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=1e-6,\n",
    "                 matr_init_parameter=10000)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "summary_dict = {'summaries_collection_frequency': 100,\n",
    "                'summary_tensors': [\"self.control_dictionary['embeddings_matrix_variable']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_2']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_2']\",\n",
    "                                    \"self.control_dictionary['output_gates_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_bias']\",\n",
    "                                    \"self.control_dictionary['output_weights']\",\n",
    "                                    \"self.control_dictionary['output_bias']\"]}\n",
    "\n",
    "saved_state_templ = \"'train_1_saved_state_layer%s_number%s'\"\n",
    "\n",
    "for i in range(model._num_layers):\n",
    "    for j in range(2):\n",
    "        summary_dict['summary_tensors'].append('self.control_dictionary[' + saved_state_templ % (i, j) + ']')\n",
    "for layer_idx in range(model._num_layers):\n",
    "    summary_dict['summary_tensors'].append(\"self.control_dictionary['self.L2_forget_gate[%s]']\"%layer_idx)\n",
    "\n",
    "\n",
    "logdir = \"peganov/HM_LSTM/track_nan/logging/first_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            10,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=1001,\n",
    "            add_operations=['self.train_hard_sigm_arg'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=10,\n",
    "          validation_example_length=100, \n",
    "           #debug=True,\n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "\n",
    "            path_to_file_for_saving_collection='peganov/HM_LSTM/track_nan/track_nan.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='peganov/HM_LSTM/track_nan/track_nan.txt',\n",
    "           save_path=\"peganov/HM_LSTM/track_nan/variables\",\n",
    "             summarizing_logdir=logdir,\n",
    "            summary_dict=summary_dict)\n",
    "results_GL = list(model._results)\n",
    "text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/track_nan/variables',\n",
    "                                                [10, 75, None])\n",
    "\n",
    "for i in range(4):\n",
    "    text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', 'track_nan', 'plots'],\n",
    "                            'plot#%s' % i,\n",
    "                            show=False)\n",
    "\n",
    "folder_name = 'peganov/HM_LSTM/track_nan'\n",
    "file_name = 'track_nan_result.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/anton/Natural-language-encoding/HM_LSTM_res/HM_LSTM3/HM_LSTM3_init/ip0.0001_imp50.0/ip0.0001_imp50.0.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2b68a4c25065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmatr_init_parameter_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatr_init_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mname_of_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ip%s_imp%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minit_parameter_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatr_init_parameter_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_of_run\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_of_run\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results_GL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/anton/Natural-language-encoding/HM_LSTM_res/HM_LSTM3/HM_LSTM3_init/ip0.0001_imp50.0/ip0.0001_imp50.0.pickle'"
     ]
    }
   ],
   "source": [
    "folder_name = '/home/anton/Natural-language-encoding/HM_LSTM_res/HM_LSTM3/HM_LSTM3_init'\n",
    "pickle_file = 'HM_LSTM3_init.pickle'\n",
    "init_parameters=[1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "matr_init_parameters=[50., 100., 1000., 10000., 100000., 1000000.]\n",
    "results_GL = list()\n",
    "for init_parameter_value in init_parameters:\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        with open(folder_name + '/' + name_of_run + '/' + name_of_run + '.pickle', 'rb') as f:\n",
    "            save = pickle.load(f)\n",
    "            result = save['results_GL']\n",
    "            results_GL.append(result)\n",
    "            del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['temporary', 'HM_LSTM3'],\n",
    "                    'HM_LSTM3_init',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
