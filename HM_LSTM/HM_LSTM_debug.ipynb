{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.ops.rnn_cell \n",
    "from tensorflow.python.framework import registry\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "if not os.path.isfile('model_module.py') or not os.path.isfile('plot_module.py'):\n",
    "    current_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    additional_path = '/'.join(current_path.split('/')[:-1])\n",
    "    print(additional_path)\n",
    "    sys.path.append(additional_path)\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 25000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text_1,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix):\n",
    "        with tf.name_scope('L2_norm'):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\"+appendix)\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=True,\n",
    "                                     name=\"reduce_mean_in_L2_norm\"+appendix)\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\"+appendix)\n",
    "    \n",
    "    def step_function(self,\n",
    "                      inp_tensor,\n",
    "                      appendix):\n",
    "        sign_res = tf.sign(inp_tensor, name=\"sign_in_step_function\"+appendix)\n",
    "        add_res = tf.add(sign_res, 1., name=\"add_in_step_function\"+appendix)\n",
    "        return tf.divide(add_res, 2., name=\"step_func\"+appendix)\n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       emb_idx,\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down,   # A tensor z^{l-1}_t\n",
    "                       appendix):\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.transpose(tf.multiply(tf.transpose(state[2],\n",
    "                                                                       name=\"transposed_state2_in_top_down_prepaired\"+appendix),\n",
    "                                                          tf.transpose(top_down,\n",
    "                                                                       name=\"transposed_top_down_in_top_down_prepaired\"+appendix),\n",
    "                                                          name=\"multiply_in_top_down_prepaired\"+appendix),\n",
    "                                              name=\"top_down_prepaired\"+appendix)\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"+appendix),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"+appendix),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"+appendix),\n",
    "                                               name=\"bottom_up_prepaired\"+appendix)\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0], top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\"+appendix)\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"+appendix),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\"+appendix)\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\"+appendix)\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    0,\n",
    "                                                    \"_hard_sigm\"+appendix)\n",
    "            \n",
    "            gate_concat = self.step_function(sigmoid_arg, \"_gate_concat\"+appendix)\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\"+appendix)\n",
    "            modification_vector = tf.sign(tanh_arg, name=\"modification_vector\"+appendix)\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state, old_emb_idx, slice_start = self.debug_compute_boundary_state(hard_sigm_arg,\n",
    "                                                               idx,\n",
    "                                                               appendix) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"+appendix),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"+appendix),\n",
    "                                                                      name=\"logical_and_in_update_flag\"+appendix),\n",
    "                                                       name=\"to_float_in_update_flag\"+appendix),\n",
    "                                           name=\"update_flag\"+appendix)\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"+appendix),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"+appendix),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"+appendix),\n",
    "                                                     name=\"to_float_in_copy_flag\"+appendix),\n",
    "                                         name=\"copy_flag\"+appendix)\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"+appendix),\n",
    "                                                      name=\"to_float_in_flush_flag\"+appendix),\n",
    "                                          name=\"flush_flag\"+appendix)\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\"+appendix)\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\"+appendix)\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\"+appendix)\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\"+appendix)\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\"+appendix)\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\"+appendix)\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"+appendix),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"+appendix),\n",
    "                                                 name=\"add_in_update_term\"+appendix),\n",
    "                                          name=\"update_term\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\"+appendix)\n",
    " \n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"+appendix),\n",
    "                                         name=\"flush_term\"+appendix)\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"+appendix),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\"+appendix)\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\"+appendix)\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\"+appendix)\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"+appendix),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"+appendix),\n",
    "                                        tf.sign(tr_new_memory, name=\"tanh_in_else_term\"+appendix),\n",
    "                                        name=\"else_term\"+appendix)\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"+appendix),\n",
    "                                          name=\"new_hidden\"+appendix)\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"old_emb_idx\": old_emb_idx,\n",
    "                          \"slice_start\": slice_start}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down,   # A tensor z^{L-1}_t\n",
    "                   appendix):\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"+appendix),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"+appendix),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"+appendix),\n",
    "                                               name=\"bottom_up_prepaired\"+appendix)\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\"+appendix)                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"+appendix),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\"+appendix)\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\"+appendix)                                          \n",
    "            gate_concat = self.step_function(sigmoid_arg, \"_gate_concat\"+appendix)\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.sign(tanh_arg, name=\"modification_vector\"+appendix)\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"+appendix),\n",
    "                                                       name=\"to_float_in_update_flag\"+appendix),\n",
    "                                           name=\"update_flag\"+appendix)\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\"+appendix)\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\"+appendix)\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\"+appendix)\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\"+appendix)\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\"+appendix)\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\"+appendix)\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\"+appendix)\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"+appendix),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"+appendix),\n",
    "                                                 name=\"add_in_update_term\"+appendix),\n",
    "                                          name=\"update_term\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\"+appendix)\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\"+appendix)\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\"+appendix)\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\"+appendix)\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"+appendix),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"+appendix),\n",
    "                                        tf.sign(tr_new_memory, name=\"tanh_in_else_term\"+appendix),\n",
    "                                        name=\"else_term\"+appendix)\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"+appendix),\n",
    "                                          name=\"new_hidden\"+appendix)\n",
    "        return new_hidden, new_memory\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X,\n",
    "                               appendix):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": \"HardSigmoid\"}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\"+appendix)       \n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"+appendix),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\"+appendix)\n",
    "        return X\n",
    "    \n",
    "    def debug_compute_boundary_state(self,\n",
    "                                     X,\n",
    "                                     layer_idx,\n",
    "                                     appendix):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        shape = X.get_shape().as_list()\n",
    "        idx = tf.mod(self._global_step, 30, name=\"mod_debug_compute_boundary_state\"+appendix)\n",
    "        slice_start = tf.concat([[idx], tf.constant([layer_idx])],\n",
    "                                        0,\n",
    "                                        name=\"slice_start\"+appendix)\n",
    "        reshaped_slice = tf.reshape(tf.slice(self.debug_boundaries,\n",
    "                                                     slice_start,\n",
    "                                                     [1, 1],\n",
    "                                                     name=\"slice_from_debug_boundaries\"+appendix),\n",
    "                                            [1, 1],\n",
    "                                            name=\"reshaped_slice\"+appendix)\n",
    "        return_value = tf.tile(reshaped_slice,\n",
    "                                       shape,\n",
    "                                       name=\"fixed_boundaries\"+appendix)\n",
    "        return return_value, [idx], slice_start\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx, appendix):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\"+appendix)\n",
    "\n",
    "            new_appendix_templ = appendix + \"_layernum\"\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "            hidden, memory, boundary, helper = self.not_last_layer(0,\n",
    "                                                                   iter_idx,\n",
    "                                                                   state[0],\n",
    "                                                                   inp,\n",
    "                                                                   state[1][0],\n",
    "                                                                   activated_boundary_states,\n",
    "                                                                   new_appendix_templ+str(0))\n",
    "\n",
    "            not_last_layer_helpers = list()\n",
    "            not_last_layer_helpers.append(helper)\n",
    "            new_state.append((hidden, memory, boundary))\n",
    "            boundaries.append(boundary)\n",
    "            # All layers except for the first and the last ones\n",
    "            if num_layers > 2:\n",
    "                for idx in range(num_layers-2):\n",
    "                    hidden, memory, boundary, helper = self.not_last_layer(idx+1,\n",
    "                                                                           iter_idx,\n",
    "                                                                          state[idx+1],\n",
    "                                                                          hidden,\n",
    "                                                                          state[idx+2][0],\n",
    "                                                                          boundary,\n",
    "                                                                          new_appendix_templ+str(idx+1))\n",
    "                    not_last_layer_helpers.append(helper)\n",
    "                    new_state.append((hidden, memory, boundary))\n",
    "                    boundaries.append(boundary)\n",
    "            hidden, memory = self.last_layer(state[-1],\n",
    "                                             hidden,\n",
    "                                             boundary,\n",
    "                                             new_appendix_templ+str(self._num_layers-1))\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"+appendix),\n",
    "                      \"old_emb_idx\": tf.concat([helper['old_emb_idx'] for helper in not_last_layer_helpers],\n",
    "                                               0,\n",
    "                                               name='old_emb_idx'+appendix),\n",
    "                      \"slice_start\": tf.stack([helper['slice_start'] for helper in not_last_layer_helpers],\n",
    "                                              name=\"slice_start\"+appendix)}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"+appendix), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs,\n",
    "                         appendix):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\"+appendix)\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\"+appendix)\n",
    "            split = tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\"+appendix)\n",
    "            return split\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state,\n",
    "                   appendix):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            new_appendix_templ = appendix + '_unrolling'\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx, new_appendix_templ+str(emb_idx))\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx+appendix)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx+appendix),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx+appendix))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\"+appendix)\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=(\"hidden_concat_in_RNN_module_on_layer%s\"%idx)+appendix)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"+appendix),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"+appendix),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm,\n",
    "                      \"old_emb_idx\": tf.stack([helper['old_emb_idx'] for helper in iteration_helpers],\n",
    "                                              name=\"old_emb_idx\"+appendix),\n",
    "                      \"slice_start\": tf.stack([helper['slice_start'] for helper in iteration_helpers],\n",
    "                                              name=\"slice_start\"+appendix)}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states,\n",
    "                      appendix):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\"+appendix)\n",
    "            output_module_gates = tf.transpose(self.step_function(tf.matmul(concat,\n",
    "                                                                            self.output_module_gates_weights,\n",
    "                                                                            name=\"matmul_in_output_module_gates\"+appendix),\n",
    "                                                                  \"_sigmoid_in_output_module_gates\"+appendix),\n",
    "                                               name=\"output_module_gates\"+appendix)\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\"+appendix)\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=(\"tr_hidden_state_total_%s\"%idx)+appendix)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=(\"tr_gated_hidden_states_%s\"%idx)+appendix))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"+appendix),\n",
    "                                               name=\"gated_hidden_states\"+appendix)\n",
    "            return tf.add(tf.matmul(gated_hidden_states,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits_output\"+appendix),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\"+appendix)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def construct_boundary_test_line(self):\n",
    "        test_line = list()\n",
    "        samples = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "        def add1(index):\n",
    "            test_line.append(list(samples[index]))\n",
    "        def add2(index):\n",
    "            test_line.append(list(samples[index]))\n",
    "            test_line.append(list(samples[index]))\n",
    "        add2(0)\n",
    "        add1(2)\n",
    "        add2(0)\n",
    "        add1(1)\n",
    "        add2(0)\n",
    "        add1(3)\n",
    "        add2(0)\n",
    "        add2(2)\n",
    "        add2(0)\n",
    "        add2(1)\n",
    "        add2(0)\n",
    "        add2(3)\n",
    "        add2(0)\n",
    "        add2(3)\n",
    "        add1(3)\n",
    "        add2(0)\n",
    "        add2(0)\n",
    "        return test_line\n",
    "    \n",
    "    def change_gate_chunk(self,\n",
    "                      layer_idx,\n",
    "                      gate_idx,\n",
    "                          coeff):\n",
    "        positive_coeffs = tf.constant(1., shape=[self._num_nodes[layer_idx]], name=\"positive_coeffs_operation_for_layer%s_gate%s\"%(layer_idx, gate_idx))\n",
    "        other_coeffs = tf.constant(coeff, shape=[self._num_nodes[layer_idx]], name=\"other_coeffs_operation_for_layer%s_gate%s\"%(layer_idx, gate_idx))\n",
    "        list_for_coeffs = list()\n",
    "        for _ in range(4):\n",
    "            list_for_coeffs.append(positive_coeffs)\n",
    "        list_for_coeffs[gate_idx] = other_coeffs\n",
    "        if layer_idx != self._num_layers-1:\n",
    "            list_for_coeffs.append(tf.constant(1., shape=[1], name=\"coeff_for_hard_sigm_operation_for_layer%s_gate%s\"%(layer_idx, gate_idx)))\n",
    "        coeffs = tf.concat(list_for_coeffs, 0, name=\"coeffs_for_layer%s_gate%s\"%(layer_idx, gate_idx))\n",
    "        modification_operation = tf.assign(self.Matrices[layer_idx],\n",
    "                                           tf.multiply(self.Matrices[layer_idx],\n",
    "                                                       coeffs,\n",
    "                                                       name=\"multiply_in_modification_operation_for_layer%s_gate%s\"%(layer_idx, gate_idx)),\n",
    "                                           name=\"modification_operation_for_layer%s_gate%s\"%(layer_idx, gate_idx))\n",
    "        return modification_operation\n",
    "    \n",
    "    def change_flow_chunk(self,\n",
    "                   layer_idx,\n",
    "                   direction,\n",
    "                    coeff):\n",
    "        list_of_coeffs = list()\n",
    "        \n",
    "        if layer_idx != self._num_layers - 1:\n",
    "            number_of_directions_on_layer = 3\n",
    "        else:\n",
    "            number_of_directions_on_layer = 2\n",
    "        for i in range(-1, number_of_directions_on_layer-1):\n",
    "            if i != direction:\n",
    "                current_coeff = 1.\n",
    "            else:\n",
    "                current_coeff = coeff\n",
    "            if not ((layer_idx == 0) and (i == -1)):\n",
    "                list_of_coeffs.append(tf.constant(current_coeff,\n",
    "                                                  shape=[self._num_nodes[layer_idx+i]],\n",
    "                                                  name=\"intermediate_coeffs_for_layer%s_direction%s\"%(layer_idx, i)))\n",
    "            else:\n",
    "                list_of_coeffs.append(tf.constant(current_coeff,\n",
    "                                                  shape=[self._embedding_size],\n",
    "                                                  name=\"intermediate_coeffs_for_layer%s_direction%s\"%(layer_idx, i)))                \n",
    "        coeffs = tf.concat(list_of_coeffs,\n",
    "                               0,\n",
    "                               name=\"coeffs_for_layer%s_direction%s\"%(layer_idx, direction))\n",
    "        modification_operation = tf.assign(self.Matrices[layer_idx],\n",
    "                                           tf.transpose(tf.multiply(tf.transpose(self.Matrices[layer_idx],\n",
    "                                                                                 name=\"transpose_in_modification_operation_for_layer%s_direction%s\"%(layer_idx, direction)),\n",
    "                                                                    coeffs,\n",
    "                                                                    name=\"multiply_in_modification_operation_for_layer%s_direction%s\"%(layer_idx, direction)),\n",
    "                                                        name=\"back_transpose_in_modification_operation_for_layer%s_direction%s\"%(layer_idx, direction)),\n",
    "                                           name=\"modification_operation_for_layer%s_direction%s\"%(layer_idx, direction))\n",
    "        return modification_operation        \n",
    "                \n",
    "    def modifify_matrices(self,\n",
    "                          gates_to_modify,\n",
    "                          directions_to_modify):\n",
    "        mod_ops = list()\n",
    "        if gates_to_modify is not None:\n",
    "            for gate in gates_to_modify:\n",
    "                mod_ops.append(self.change_gate_chunk(*gate))\n",
    "        if directions_to_modify is not None:\n",
    "            for direction in directions_to_modify:\n",
    "                mod_ops.append(self.change_flow_chunk(*direction))\n",
    "        if len(mod_ops) > 0:\n",
    "            return [tf.group(*mod_ops, name=\"final_operation\")]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def do_nothing(self,\n",
    "                   gates_to_modify,\n",
    "                   directions_to_modify):\n",
    "        mod_ops = list()\n",
    "        if gates_to_modify is not None:\n",
    "            for gate in gates_to_modify:\n",
    "                mod_ops.append(self.Matrices[gate[0]])\n",
    "        if directions_to_modify is not None:\n",
    "            for direction_to_modify in directions_to_modify:\n",
    "                mod_ops.append(self.Matrices[direction_to_modify[0]])\n",
    "        return tf.group(*mod_ops, name=\"empty_final_operation\")\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 gates_to_modify=None,\n",
    "                 directions_to_modify=None,\n",
    "                 embedding_size=2):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"type\": 12}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                debug_boundaries = self.construct_boundary_test_line()\n",
    "                self.debug_boundaries = tf.constant(debug_boundaries, name=\"debug_boundaries\")\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.ones([self._vocabulary_size, self._embedding_size],\n",
    "                                                             name=\"embeddings_matrix_initialize\"),\n",
    "                                                     trainable=True,\n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                init_bias_name = \"HM_LSTM_bias_%s_initializer\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\"\n",
    "                \n",
    "                self.Matrices.append(tf.Variable(tf.ones([self._embedding_size + self._num_nodes[0] + self._num_nodes[1],\n",
    "                                                          4 * self._num_nodes[0] + 1],\n",
    "                                                         name=init_matr_name%0),\n",
    "                                                 trainable=False,\n",
    "                                                 name=matr_name%0))\n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[0] + 1],\n",
    "                                                        name=init_bias_name%0),\n",
    "                                               trainable=False,\n",
    "                                               name=bias_name%0))\n",
    "                if self._num_layers > 2:\n",
    "                    for i in range(self._num_layers - 2):\n",
    "                        self.Matrices.append(tf.Variable(tf.ones([self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2],\n",
    "                                                                  4 * self._num_nodes[i+1] + 1],\n",
    "                                                                 name=init_matr_name%(i+1)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=matr_name%(i+1)))\n",
    "                        self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[i+1] + 1],\n",
    "                                                                name=init_bias_name%(i+1)),\n",
    "                                                       trainable=False,\n",
    "                                                       name=bias_name%(i+1)))\n",
    "                self.Matrices.append(tf.Variable(tf.ones([self._num_nodes[-1] + self._num_nodes[-2],\n",
    "                                                          4 * self._num_nodes[-1]],\n",
    "                                                         name=init_matr_name%(self._num_layers-1)),\n",
    "                                                 trainable=False,\n",
    "                                                 name=matr_name%(self._num_layers-1)))     \n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[-1]],\n",
    "                                                        name=init_bias_name%(self._num_layers-1)),\n",
    "                                               trainable=False,\n",
    "                                               name=bias_name%(self._num_layers-1)))\n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.ones([dim_classifier_input, self._num_layers],\n",
    "                                                                       name=\"output_gates_weights_initializer\"),\n",
    "                                                               trainable=False,\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_weights = tf.Variable(tf.ones([dim_classifier_input, self._vocabulary_size],\n",
    "                                                          name=\"output_weights_initializer\"),\n",
    "                                                  trainable=False,\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               trainable=False,\n",
    "                                               name=\"output_bias\")\n",
    "\n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "\n",
    "\n",
    "                    saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 0)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 0)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 1)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 1)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 2)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 2))))\n",
    "                    saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                        tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "\n",
    "                    \"\"\"@tf.RegisterGradient(\"HardSigmoid\")\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\"\"\"\n",
    "\n",
    "                    # appendix is used for constructing of tensor name. It is appended to tensor name\n",
    "                    # to indicate to which part of graph operation belongs\n",
    "                    appendix = \"_train\"\n",
    "                    with tf.name_scope('matrix_modification'):\n",
    "                        mod_op = [tf.cond(tf.equal(self._global_step, 0, name=\"equal_in_mod_op_init\"+appendix),\n",
    "                                              true_fn=lambda: self.modifify_matrices(gates_to_modify, directions_to_modify),\n",
    "                                              false_fn=lambda: self.do_nothing(gates_to_modify, directions_to_modify),\n",
    "                                              name=\"cond_in_mod_op_init\"+appendix)]\n",
    "\n",
    "                    with tf.control_dependencies(mod_op):\n",
    "                        embedded_inputs = self.embedding_module(train_inputs, appendix)\n",
    "                        state, hidden_states, train_helper = self.RNN_module(embedded_inputs, saved_state, appendix)\n",
    "                        logits = self.output_module(hidden_states, appendix)\n",
    "                    \n",
    "                        self.old_emb_idx = tf.reshape(train_helper['old_emb_idx'], [-1], name=\"old_emb_idx\")\n",
    "                        self.slice_start = train_helper['slice_start']\n",
    "                    \n",
    "                        self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"+appendix),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\"+appendix)\n",
    "\n",
    "                        self.save_list = list()\n",
    "                        save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                        for i in range(self._num_layers-1):\n",
    "                            self.save_list.append(tf.assign(saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                            self.save_list.append(tf.assign(saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                            self.save_list.append(tf.assign(saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                        self.save_list.append(tf.assign(saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                        self.save_list.append(tf.assign(saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                        \"\"\"skip operation\"\"\"\n",
    "                        self._skip_operation = tf.group(*self.save_list, name=\"skip_operation\")\n",
    "\n",
    "                        with tf.control_dependencies(self.save_list):\n",
    "                            # Classifier.\n",
    "                            \"\"\"loss\"\"\"\n",
    "                            self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                        # Optimizer.\n",
    "\n",
    "                        # global variables initializer\n",
    "                        self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                        \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                        self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                        self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                        \"\"\"learning rate\"\"\"\n",
    "                        self._learning_rate = tf.train.exponential_decay(10.0,\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                        optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                        gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                        \"\"\"optimizer\"\"\"\n",
    "                        self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                        \"\"\"train prediction\"\"\"\n",
    "                        self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    " \n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "                    appendix = \"_validation\"\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input], appendix)\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state,\n",
    "                                                                                            appendix)\n",
    "                    sample_logits = self.output_module(sample_hidden_states, appendix) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"L2_norm_of_hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = HM_LSTM(1,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 1,\n",
    "                 3,\n",
    "                 [3, 4, 5],\n",
    "                 .001,               # init_slope\n",
    "                 0.001,                  # slope_growth\n",
    "                 100,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                #gates_to_modify=[[2, 3, -1.]],\n",
    "                directions_to_modify=[[0, 0, 0.]],\n",
    "                embedding_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step: 0\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 1\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 2.  2.  2.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 2\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 3.  3.  3.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 3\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 4\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 2.  2.  2.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 5\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 3.  3.  3.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 1.  1.  1.  1.  1.]]\n",
      "step: 6\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 4.  4.  4.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 1.  1.  1.  1.  1.]]\n",
      "step: 7\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 5.  5.  5.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 1.  1.  1.  1.  1.]]\n",
      "step: 8\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 6.  6.  6.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 2.  2.  2.  2.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 2.  2.  2.  2.  2.]]\n",
      "step: 9\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 2.  2.  2.  2.  2.]]\n",
      "step: 10\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 2.  2.  2.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 2.  2.  2.  2.  2.]]\n",
      "step: 11\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 3.  3.  3.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 2.  2.  2.  2.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 2.  2.  2.  2.  2.]]\n",
      "step: 12\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 3.  3.  3.  3.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 2.  2.  2.  2.  2.]]\n",
      "step: 13\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 3.  3.  3.  3.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 2.  2.  2.  2.  2.]]\n",
      "step: 14\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 2.  2.  2.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 3.  3.  3.  3.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 2.  2.  2.  2.  2.]]\n",
      "step: 15\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 3.  3.  3.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 3.  3.  3.  3.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 3.  3.  3.  3.  3.]]\n",
      "step: 16\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 4.  4.  4.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 4.  4.  4.  4.  4.]]\n",
      "step: 17\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 5.  5.  5.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 4.  4.  4.  4.  4.]]\n",
      "step: 18\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 6.  6.  6.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 4.  4.  4.  4.  4.]]\n",
      "step: 19\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 7.  7.  7.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 2.  2.  2.  2.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 5.  5.  5.  5.  5.]]\n",
      "step: 20\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 6.  6.  6.  6.  6.]]\n",
      "step: 21\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 6.  6.  6.  6.  6.]]\n",
      "step: 22\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 2.  2.  2.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 6.  6.  6.  6.  6.]]\n",
      "step: 23\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 3.  3.  3.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 2.  2.  2.  2.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 7.  7.  7.  7.  7.]]\n",
      "step: 24\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 8.  8.  8.  8.  8.]]\n",
      "step: 25\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 9.  9.  9.  9.  9.]]\n",
      "step: 26\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 1.  1.  1.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 9.  9.  9.  9.  9.]]\n",
      "step: 27\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 2.  2.  2.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 9.  9.  9.  9.  9.]]\n",
      "step: 28\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 3.  3.  3.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 9.  9.  9.  9.  9.]]\n",
      "step: 29\n",
      "self.save_list: \n",
      "   [0]: [[ 1.  1.  1.]]\n",
      "   [1]: [[ 4.  4.  4.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 1.  1.  1.  1.]]\n",
      "   [4]: [[ 1.  1.  1.  1.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 1.  1.  1.  1.  1.]]\n",
      "   [7]: [[ 9.  9.  9.  9.  9.]]\n",
      "Number of steps = 35     Percentage = 0.00%     Time = 1s     Learning rate = 10.0000\n"
     ]
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/debug_summary_log\"\n",
    "summary_dict={'summary_collection_frequency': 1,\n",
    "              'summary_tensors': 'self._global_step'}\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            50,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            20,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=35,\n",
    "            print_intermediate_results=True,\n",
    "            add_operations = ['self.save_list'],\n",
    "          print_steps=[i for i in range(30)],\n",
    "          block_validation=True,\n",
    "          path_to_file_for_saving_prints='test_folder/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def carry_out_experiment(parameters, folder):\n",
    "    model = HM_LSTM(1,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 1,\n",
    "                 3,\n",
    "                 parameters[1],       # number of nodes\n",
    "                 .001,               # init_slope\n",
    "                 0.001,                  # slope_growth\n",
    "                 100,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                gates_to_modify=parameters[2],          \n",
    "                directions_to_modify=parameters[3],\n",
    "                embedding_size=parameters[0])           # embedding size\n",
    "    \n",
    "    model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            50,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            20,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=35,\n",
    "            print_intermediate_results=False,\n",
    "            add_operations = ['self.save_list'],\n",
    "          print_steps=[i for i in range(30)],\n",
    "          block_validation=True,\n",
    "          path_to_file_for_saving_prints=folder+'/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_experiments=[[6, [3, 4, 5], [[0, 1, -1.]], None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step: 0\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 1\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 2\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 3\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 4\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 5\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 6\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 7\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 8\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 9\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 10\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 11\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 12\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 13\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 14\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 15\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 16\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 17\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 18\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 19\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 20\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 21\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 22\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 23\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 24\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 25\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 1.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 1.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 26\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 27\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 28\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 29\n",
      "self.save_list: \n",
      "   [0]: [[ 0.  0.  0.]]\n",
      "   [1]: [[ 0.  0.  0.]]\n",
      "   [2]: [[ 0.]]\n",
      "   [3]: [[ 0.  0.  0.  0.]]\n",
      "   [4]: [[ 0.  0.  0.  0.]]\n",
      "   [5]: [[ 0.]]\n",
      "   [6]: [[ 0.  0.  0.  0.  0.]]\n",
      "   [7]: [[ 0.  0.  0.  0.  0.]]\n",
      "Number of steps = 35     Percentage = 0.00%     Time = 0s     Learning rate = 10.0000\n"
     ]
    }
   ],
   "source": [
    "for experiment in list_of_experiments:\n",
    "    carry_out_experiment(experiment, 'test_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
