{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not one byte characters:  0\n",
      "min order index:  9\n",
      "max order index:  255\n",
      "total number of characters:  196\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99350000 n in the February 1934 riots, anarchists divided over a 'united \n",
      "10000 ture in Mutual Aid: A Factor of Evolution (1897). Subsequent ana\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 10000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ\n",
      "char2id(u'a') = 67,  char2id(u'z') = 92,  char2id(u' ') = 2\n",
      "id2char(78) = l,  id2char(156) = Ø,  id2char(140) = È\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'n in the Fe', u\".\\n* ''[[Con\", u\"oldier's so\", u'\\xf6hm-Bawerk ', u'tification,', u' warrior, a', u'uot; would ', u' 115       ', u'orbata acid', u'>\\n      <co', u'ate, the co', u'other natio', u'ing the his', u'et bromine;', u' Christ&quo', u' average]] ', u' their home', u'ks and a ri', u'on]]/[[Joel', u' new era fo', u'aph that th', u' known as t', u's from the ', u'ast majorit', u'trips, thou', u'ent of regi', u'metric aspe', u'd named by ', u'Z</timestam', u'tude of 1 c', u'!&quot; [ht', u'o ==\\n\\n* [[D', u'[[Belarusia', u'iton]], Rus', u'ccessful si', u'es his theo', u' explain th', u' the South.', u'sing with a', u'd ball is h', u'e could des', u'[Friedrich ', u'th virtuall', u' foreign ac', u'variant in ', u'd and watch', u\"t; ''[[Foot\", u' became Lea', u'stern Europ', u' </contribu', u'ese terms n', u'arting in t', u'gence of th', u'of the cons', u'uickly swit', u', thus star', u'lly develop', u'g the offic', u'esult, the ', u'red HMMWV. ', u'ament is de', u'University ', u'&quot;&gt;1', u'solely deco']\n",
      "[u'ebruary 193', u'ncentrate (', u'ong.\\n\\n==Com', u' wrote exte', u', when used', u'and elder h', u' have had o', u'        Sas', u'do]]\\n[[fa: ', u'omment>fix<', u'ombined sal', u'ons who fol', u'story of th', u'; however, ', u'ot; (Mosiah', u' of .847 wa', u'e field [[O', u'idge of fur', u'l Schumache', u'or Battle.n', u'he animator', u'the [[Pacif', u' local [[co', u'ty of execu', u'ugh several', u'istrars, de', u'ects will b', u' [[Bede]] i', u'mp>\\n      <', u'centimetre.', u'ttp://www.e', u'Derivative ', u'an language', u'ssian physi', u'ingles to d', u'ory (ISBN 0', u'he ability ', u'. He was of', u'a synthesiz', u'his last co', u'scribe as e', u' von Wieser', u'ly no-one t', u'ctors or th', u' the first ', u'hed it grow', u'tball World', u'ader of the', u'pe, many of', u'utor>\\n     ', u'not only de', u'the mid-198', u'his scene (', u'spirators]]', u'tched from ', u'rting the S', u'ped than Ma', u'ces of [[Ea', u' leaders of', u' The M1114 ', u'eemed incom', u' System]]\\n*', u'14,772&lt;/', u'orative. Th']\n",
      "[u'tu']\n",
      "[u'ur']\n"
     ]
    }
   ],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices_GL = {\"batch_size\": 0,\n",
    "              \"num_unrollings\": 1,\n",
    "              \"num_layers\": 2,\n",
    "              \"num_nodes\": 3,\n",
    "              \"half_life\": 4,\n",
    "              \"decay\": 5,\n",
    "              \"num_steps\": 6,\n",
    "              \"averaging_number\": 7,\n",
    "              \"type\": 8}\n",
    "\n",
    "\n",
    "class reverse_rnn(MODEL):\n",
    "    def layer(self, \n",
    "              inp_t_or_state_down_t_minus_1,\n",
    "              state_t_minus_1,\n",
    "              state_up_t_minus_1,\n",
    "              layer_num):\n",
    "        X_t = tf.concat(1, [inp_t_or_state_down_t_minus_1,\n",
    "                            state_t_minus_1,\n",
    "                            state_up_t_minus_1])\n",
    "        RES = tf.matmul(X_t, self.Matrices[layer_num]) + self.Biases[layer_num]\n",
    "        state_t = tf.tanh(RES)\n",
    "        return state_t\n",
    "    \n",
    "    def last_layer(self, \n",
    "                   state_down_t_minus_1,\n",
    "                   state_t_minus_1):\n",
    "        X_t = tf.concat(1, [state_down_t_minus_1,\n",
    "                            state_t_minus_1])\n",
    "        RES = tf.matmul(X_t, self.Matrices[-1]) + self.Biases[-1]\n",
    "        state_t = tf.tanh(RES)\n",
    "        return state_t\n",
    "\n",
    "    \n",
    "    def iteration(self, inp, state):\n",
    "        num_layers = len(state)\n",
    "        #print('num_layers: ', num_layers)\n",
    "        new_state = list()\n",
    "        inter_state = self.layer(inp,\n",
    "                                 state[0],\n",
    "                                 state[1],\n",
    "                                 0)\n",
    "        out = inter_state\n",
    "        new_state.append(inter_state)\n",
    "        if num_layers > 2:\n",
    "            for i in range(num_layers-2):\n",
    "                inter_state = self.layer(state[i],\n",
    "                                         state[i+1],\n",
    "                                         state[i+2],\n",
    "                                         i+1)\n",
    "                new_state.append(inter_state)\n",
    "        inter_state = self.last_layer(state[-2],\n",
    "                                      state[-1])\n",
    "        new_state.append(inter_state)\n",
    "        return out, new_state\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 mean=0.,\n",
    "                 stddev=0.1,\n",
    "                 shift=0.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"type\": 8}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with self._graph.device('/gpu:0'): \n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._vocabulary_size + self._num_nodes[0] + self._num_nodes[1],\n",
    "                                                                      self._num_nodes[0]],\n",
    "                                                                     mean=mean, stddev=stddev)))\n",
    "                self.Biases.append(tf.Variable([shift for _ in range(self._num_nodes[0])]))\n",
    "                if self._num_layers > 2:\n",
    "                    for i in range(self._num_layers - 2):\n",
    "                        self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2],\n",
    "                                                                              self._num_nodes[i+1]],\n",
    "                                                                             mean=mean, stddev=stddev)))\n",
    "                        self.Biases.append(tf.Variable([shift for _ in range(self._num_nodes[i+1])]))\n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[-1] + self._num_nodes[-2],\n",
    "                                                                      self._num_nodes[-1]],\n",
    "                                                                     mean=mean, stddev=stddev)))     \n",
    "                self.Biases.append(tf.Variable([shift for _ in range(self._num_nodes[-1])]))\n",
    "\n",
    "                # classifier \n",
    "                weights = tf.Variable(tf.truncated_normal([self._num_nodes[-1], self._vocabulary_size], stddev = 0.1))\n",
    "                bias = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                self._train_data = list()\n",
    "                for _ in range(self._num_unrollings + 1):\n",
    "                    self._train_data.append(\n",
    "                        tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size]))\n",
    "                train_inputs = self._train_data[: self._num_unrollings]\n",
    "                train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                # Unrolled LSTM loop.\n",
    "\n",
    "                saved_state = list()\n",
    "                for i in range(self._num_layers):\n",
    "                    saved_state.append(tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]]), trainable=False))\n",
    "\n",
    "                outputs = list()\n",
    "                state = saved_state\n",
    "                for inp in train_inputs:\n",
    "                    output, state = self.iteration(inp, state)\n",
    "                    outputs.append(output)\n",
    "\n",
    "                save_list = list()\n",
    "                for i in range(self._num_layers):\n",
    "                    save_list.append(saved_state[i].assign(state[i]))\n",
    "                \n",
    "                \"\"\"skip operation\"\"\"\n",
    "                self._skip_operation = tf.group(*save_list)\n",
    "\n",
    "                with tf.control_dependencies(save_list):\n",
    "                        # Classifier.\n",
    "                    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), weights, bias)\n",
    "                    \"\"\"loss\"\"\"\n",
    "                    self._loss = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits, tf.concat(0, train_labels)))\n",
    "                # Optimizer.\n",
    "                self._global_step = tf.Variable(0)\n",
    "                \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                self._half_life = tf.placeholder(tf.int32)\n",
    "                self._decay = tf.placeholder(tf.float32)\n",
    "                \"\"\"learning rate\"\"\"\n",
    "                self._learning_rate = tf.train.exponential_decay(0.5,\n",
    "                                                                 self._global_step,\n",
    "                                                                 self._half_life,\n",
    "                                                                 self._decay,\n",
    "                                                                 staircase=True)\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                \"\"\"optimizer\"\"\"\n",
    "                self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                \"\"\"train prediction\"\"\"\n",
    "                self._train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                saved_sample_state = list()\n",
    "                for i in range(self._num_layers):\n",
    "                    saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[i]]), trainable=False)) \n",
    "                \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size])\n",
    "\n",
    "                reset_list = list()\n",
    "                for i in range(self._num_layers):\n",
    "                    reset_list.append(saved_sample_state[i].assign(tf.zeros([1, self._num_nodes[i]])))\n",
    "\n",
    "                \"\"\"reset sample state\"\"\"\n",
    "                self._reset_sample_state = tf.group(*reset_list)\n",
    "\n",
    "                sample_output, sample_state = self.iteration(self._sample_input, saved_sample_state)\n",
    "\n",
    "                sample_save_list = list()\n",
    "                for i in range(self._num_layers):\n",
    "                    sample_save_list.append(saved_sample_state[i].assign(sample_state[i]))\n",
    "\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    \"\"\"sample prediction\"\"\"\n",
    "                    self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, weights, bias)) \n",
    "                \n",
    "                \n",
    "                \"\"\"saver\"\"\"\n",
    "                self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                            \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append('reverse_rnn')\n",
    "        return metadata\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = reverse_rnn(64,\n",
    "            vocabulary,\n",
    "            characters_positions_in_vocabulary,\n",
    "            10,\n",
    "            3,\n",
    "            [256, 256, 256],\n",
    "            train_text,\n",
    "            valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.167276 learning rate: 0.500000\n",
      "Percentage_of correct: 0.00%\n",
      "================================================================================\n",
      "´ô\n",
      ": àt4£) ¢ëå^wïªzZÝ¥E ï0\":îiÔ0Wh8å?÷]ELx ÿÎÿÏX^¼¾·Ò îbPöø¯?CÕ%Îø\\çîMÊtzå¢vaaÚ\n",
      "CºíØ]«ï2Ö²¾,Åkzf³¼Òêm?å>àwtÃt¬xy .[ßÐ\tIóá³h$ãÛoíZð/HÍÑ¼Þ½BRîµBgó.ËØc×Ôt-ªA®zoìæ\n",
      "Xòæïöýæs0¹çîªe´Á¥¡{+Þ¦é¾(Ñ§Þ¸GºyonvûùÂgKG²0SètÈæÁ1±Íý§&æ4.G4îþï¸Tq[¼àwÁ«Í¼àV¾_Ë\n",
      "½ítuáWçìÌ%oïÎÁÌ¢qT\\¯3ä·Q´Öü-qQÍ,,äqêÅ\\tY(íèTØB}I úæÊþï7gå0W¬DçÌ^ri:¤ÄHo)ÍÊÓåL²»ý\n",
      "£¶R$1`$¾ÉÇq-æË ItWPzdÞ7ÇÉñ÷ïªâØ$@»O\tõ§/¥@YqRTðL¹þY;§¯jlÔÍ? TßÁfuSÒoÑÃêNðñfyyt¤VT\n",
      "================================================================================\n",
      "Validation percentage of correct: 3.22%\n",
      "\n",
      "Average loss at step 400000: 4.261978 learning rate: 0.331710\n",
      "Percentage_of correct: 7.95%\n",
      "Validation percentage of correct: 6.83%\n",
      "\n",
      "Average loss at step 800000: 3.827640 learning rate: 0.220063\n",
      "Percentage_of correct: 9.20%\n",
      "Validation percentage of correct: 9.60%\n",
      "\n",
      "Average loss at step 1200000: 3.644975 learning rate: 0.145994\n",
      "Percentage_of correct: 10.22%\n",
      "Validation percentage of correct: 10.53%\n",
      "\n",
      "Average loss at step 1600000: 3.562257 learning rate: 0.096856\n",
      "Percentage_of correct: 10.50%\n",
      "Validation percentage of correct: 10.44%\n",
      "\n",
      "Average loss at step 2000000: 3.516019 learning rate: 0.064256\n",
      "Percentage_of correct: 12.05%\n",
      "Validation percentage of correct: 10.35%\n",
      "\n",
      "Average loss at step 2400000: 3.490023 learning rate: 0.042629\n",
      "Percentage_of correct: 13.60%\n",
      "Validation percentage of correct: 12.76%\n",
      "\n",
      "Average loss at step 2800000: 3.486297 learning rate: 0.028281\n",
      "Percentage_of correct: 13.68%\n",
      "Validation percentage of correct: 12.77%\n",
      "\n",
      "Average loss at step 3200000: 3.482254 learning rate: 0.018762\n",
      "Percentage_of correct: 13.48%\n",
      "Validation percentage of correct: 12.56%\n",
      "\n",
      "Average loss at step 3600000: 3.483440 learning rate: 0.012447\n",
      "Percentage_of correct: 13.65%\n",
      "Validation percentage of correct: 12.51%\n",
      "\n",
      "Average loss at step 4000000: 3.481161 learning rate: 0.008258\n",
      "Percentage_of correct: 13.40%\n",
      "================================================================================\n",
      "Ìrbo2dlsgcg],pihedCr&naollrr2.h*tapd l9gsd o<ohtso'e t]ontdee tT[d-si gnolelaoos\n",
      "r r[r[fynct8 aldognxtd;]vtp&6\n",
      "At'trco 3|o dd'oAe8stYIg[.tv,traAc|eeamiv.Oni'e[]n\n",
      "¸sd-ttt'9| re[tc  hou  y gwf dir r]ssf a trai' nndyhtd t[prhfobo burneMrbg6To]t \n",
      "ã:irlo]sepyo[wh e{;(eQ.<du-itn tn  rd p)a l egTroeon bhma<srr|  yaiddga'l|eelnBw\n",
      "m]o'-tyrRrm vthosfrZssadlclca|rtu[ i/hiNot]enm neeaahm9o. imheaapTiut] esrse\n",
      "nWl\n",
      "================================================================================\n",
      "Validation percentage of correct: 12.75%\n",
      "\n",
      "Number of steps = 4000001     Percentage = 13.61%     Time = 72468s     Leraning rate = 0.01\n"
     ]
    }
   ],
   "source": [
    "model.run(80,\n",
    "          0.95,\n",
    "            8000,\n",
    "            500,\n",
    "            1.5,\n",
    "            50,\n",
    "            200,\n",
    "            print_intermediate_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps = 20000     Percentage = 46.98%     Time = 656s     Learning rate = 0.0212\n",
      "Number of steps = 20000     Percentage = 47.07%     Time = 656s     Learning rate = 0.0212\n",
      "Number of steps = 20000     Percentage = 47.06%     Time = 664s     Learning rate = 0.0212\n"
     ]
    }
   ],
   "source": [
    "iter_num = 3\n",
    "results_GL = list()   \n",
    "for i in range(iter_num):\n",
    "    model = reverse_rnn(64,\n",
    "                             vocabulary,\n",
    "                             characters_positions_in_vocabulary,\n",
    "                             30,\n",
    "                             2,\n",
    "                             [128, 128],\n",
    "                             train_text,\n",
    "                             valid_text)\n",
    "    model.simple_run(200,\n",
    "                         'reverse_rnn/estimate_variables/average#%s' % i,\n",
    "                            20000,\n",
    "                               4000,\n",
    "                               5000,        #learning has a chance to be stopped after every block of steps\n",
    "                               30,\n",
    "                               0.9,\n",
    "                               3,\n",
    "                    fixed_num_steps=True)\n",
    "    results_GL.extend(model._results)\n",
    "    model.destroy()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
