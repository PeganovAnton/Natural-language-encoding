{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  #Download a file if not present, and make sure it's the right size.\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "def read_data(filename):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        f = zipfile.ZipFile(filename)\n",
    "        for name in f.namelist():\n",
    "            full_text = tf.compat.as_str(f.read(name))\n",
    "        f.close()\n",
    "        \"\"\"f = open('enwik8', 'w')\n",
    "        f.write(text.encode('utf8'))\n",
    "        f.close()\"\"\"\n",
    "    else:\n",
    "        f = open('enwik8', 'r')\n",
    "        full_text = f.read().decode('utf8')\n",
    "        f.close()\n",
    "    return full_text\n",
    "        \n",
    "    f = codecs.open('enwik8', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def check_not_one_byte(text):\n",
    "    not_one_byte_counter = 0\n",
    "    max_character_order_index = 0\n",
    "    min_character_order_index = 2**16 \n",
    "    present_characters = [0]*256\n",
    "    number_of_characters = 0\n",
    "    for i in range(len(text)):\n",
    "        if ord(text[i]) > 255:\n",
    "            not_one_byte_counter += 1 \n",
    "        if len(present_characters) <  ord(text[i]):\n",
    "            present_characters.extend([0]*(ord(text[i]) - len(present_characters) + 1))\n",
    "            present_characters[ord(text[i])] = 1\n",
    "            number_of_characters += 1\n",
    "        elif present_characters[ord(text[i])] == 0:\n",
    "            present_characters[ord(text[i])] = 1\n",
    "            number_of_characters += 1\n",
    "        if ord(text[i]) > max_character_order_index:\n",
    "            max_character_order_index = ord(text[i])\n",
    "        if ord(text[i]) < min_character_order_index:\n",
    "            min_character_order_index = ord(text[i])\n",
    "    return not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters\n",
    "\n",
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_size = 10000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 0\n",
    "valid_size = 10000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "def char2id(char):\n",
    "  if characters_positions_in_vocabulary[ord(char)] != -1:\n",
    "    return characters_positions_in_vocabulary[ord(char)]\n",
    "  else:\n",
    "    print(u'Unexpected character: %s\\nUnexpected character number: %s\\nUnexpected character has its place = %s\\n' % (char, ord(char), present_characters_indices[i]))\n",
    "    return characters_positions_in_vocabulary[ord(char)]\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if (dictid >= 0) and (dictid < vocabulary_size):\n",
    "    return vocabulary[dictid]\n",
    "  else:\n",
    "    print(u\"unexpected id\")\n",
    "    return u'\\0'\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a'), char2id(u'z'), char2id(u' ')))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78), id2char(156), id2char(140)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [u\"\"] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [u\"\".join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches_test = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches_test.next()))\n",
    "print(batches2string(train_batches_test.next()))\n",
    "print(batches2string(valid_batches_test.next()))\n",
    "print(batches2string(valid_batches_test.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def percent_of_correct_predictions(predictions, labels):\n",
    "    num_characters = predictions.shape[0]\n",
    "    num_correct = 0\n",
    "    for i in range(num_characters):\n",
    "        if labels[i, np.argmax(predictions, axis=1)[i]] == 1:\n",
    "            num_correct += 1\n",
    "    return float(num_correct) / num_characters * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "num_nodes = [64]\n",
    "\n",
    "half_life = 3000\n",
    "decay = 0.6\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    with graph.device('/gpu:0'): \n",
    "        #list of LSTM cells in multilayered network\n",
    "        cell_list = list()\n",
    "        for i in range(num_layers):\n",
    "            cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(num_nodes[i], state_is_tuple=True))\n",
    "        LSTM = tf.nn.rnn_cell.MultiRNNCell(cell_list, state_is_tuple=True)\n",
    "\n",
    "        weights_initializer = tf.truncated_normal_initializer(mean=-0.1, stddev=0.1, seed=None, dtype=tf.float32)\n",
    "        LSTM_scope = 'LSTM'\n",
    "        with tf.variable_scope(LSTM_scope, reuse=False):\n",
    "          for i in range(num_layers):\n",
    "            with tf.variable_scope(\"Cell%d\" % i):\n",
    "              with tf.variable_scope(\"BasicLSTMCell\"):\n",
    "                with tf.variable_scope(\"Linear\"):\n",
    "                  if i == 0:\n",
    "                    input_dim = vocabulary_size + cell_list[i].output_size\n",
    "                  else:\n",
    "                    input_dim = cell_list[i-1].output_size + cell_list[i].output_size\n",
    "                  output_dim = 4*cell_list[i].output_size\n",
    "                  tf.get_variable(\"Matrix\", [input_dim, output_dim], initializer=weights_initializer)\n",
    "                  tf.get_variable(\"Bias\", \n",
    "                                  initializer=tf.zeros_initializer(shape=[output_dim], dtype=tf.float32))\n",
    "\n",
    "        # classifier \n",
    "        weights = tf.Variable(tf.truncated_normal([LSTM.output_size, vocabulary_size], stddev = 0.1))\n",
    "        bias = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        train_data = list()\n",
    "        for _ in range(num_unrollings + 1):\n",
    "            train_data.append(\n",
    "                tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        train_inputs = train_data[:num_unrollings]\n",
    "        train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "        # Unrolled LSTM loop.\n",
    "\n",
    "        state = list()\n",
    "        saved_state = list()\n",
    "        for i in range(num_layers):\n",
    "            zero_c, zero_h = cell_list[i].zero_state(batch_size, tf.float32)\n",
    "            shape_c = zero_c.get_shape().as_list()\n",
    "            shape_h = zero_h.get_shape().as_list()\n",
    "            state.append((tf.zeros(shape_c), tf.zeros(shape_h))) \n",
    "            saved_state.append((tf.Variable(tf.zeros(shape_c)), tf.Variable(tf.zeros(shape_h))))\n",
    "\n",
    "        outputs = list()\n",
    "        state = saved_state\n",
    "        for i in train_inputs:\n",
    "            with tf.variable_scope('', reuse=True): \n",
    "                output, state = LSTM(i, state, scope=LSTM_scope)\n",
    "            outputs.append(output)\n",
    "\n",
    "        save_list = list()\n",
    "        for i in range(num_layers):\n",
    "            save_list.append(saved_state[i][0].assign(state[i][0]))\n",
    "            save_list.append(saved_state[i][1].assign(state[i][1]))      \n",
    "\n",
    "        with tf.control_dependencies(save_list):\n",
    "                # Classifier.\n",
    "            logits = tf.nn.xw_plus_b(tf.concat(0, outputs), weights, bias)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, half_life, decay, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Sampling and validation eval: batch 1, no unrolling.\n",
    "        saved_sample_state = list()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            sample_zero_c, sample_zero_h = cell_list[i].zero_state(1, tf.float32)\n",
    "            sample_shape_c = sample_zero_c.get_shape().as_list()\n",
    "            sample_shape_h = sample_zero_h.get_shape().as_list()\n",
    "            saved_sample_state.append((tf.Variable(tf.zeros(sample_shape_c)), tf.Variable(tf.zeros(sample_shape_h))))  \n",
    "\n",
    "        sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "\n",
    "        reset_list = list()\n",
    "        for i in range(num_layers):\n",
    "            reset_list.append(saved_sample_state[i][0].assign(tf.zeros(sample_shape_c)))\n",
    "            reset_list.append(saved_sample_state[i][1].assign(tf.zeros(sample_shape_h)))\n",
    "\n",
    "        reset_sample_state = tf.group(*reset_list)\n",
    "\n",
    "        with tf.variable_scope('', reuse=True): \n",
    "            sample_output, sample_state = LSTM(sample_input, saved_sample_state, scope=LSTM_scope)\n",
    "\n",
    "        sample_save_list = list()\n",
    "        for i in range(num_layers):\n",
    "            sample_save_list.append(saved_sample_state[i][0].assign(sample_state[i][0]))\n",
    "            sample_save_list.append(saved_sample_state[i][1].assign(sample_state[i][1]))    \n",
    "\n",
    "        with tf.control_dependencies(sample_save_list):\n",
    "            sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "num_train_points = 100\n",
    "num_train_points_per_1_validation_point = 10\n",
    "train_frequency = num_steps / num_train_points\n",
    "averaging_number = 10\n",
    "averaging_number = min(averaging_number, train_frequency)\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "data_for_plot = {'train': {'step': list(), 'percentage': list()},\n",
    "                 'validation': {'step': list(), 'percentage': list()}}\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    start_time = time.clock()\n",
    "    average_percentage_of_correct = 0.\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate],\n",
    "                                            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if (step % train_frequency) == 0 or step > ((step / train_frequency + 1) * train_frequency - averaging_number):\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            average_percentage_of_correct += percent_of_correct_predictions(predictions, labels)\n",
    "        if step % train_frequency == 0:\n",
    "            if step > 0:\n",
    "                average_percentage_of_correct /= averaging_number\n",
    "            data_for_plot['train']['step'].append(step)\n",
    "            data_for_plot['train']['percentage'].append(average_percentage_of_correct)\n",
    "            if step % (train_frequency * num_train_points_per_1_validation_point) == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / (train_frequency * num_train_points_per_1_validation_point)\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                print('Percentage_of correct: %.2f%%' % average_percentage_of_correct)\n",
    "                if step % (train_frequency * num_train_points_per_1_validation_point * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = sample(random_distribution())\n",
    "                        sentence = characters(feed)[0]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction)\n",
    "                            sentence += characters(feed)[0]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                validation_percentage_of_correct = 0.\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    validation_percentage_of_correct += percent_of_correct_predictions(predictions, b[1])\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "                validation_percentage_of_correct /= valid_size\n",
    "                print('Validation percentage of correct: %.2f%%\\n' % validation_percentage_of_correct)\n",
    "                data_for_plot['validation']['step'].append(step)\n",
    "                data_for_plot['validation']['percentage'].append(validation_percentage_of_correct)\n",
    "            average_percentage_of_correct = 0.\n",
    "    finish_time = time.clock()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = {0: 'magenta',\n",
    "          1: 'cyan',\n",
    "          2: 'yellow',\n",
    "          3: 'firebrick',\n",
    "          4: 'blue',\n",
    "          5: 'gray',\n",
    "          6: 'm',\n",
    "          7: 'green',\n",
    "          8: 'darkgoldenrod',\n",
    "          9: 'purple',\n",
    "          10: 'k',\n",
    "          11: 'r',\n",
    "          12: '#E24A33',\n",
    "          13: '#92C6FF', \n",
    "          14: '#0072B2',\n",
    "          15: '#30a2da',\n",
    "          16: '#4C72B0',\n",
    "          17: '#8EBA42',\n",
    "          18: '#6d904f'}\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(data_for_plot['train']['step'], data_for_plot['train']['percentage'], colors[0])\n",
    "plt.plot(data_for_plot['validation']['step'], data_for_plot['validation']['percentage'], colors[1])\n",
    "plt.title('Computation took %.f seconds' % (finish_time - start_time))\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('percentage of correct')\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.text(x2 + 0.05 * (x2 - x1), (y2 - y1) * 0.75 + y1, 'train percentage',  va = 'center', ha = 'left', color=colors[0])\n",
    "plt.text(x2 + 0.05 * (x2 - x1), (y2 - y1) * 0.25 + y1, 'validation percentage',  va = 'center', ha = 'left', color=colors[1])\n",
    "nodes_string = \"%s\"\n",
    "if num_layers > 1:\n",
    "    for i in range(num_layers - 1):\n",
    "        nodes_string += \"_%s\"\n",
    "    nodes_string = '(' + nodes_string + ')'\n",
    "plot_filename = \"nl%s_;nn_%s;nu_%s;hl_%s;dc_%s;bs_%s;emb_%s\" % (num_layers,\n",
    "                                                                nodes_string,\n",
    "                                                                num_unrollings,\n",
    "                                                                half_life,\n",
    "                                                                decay,\n",
    "                                                                batch_size,\n",
    "                                                                False)\n",
    "print(plot_filename)\n",
    "plot_filename = (plot_filename % tuple(num_nodes)) + '.png'\n",
    "plt.savefig(plot_filename)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
