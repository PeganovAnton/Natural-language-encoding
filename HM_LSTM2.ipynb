{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 10000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text_1,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "\n",
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\"+appendix)\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=True,\n",
    "                                     name=\"reduce_mean_in_L2_norm\"+appendix)\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\"+appendix)\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.transpose(tf.multiply(tf.transpose(state[2],\n",
    "                                                                       name=\"transposed_state2_in_top_down_prepaired\"),\n",
    "                                                          tf.transpose(top_down,\n",
    "                                                                       name=\"transposed_top_down_in_top_down_prepaired\"),\n",
    "                                                          name=\"multiply_in_top_down_prepaired\"),\n",
    "                                              name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            boundary_state_reversed = tf.subtract(one, state[3], name=\"boundary_state_reversed\")\n",
    "            tr_boundary_state_reversed = tf.transpose(boundary_state_reversed,\n",
    "                                                      name=\"transposed_boundary_state_reversed_in_state0_prepaired\")\n",
    "            tr_state0 = tf.transpose(state[0],\n",
    "                                     name=\"transposed_state0_state0_prepaired\")\n",
    "\n",
    "            state0_prepaired = tf.transpose(tf.multiply(tr_boundary_state_reversed,\n",
    "                                                        tr_state0,\n",
    "                                                        name=\"multiply_in_state0_prepaired\"),\n",
    "                                            name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            [gates_matrix, boundary_matrix] = tf.split(self.Matrices[idx],\n",
    "                                                       [4*self._num_nodes[idx], 1],\n",
    "                                                       axis=1,\n",
    "                                                       name=\"split_matrix\")\n",
    "            [gates_bias, boundary_bias] = tf.split(self.Biases[idx],\n",
    "                                                   [4*self._num_nodes[idx], 1],\n",
    "                                                   name=\"split_bias\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      gates_matrix,\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            gates_bias,\n",
    "                            name=\"concat\")\n",
    "            with self._graph.gradient_override_map({\"MatMul\": \"NewMul%s\"%idx}):\n",
    "                matmul_in_hard_sigm_arg_minus = tf.matmul(X,\n",
    "                                                          boundary_matrix,\n",
    "                                                          name=\"matmul_in_hard_sigm_arg_minus\") \n",
    "            with self._graph.gradient_override_map({\"Add\": \"NewAdd%s\"%idx}):\n",
    "                hard_sigm_arg_minus = tf.add(matmul_in_hard_sigm_arg_minus,\n",
    "                                             boundary_bias,\n",
    "                                             name=\"hard_sigm_arg_minus\")\n",
    "            \n",
    "            matmul_in_hard_sigm_arg = tf.matmul(X,\n",
    "                                                boundary_matrix,\n",
    "                                                name=\"matmul_in_hard_sigm_arg\") \n",
    "            hard_sigm_arg = tf.add(matmul_in_hard_sigm_arg,\n",
    "                                   boundary_bias,\n",
    "                                   name=\"hard_sigm_arg\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat,\n",
    "                                               [3*self._num_nodes[idx], self._num_nodes[idx]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            with tf.name_scope('normal_boundary'):\n",
    "                boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "            with tf.name_scope('minus_boundary'):\n",
    "                boundary_state_minus = self.compute_boundary_state(hard_sigm_arg_minus) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                                      name=\"logical_and_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"),\n",
    "                                                     name=\"to_float_in_copy_flag\"),\n",
    "                                         name=\"copy_flag\")\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                                      name=\"to_float_in_flush_flag\"),\n",
    "                                          name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg}\n",
    "        return new_hidden, new_memory, boundary_state, boundary_state_minus, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "        return new_hidden, new_memory\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with tf.name_scope('boundary_state'):\n",
    "            with self._graph.gradient_override_map({\"Sign\": \"HardSigmoid\"}):\n",
    "                X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "            \"\"\"X = tf.sign(X)\"\"\"\n",
    "            X = tf.divide(tf.add(X,\n",
    "                                 tf.constant([[1.]]),\n",
    "                                 name=\"add_in_compute_boundary_state\"),\n",
    "                          2.,\n",
    "                          name=\"output_of_compute_boundary_state\")\n",
    "            return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "            boundaries_minus = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "            hidden, memory, boundary, boundary_minus, helper = self.not_last_layer(0,\n",
    "                                                                                   state[0],\n",
    "                                                                                   inp,\n",
    "                                                                                   state[1][0],\n",
    "                                                                                   activated_boundary_states)\n",
    "\n",
    "            not_last_layer_helpers = list()\n",
    "            not_last_layer_helpers.append(helper)\n",
    "            new_state.append((hidden, memory, boundary, boundary_minus))\n",
    "            boundaries.append(boundary)\n",
    "            boundaries_minus.append(boundary_minus)\n",
    "            # All layers except for the first and the last ones\n",
    "            if num_layers > 2:\n",
    "                for idx in range(num_layers-2):\n",
    "                    hidden, memory, boundary, boundary_minus, helper = self.not_last_layer(idx+1,\n",
    "                                                                                           state[idx+1],\n",
    "                                                                                           hidden,\n",
    "                                                                                           state[idx+2][0],\n",
    "                                                                                           boundary)\n",
    "                    not_last_layer_helpers.append(helper)\n",
    "                    new_state.append((hidden, memory, boundary, boundary_minus))\n",
    "                    boundaries.append(boundary)\n",
    "                    boundaries_minus.append(boundary_minus)\n",
    "            hidden, memory = self.last_layer(state[-1],\n",
    "                                             hidden,\n",
    "                                             boundary)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\")}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), tf.concat(boundaries_minus, 1, name=\"iteration_boundaries_minus_output\"), helper\n",
    "        \n",
    "    def compute_new_frequencies(self, frequencies, flushes, boundary_states, emb_idx):\n",
    "        with tf.name_scope('compute_new_frequencies_%s'%emb_idx):\n",
    "            boundaries_by_layer = tf.split(boundary_states, self._num_layers - 1, axis=1, name=\"boundaries_by_layer\")\n",
    "            # total number of flushes on layer in the batch\n",
    "            # 'new_flushes' as well as 'flushes' contains information about the moments when FLUSH was performed the last time     \n",
    "            new_flushes = list()\n",
    "            new_frequencies = list()\n",
    "            # 'intervals' contains distance in number of characters between two last FLUSHes. If FLUSH \n",
    "            # is not performed on current character interval is zero\n",
    "            gamma = self._gamma\n",
    "            for layer_idx in range(self._num_layers-1):\n",
    "                with tf.name_scope('for_layer_%s'%layer_idx):\n",
    "                    flush_mask = tf.to_float(tf.equal(tf.reshape(boundaries_by_layer[layer_idx],\n",
    "                                                                 [-1],\n",
    "                                                                 name=\"reshape_in_flush_mask\"),\n",
    "                                                      1.,\n",
    "                                                      name=\"equal_in_flush_mask\"),\n",
    "                                             name=\"flush_mask\")\n",
    "                    not_flush_mask = tf.subtract(1., flush_mask, name=\"not_flush_mask\")\n",
    "                    number_of_flushes_on_layer = tf.reduce_sum(boundaries_by_layer[layer_idx],\n",
    "                                                               name=\"number_of_flushes\")\n",
    "                    flush_term = tf.multiply(flush_mask,\n",
    "                                             tf.to_float(tf.add(tf.multiply(self._global_step,\n",
    "                                                                            self._num_unrollings,\n",
    "                                                                            name=\"multiply_global_step_num_unrollings\"),\n",
    "                                                                emb_idx,\n",
    "                                                                name=\"add_in_flush_term\"),\n",
    "                                                         name=\"to_float_in_flush_term\"),\n",
    "                                             name=\"flush_term\")\n",
    "                    not_flush_term = tf.multiply(not_flush_mask,\n",
    "                                                 flushes[layer_idx],\n",
    "                                                 name=\"not_flush_term\")\n",
    "                    new_flushes_on_layer = tf.add(flush_term, not_flush_term, name=\"new_flushes\")\n",
    "                    intervals_on_layer = tf.subtract(new_flushes_on_layer, flushes[layer_idx], name=\"intervals\")\n",
    "                    new_flushes.append(new_flushes_on_layer)                                \n",
    "                    effective_interval = tf.divide(tf.reduce_sum(intervals_on_layer,\n",
    "                                                                 name=\"reduce_sum_in_mean_interval\"),\n",
    "                                                   float(self._batch_size),\n",
    "                                                   name=\"effective_interval\")\n",
    "                    number_flushes_factor = tf.divide(number_of_flushes_on_layer,\n",
    "                                                      float(self._batch_size),\n",
    "                                                      name=\"number_flushes_factor\")\n",
    "                    effective_antigamma = tf.subtract(1.,\n",
    "                                                      tf.multiply(number_flushes_factor,\n",
    "                                                                  gamma,\n",
    "                                                                  name=\"effective_gamma\"),\n",
    "                                                      name=\"effective_antigamma\")\n",
    "                    gamma_term = tf.multiply(effective_interval, gamma, name=\"gamma_term\")\n",
    "                    anti_gamma_term = tf.multiply(frequencies[layer_idx], effective_antigamma, name=\"anti_gamma_term\")\n",
    "                    new_frequencies.append(tf.add(gamma_term, anti_gamma_term, name=\"new_frequency\"))\n",
    "            return new_frequencies, new_flushes\n",
    "                                                      \n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state,\n",
    "                   regime='validation'):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            saved_iteration_boundaries_minus = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            \n",
    "            # variables for controling frequency\n",
    "            if regime == 'train':\n",
    "                frequencies = self.saved_average_frequencies\n",
    "                last_flushes = self.saved_last_flushes\n",
    "            \n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, iteration_boundaries_minus, helper = self.iteration(emb, state, emb_idx)\n",
    "                if regime == 'train':\n",
    "                    frequencies, last_flushes = self.compute_new_frequencies(frequencies,\n",
    "                                                                             last_flushes,\n",
    "                                                                             iteration_boundaries,\n",
    "                                                                             emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                saved_iteration_boundaries_minus.append(iteration_boundaries_minus)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"L2_norm_before_reshaping_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"all_boundaries_minus\": tf.stack(saved_iteration_boundaries,\n",
    "                                                       axis=1,\n",
    "                                                       name=\"stack_of_boundaries_minus\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm}\n",
    "            if regime == 'train':\n",
    "                return state, saved_hidden_states, helper, frequencies, last_flushes\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.transpose(tf.sigmoid(tf.matmul(concat,\n",
    "                                                                    self.output_module_gates_weights,\n",
    "                                                                    name=\"matmul_in_output_module_gates\"),\n",
    "                                                          name=\"sigmoid_in_output_module_gates\"),\n",
    "                                               name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=\"tr_hidden_state_total_%s\"%idx)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=\"tr_gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"),\n",
    "                                               name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.tanh(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "            \n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "    \n",
    "    def tune(self,\n",
    "             new_frequencies,\n",
    "             last_flushes):\n",
    "        with tf.name_scope('tuning'):\n",
    "            list_of_operations = list()\n",
    "            difference_computing_list = list()\n",
    "            gamma = self._gamma\n",
    "            for layer_idx in range(self._num_layers - 1):\n",
    "                with tf.name_scope('for_layer%s'%layer_idx):\n",
    "                    frequencies_difference = tf.subtract(new_frequencies[layer_idx],\n",
    "                                                         self._desired_frequencies[layer_idx],\n",
    "                                                         name=\"frequencies_difference\")\n",
    "                    frequencies_to_pass_to_assign = tf.add(self._desired_frequencies[layer_idx],\n",
    "                                                           frequencies_difference,\n",
    "                                                           name=\"frequencies_to_pass_to_assign\")\n",
    "                    with tf.name_scope('stuck'):\n",
    "                        changes_in_flushes = tf.subtract(last_flushes[layer_idx],\n",
    "                                                         self.saved_last_flushes[layer_idx],\n",
    "                                                         name=\"changes_in_flushes\")\n",
    "                        stuck_mask = tf.to_float(tf.equal(changes_in_flushes,\n",
    "                                                          0.,\n",
    "                                                          name=\"equal_in_stuck_mask\"),\n",
    "                                                 name=\"stuck_mask\")\n",
    "                        not_stuck_mask = tf.subtract(1., stuck_mask, name=\"not_stuck_mask\")\n",
    "                        stuck_value = tf.to_float(tf.multiply(self._global_step,\n",
    "                                                              self._num_unrollings,\n",
    "                                                              name=\"multiply_in_stuck_value\"),\n",
    "                                                  name=\"stuck_value\")\n",
    "                        stuck_term = tf.multiply(stuck_value, stuck_mask, name=\"stuck_term\")\n",
    "                        not_stuck_term = tf.multiply(last_flushes[layer_idx], not_stuck_mask, name=\"not_stuck_term\")\n",
    "                        new_flushes = tf.add(stuck_term, not_stuck_term, name=\"new_flushes\")\n",
    "                        \n",
    "                        number_stuck_flushes = tf.reduce_sum(stuck_mask, name=\"number_stuck_flushes\")\n",
    "                        effective_interval = tf.divide(tf.multiply(float(self._num_unrollings),\n",
    "                                                                   number_stuck_flushes,\n",
    "                                                                   name=\"multiply_in_effective_interval\"),\n",
    "                                                       float(self._batch_size),\n",
    "                                                       name=\"effective_interval\")\n",
    "                        number_flushes_factor = tf.divide(number_stuck_flushes,\n",
    "                                                          float(self._batch_size),\n",
    "                                                          name=\"number_flushes_factor\")\n",
    "                        effective_antigamma = tf.subtract(1.,\n",
    "                                                          tf.multiply(number_flushes_factor,\n",
    "                                                                      gamma,\n",
    "                                                                      name=\"effective_gamma\"),\n",
    "                                                          name=\"effective_antigamma\")\n",
    "                        gamma_term = tf.multiply(effective_interval, gamma, name=\"gamma_term\")\n",
    "                        anti_gamma_term = tf.multiply(frequencies_to_pass_to_assign, effective_antigamma, name=\"anti_gamma_term\")\n",
    "                        frequencies_to_pass_to_assign = tf.add(gamma_term, anti_gamma_term, name=\"new_frequency\")\n",
    "\n",
    "                    list_of_operations.append(tf.assign(self.saved_average_frequencies[layer_idx],\n",
    "                                                        frequencies_to_pass_to_assign,\n",
    "                                                        name=\"assign_frequency\"))\n",
    "                    list_of_operations.append(tf.assign(self.saved_last_flushes[layer_idx],\n",
    "                                                        new_flushes,\n",
    "                                                        name=\"assign_flushes\")) \n",
    "                    unclipped_shift = tf.negative(tf.multiply(frequencies_difference,\n",
    "                                                              self._tuning_speed,\n",
    "                                                              name=\"multiply_in_shift\"),\n",
    "                                                  name=\"unclipped_shift\")\n",
    "                    shift_sign = tf.sign(unclipped_shift, name=\"difference_sign\")\n",
    "                    abs_shift = tf.minimum(tf.abs(frequencies_difference,\n",
    "                                                  name=\"abs_in_abs_shift\"),\n",
    "                                           self._max_shift,\n",
    "                                           name=\"abs_shift\") \n",
    "                    shift = tf.multiply(abs_shift, shift_sign, name=\"shift\")\n",
    "                    list_of_operations.append(tf.assign_add(self.alphas[layer_idx], shift, name=\"new_alpha\"))\n",
    "            return list_of_operations\n",
    "                \n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1.,\n",
    "                 gamma=.02,                      # parameter used for computing exponential average of frequency\n",
    "                 desired_frequencies=[5., 20.],  # desired average number steps between passing information to next layer\n",
    "                 init_alphas=[1., 1.],           # alpha parameters for overiding gradients\n",
    "                 tuning_speed=.001,               # this parameter defines the coefficient which is used for modifying alphas\n",
    "                 max_shift=0.):                  # max difference of alpha\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._gamma = gamma\n",
    "        self._desired_frequencies = desired_frequencies\n",
    "        self._init_alphas = init_alphas\n",
    "        self._tuning_speed = tuning_speed\n",
    "        self._max_shift = max_shift\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"gamma\": 14,\n",
    "                         \"desired_frequencies\": 15,\n",
    "                         \"init_alphas\": 16,\n",
    "                         \"tuning_speed\": 17,\n",
    "                         \"max_shift\": 18,\n",
    "                         \"type\": 19}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                init_bias_name = \"HM_LSTM_bias_%s_initializer\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\"\n",
    "                \n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._embedding_size + self._num_nodes[0] + self._num_nodes[1],\n",
    "                                                                      4 * self._num_nodes[0] + 1],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(self._init_parameter/(self._embedding_size+self._num_nodes[0]+self._num_nodes[1])),\n",
    "                                                                     name=init_matr_name%0),\n",
    "                                                 name=matr_name%0))\n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[0] + 1],\n",
    "                                                        name=init_bias_name%0),\n",
    "                                               name=bias_name%0))\n",
    "                if self._num_layers > 2:\n",
    "                    for i in range(self._num_layers - 2):\n",
    "                        self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2],\n",
    "                                                                              4 * self._num_nodes[i+1] + 1],\n",
    "                                                                             mean=0.,\n",
    "                                                                             stddev=math.sqrt(self._init_parameter/(self._num_nodes[i]+self._num_nodes[i+1]+self._num_nodes[i+2])),\n",
    "                                                                             name=init_matr_name%(i+1)),\n",
    "                                                         name=matr_name%(i+1)))\n",
    "                        self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[i+1] + 1],\n",
    "                                                                name=init_bias_name%(i+1)),\n",
    "                                                       name=bias_name%(i+1)))\n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[-1] + self._num_nodes[-2],\n",
    "                                                                      4 * self._num_nodes[-1]],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(self._init_parameter/(self._num_nodes[-1]+self._num_nodes[-2])),\n",
    "                                                                     name=init_matr_name%(self._num_layers-1)),\n",
    "                                                 name=matr_name%(self._num_layers-1)))     \n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[-1]],\n",
    "                                                        name=init_bias_name%(self._num_layers-1)),\n",
    "                                               name=bias_name%(self._num_layers-1)))\n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "\n",
    "\n",
    "                    saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 0)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 0)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 1)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 1)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 2)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 2)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 3)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 3))))                                            \n",
    "                    saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                        tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "                    # coefficients used for gradient overriding\n",
    "                    self.alphas = list()\n",
    "                    for alpha_idx, init_value in enumerate(self._init_alphas):\n",
    "                        self.alphas.append(tf.Variable(init_value, trainable=False, name=\"alpha%s\"%alpha_idx))\n",
    "\n",
    "                    # saved average frequencies\n",
    "                    self.saved_average_frequencies = list()\n",
    "                    for frequency_idx, init_value in enumerate(self._desired_frequencies):\n",
    "                        self.saved_average_frequencies.append(tf.Variable(init_value, trainable=False, name=\"frequency%s\"%frequency_idx))\n",
    "                        \n",
    "                    # saved last upgrade steps\n",
    "                    self.saved_last_flushes = list()\n",
    "                    for layer_idx in range(self._num_layers-1):\n",
    "                        self.saved_last_flushes.append(tf.Variable(tf.zeros([self._batch_size],\n",
    "                                                                            name=\"saved_last_flushes_init_on_layer%s\"%layer_idx),\n",
    "                                                                   trainable=False,\n",
    "                                                                   name=\"saved_last_flushes_init_on_layer%s\"%layer_idx))\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "\n",
    "                    @tf.RegisterGradient(\"HardSigmoid\")\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "                    \n",
    "                    # this gradient is used for controlling the effect that missing (1-z) factor in formula (5) has on\n",
    "                    # boundary state                    \n",
    "                    def create_gradient_functions(idx):\n",
    "                        @tf.RegisterGradient(\"NewMul%s\"%idx)\n",
    "                        def NewMulGrad(op, grad):\n",
    "\n",
    "                            t_a = op.get_attr(\"transpose_a\")\n",
    "                            t_b = op.get_attr(\"transpose_b\")\n",
    "                            a = math_ops.conj(op.inputs[0])\n",
    "                            b = math_ops.conj(op.inputs[1])\n",
    "                            if not t_a and not t_b:\n",
    "                                grad_a = math_ops.matmul(grad, b, transpose_b=True)\n",
    "                                grad_b = math_ops.matmul(a, grad, transpose_a=True)\n",
    "                            elif not t_a and t_b:\n",
    "                                grad_a = math_ops.matmul(grad, b)\n",
    "                                grad_b = math_ops.matmul(grad, a, transpose_a=True)\n",
    "                            elif t_a and not t_b:\n",
    "                                grad_a = math_ops.matmul(b, grad, transpose_b=True)\n",
    "                                grad_b = math_ops.matmul(a, grad)\n",
    "                            elif t_a and t_b:\n",
    "                                grad_a = math_ops.matmul(b, grad, transpose_a=True, transpose_b=True)\n",
    "                                grad_b = math_ops.matmul(grad, a, transpose_a=True, transpose_b=True)\n",
    "                            return grad_a, self.alphas[idx]*grad_b\n",
    "                        @tf.RegisterGradient(\"NewAdd%s\"%idx)\n",
    "                        def NewAddGrad(op, grad):\n",
    "                            x = op.inputs[0]\n",
    "                            y = op.inputs[1]\n",
    "                            sx = array_ops.shape(x)\n",
    "                            sy = array_ops.shape(y)\n",
    "                            rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n",
    "                            return (array_ops.reshape(math_ops.reduce_sum(grad, rx), sx),\n",
    "                                    self.alphas[idx]*array_ops.reshape(math_ops.reduce_sum(grad, ry), sy))\n",
    "                \n",
    "                    for layer_idx in range(self._num_layers-1):\n",
    "                        create_gradient_functions(layer_idx)\n",
    "\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper, new_frequencies, last_flushes = self.RNN_module(embedded_inputs,\n",
    "                                                                                                        saved_state,\n",
    "                                                                                                        regime='train')\n",
    "                    tuning_operations = self.tune(new_frequencies, last_flushes)\n",
    "                    with tf.control_dependencies(tuning_operations):\n",
    "                        logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.boundary_matrices = list()\n",
    "                    for layer_idx in range(self._num_layers - 1):\n",
    "                        _, boundary_matrix = tf.split(self.Matrices[layer_idx],\n",
    "                                                      [4*self._num_nodes[layer_idx], 1],\n",
    "                                                      axis=1,\n",
    "                                                      name=\"split_for_boundary_matrix_%s\"%layer_idx)\n",
    "                        self.boundary_matrices.append(tf.reshape(boundary_matrix,\n",
    "                                                                 [-1],\n",
    "                                                                 name=\"boundary_matrix_%s\"%layer_idx))\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                        save_list.append(tf.assign(saved_state[i][3],\n",
    "                                                   state[i][3],\n",
    "                                                   name=save_list_templ%(i, 3)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 3)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 3))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][3],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 3)),\n",
    "                                                    name=reset_list_templ%(i, 3)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state,\n",
    "                                                                                            regime='validation')\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][3],\n",
    "                                                          sample_state[i][3],\n",
    "                                                          name=save_list_templ%(i, 3)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.boundary_minus = tf.reshape(validation_helper[\"all_boundaries_minus\"], [-1], name=\"sample_boundary_minus\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(list(self._num_nodes))\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._gamma)\n",
    "        metadata.append(list(self._desired_frequencies))\n",
    "        metadata.append(list(self._init_alphas))\n",
    "        metadata.append(self._tuning_speed)\n",
    "        metadata.append(self._max_shift)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = HM_LSTM(50,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [112, 92, 102],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.278076 learning rate: 8.781140\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "uA¨\"MÀ`0ò Ò×¾ªÒíÚdü  -uÀdß4×ÏQÇæ¼iaÁÜ}&IFNØ®Üð¤r¯Áÿ7ÐL:òÅðMöQõì÷*æ`ÞNxl<-ã­ù 7E\n",
      "'y~zÅbë/$2fV9x .Úë>$òõk¿ÁH¹{²] ØøÔ<¸¾g4GßÿAüÔªLh®H¹n~0³n°8VãSV²¥:4ßhz=ØªÏ<Á(7Âó\n",
      "\"ìç,¢4ymx@¼ÃkO»Âíé%÷&Å·³¬`Hr¸½\t/U¡´Êx8ùÿ`¥ØF/%æTxQbe#ô%9­ð_Ò\n",
      "s­²ÕØ¨CiîÞ:;OÃ®;Cä\n",
      "u\\e<~0ê d«Ù\"Í¿ÿ\tÛ 99ª½Ï /ðã.2Àß êä\" eL¢Øs Ò ôÛ UéØý«ÃÛ»øFûOêë'(åËÅ¤ }|%2¸(ãe. æ\n",
      "L@¸WHXûnåóÑ\"9kýÍÄ odaØ9ÍweÎ 4¤ì¥®s!&¤yMe%è­'§ÎäR VÝã­{¹¤5eO?j a¦Ce§ãT¦ïl õ?ú\n",
      "À \n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [ -7.13297119e-03  -4.73072214e-06]\n",
      "1:\n",
      "self.sigm_arg:  [ -2.44801003e-03  -4.73072214e-06]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.01214465 -0.0008235 ]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.01977807  0.0037915 ]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.08687195 -0.00465531]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.02139721  0.00337791]\n",
      "6:\n",
      "self.sigm_arg:  [-0.03068236  0.00525481]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.09438723 -0.00439097]\n",
      "8:\n",
      "self.sigm_arg:  [-0.01656973 -0.00060848]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.00080296  0.00084963]\n",
      "Validation percentage of correct: 13.10%\n",
      "\n",
      "step: 2\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 3.34498\n",
      "   [1]: 10.9451\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 59.  58.  59.  59.  59.  59.  59.  59.  59.  59.  59.  59.  57.  59.  59.\n",
      "  58.  59.  59.  59.  58.  59.  58.  59.  58.  58.  59.  59.  59.  59.  59.\n",
      "  59.  59.  59.  59.  59.  59.  59.  59.  59.  57.  59.  58.  59.  55.  57.\n",
      "  59.  59.  56.  59.  58.]\n",
      "   [1]: [ 59.  57.  59.  51.  44.  59.  58.  59.  59.  59.  59.  59.  59.  58.  55.\n",
      "  57.  59.  59.  59.  59.  59.  57.  59.  57.  59.  59.  58.  59.  59.  59.\n",
      "  43.  59.  57.  59.  59.  59.  59.  59.  59.  55.  59.  59.  58.  54.  59.\n",
      "  58.  59.  59.  58.  57.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00011026  0.05218226 -0.02234529  0.00694805  0.03302929 -0.06524235\n",
      " -0.05989112  0.03687745 -0.07292347  0.07204233  0.01908029 -0.04832824\n",
      "  0.03080817  0.06691229 -0.02332219  0.02154074 -0.01533177  0.06694882\n",
      "  0.05635941  0.02005884 -0.02417503 -0.00930644 -0.02883074  0.06112429\n",
      "  0.00835081  0.0313825   0.04835791  0.07783852 -0.06963007 -0.03015075\n",
      " -0.06138599  0.03188998 -0.04919959  0.01921471 -0.08506208 -0.01329075\n",
      " -0.06517196 -0.06983909  0.03625596 -0.00609786  0.00716133 -0.06412799\n",
      "  0.01094367  0.00436273  0.01075662 -0.01559011  0.00718484 -0.04423679\n",
      "  0.0421363  -0.01850908  0.04356245  0.01665615 -0.01049646  0.06369561\n",
      " -0.09879529  0.06484045  0.07509665 -0.00073822 -0.00664045 -0.00364637\n",
      " -0.01604084  0.01280125  0.10891118 -0.03439979  0.03752337 -0.05961461\n",
      " -0.04583155  0.03884608  0.05764791 -0.10328977 -0.01090309 -0.07520913\n",
      "  0.09734134  0.0444479  -0.03939951  0.06902131  0.00982816  0.02857725\n",
      "  0.04460588 -0.01999116  0.00186792 -0.08269259 -0.00988729  0.05871532\n",
      "  0.03343479  0.00467847 -0.05111473 -0.02334941  0.02969522  0.04024813\n",
      " -0.02287657 -0.02137943  0.0453661   0.00293595  0.02136605 -0.03859501\n",
      " -0.02776527  0.02475958 -0.06946909  0.00726995  0.01849658 -0.04268843\n",
      " -0.0263952   0.03253547 -0.01870258  0.06602712  0.00787516  0.0305498\n",
      " -0.01297091  0.06380955  0.00383707  0.02555219 -0.01507885  0.05839655\n",
      "  0.02648744  0.00312811 -0.089076   -0.0186947  -0.00033122 -0.0229202\n",
      " -0.03724208  0.10603139  0.08053271 -0.06079138  0.08225852  0.05047754\n",
      "  0.00803616 -0.01954464  0.02930865  0.0137822  -0.0714818   0.01835815\n",
      " -0.05397812 -0.00570283 -0.00631094 -0.04716538 -0.05601508  0.02747257\n",
      " -0.05328626  0.09788271  0.05141069 -0.03827968 -0.01247711  0.01317897\n",
      " -0.02162201 -0.02546299  0.09490304 -0.07369401  0.01367586 -0.01627259\n",
      "  0.0104161  -0.00273379 -0.02857916 -0.03494668 -0.08879095 -0.01355727\n",
      "  0.07910457 -0.06513889  0.03893147  0.04990415  0.07126296 -0.00588466\n",
      " -0.10057071 -0.07958495  0.01774388 -0.02228217 -0.07239519  0.05586403\n",
      "  0.00958447  0.02958828 -0.06031886 -0.04443987  0.03818992 -0.02058721\n",
      "  0.04476015  0.07785532  0.02417598 -0.05123406  0.02529949 -0.00354237\n",
      "  0.04509437  0.07104884 -0.0125833  -0.02711392 -0.0291778  -0.06004534\n",
      " -0.04325688 -0.07242904 -0.0663748   0.0403153   0.02547911  0.07258438\n",
      " -0.0269324   0.04315535  0.01957632 -0.00627292 -0.02602567 -0.00291701\n",
      "  0.08258445 -0.0695233  -0.00663569  0.06689794  0.04865811  0.01934842\n",
      "  0.09115462 -0.05237525 -0.01144268 -0.04107161  0.05315593 -0.06842934\n",
      "  0.04771369 -0.01879411  0.02543181 -0.04657699  0.04808798  0.06344582\n",
      "  0.01610725  0.06842677 -0.10910381 -0.01700981  0.00589494 -0.07754076\n",
      "  0.04873559  0.06715701  0.02426055 -0.02644004  0.03468363 -0.06398994\n",
      " -0.05522844 -0.0254983  -0.01056502 -0.08206156  0.00435032  0.00294202\n",
      " -0.05115137 -0.07283729  0.02552605 -0.01330868  0.01727588 -0.08971445\n",
      " -0.003974   -0.02883236 -0.02784631 -0.07665877  0.00311422  0.03179204\n",
      "  0.04305389  0.00376586 -0.02965908 -0.02979013  0.01810026  0.05095489\n",
      " -0.09840739 -0.00595898 -0.05660725 -0.0039019  -0.00227551 -0.06166978\n",
      " -0.06867301  0.03343165  0.05764351  0.04952319 -0.07207871  0.07484955\n",
      "  0.07343838  0.00943626  0.10869203  0.01594286  0.00072972 -0.04848999\n",
      "  0.05170831  0.00436515 -0.04434691 -0.00540861 -0.0200626   0.06716235\n",
      " -0.08025622  0.02322841  0.04717104  0.00869316  0.05221532  0.01023385\n",
      "  0.01639787 -0.03549295  0.05225309  0.08499961  0.05156444 -0.08257762\n",
      " -0.04205799 -0.00840813 -0.04901866  0.07496732  0.07644895  0.0269831\n",
      " -0.06409544 -0.00228301  0.02818354  0.01051021 -0.05334139  0.01955895\n",
      "  0.01975667 -0.107292    0.03546699  0.09033851 -0.07460183  0.036464\n",
      "  0.05024754  0.01537925  0.0490127   0.02791057 -0.0429433  -0.06214761\n",
      "  0.04221353 -0.041871    0.04010274 -0.00362043 -0.02950167 -0.01001524\n",
      " -0.00195853 -0.018232   -0.03496523  0.07324656 -0.01711766 -0.03487767\n",
      "  0.03015815  0.03289692 -0.04633885  0.0256763  -0.0637503   0.01752039\n",
      "  0.00517164  0.00516758]\n",
      "   [1]: [ -4.14744355e-02  -8.95774085e-03   7.47957686e-03  -3.70600745e-02\n",
      "  -7.01356158e-02   5.39043546e-02  -5.78542538e-02   3.85356955e-02\n",
      "  -4.48063798e-02  -8.46423879e-02  -8.80414806e-03   3.02013010e-02\n",
      "   9.67556313e-02   6.29423559e-02   2.88035814e-02   1.10482506e-01\n",
      "   6.11722022e-02   1.54795256e-02   5.47036491e-02   1.08308473e-03\n",
      "   1.98608432e-02  -5.17354831e-02  -1.08153366e-01  -6.89713657e-02\n",
      "   3.00532263e-02   6.00005686e-02  -1.09658977e-02   8.11588019e-02\n",
      "   3.91376987e-02   4.84365299e-02  -3.56481783e-03  -7.73664713e-02\n",
      "   2.85773240e-02  -1.05449297e-02   1.85982324e-04   2.65617259e-02\n",
      "   3.97510566e-02   8.02454054e-02  -3.79342660e-02   3.93168852e-02\n",
      "   6.04994334e-02  -7.46096894e-02   1.44821324e-03   4.12236787e-02\n",
      "   1.90620217e-02  -1.12087525e-01  -4.22748365e-02  -1.75661370e-02\n",
      "  -8.83730426e-02   6.79820776e-03  -6.97045922e-02   4.54354510e-02\n",
      "  -3.62480395e-02  -1.03115313e-01  -5.23535609e-02   5.64331142e-03\n",
      "  -3.93329076e-02   5.71333431e-03   3.88194248e-02  -1.67039763e-02\n",
      "   5.17468452e-02  -4.93031275e-03   9.07465890e-02  -8.10193345e-02\n",
      "  -8.00469369e-02  -5.83978975e-03   5.18189818e-02   1.98678840e-02\n",
      "  -2.76857261e-02   1.48129873e-02   3.91018428e-02   5.08714914e-02\n",
      "  -3.10961790e-02   3.99616770e-02  -5.59471920e-02   1.96157694e-02\n",
      "  -1.71626583e-02   3.37271467e-02  -4.51124609e-02  -6.91817179e-02\n",
      "  -3.77276354e-02   3.53243910e-02   4.78078909e-02  -7.28518814e-02\n",
      "   2.05002055e-02   2.45901998e-02   8.75700824e-03   5.52364215e-02\n",
      "   5.70302606e-02  -7.60180131e-02  -5.51014580e-02   8.78812373e-02\n",
      "  -3.23807895e-02   5.00111021e-02   3.49642783e-02   3.19404975e-02\n",
      "  -2.13961005e-02  -3.24458927e-02  -3.10702752e-02   1.30172092e-02\n",
      "   2.69121882e-02   6.51339889e-02   8.14696774e-02   2.55542640e-02\n",
      "  -5.81963640e-03   5.06313704e-03  -4.16570827e-02  -1.25513170e-02\n",
      "   4.47107032e-02   1.46093005e-02   5.70095889e-02   4.68878374e-02\n",
      "   4.11673039e-02  -1.10855810e-02   1.45429820e-02   9.69858244e-02\n",
      "   4.43658717e-02  -1.14593143e-02   6.27773777e-02  -6.60676858e-04\n",
      "  -2.12718975e-02   1.10776573e-01   6.02203533e-02  -8.64158645e-02\n",
      "  -1.37125887e-02   2.20903773e-02   6.81310566e-03   1.86972506e-02\n",
      "  -8.13938081e-02  -1.44543184e-03   9.33245197e-03  -1.67369656e-02\n",
      "  -2.19490584e-02   1.24185421e-02   5.26858903e-02  -1.00834720e-01\n",
      "  -4.55249362e-02  -6.68791160e-02   1.90637000e-02   7.29955435e-02\n",
      "   8.66876915e-02  -7.80977383e-02  -8.29732139e-03  -2.63074785e-02\n",
      "  -1.24757811e-02   1.32958889e-02   1.59481578e-02  -7.22342879e-02\n",
      "  -5.51917590e-02   1.35870967e-02   2.92440001e-02   9.11463425e-03\n",
      "  -8.18607137e-02   9.40263364e-03   3.91673706e-02  -7.20877200e-02\n",
      "   4.85788547e-02  -7.32781366e-02   3.20953690e-02  -7.72040039e-02\n",
      "   4.51676873e-03  -7.07559586e-02   2.00458299e-02  -1.26129864e-02\n",
      "   1.06675990e-01  -5.00395000e-02   7.30673224e-03   2.11709663e-02\n",
      "  -3.89376916e-02   8.14474598e-02  -3.50165106e-02   3.45005170e-02\n",
      "  -1.21407665e-03  -7.38235861e-02  -4.46204171e-02   5.49926646e-02\n",
      "   3.29763326e-03   9.09797102e-02   8.46882164e-02   2.07812283e-02\n",
      "  -2.95207705e-02  -1.98165011e-02  -3.62919345e-02   9.11286995e-02\n",
      "   5.81138954e-02   9.81195495e-02   2.21711360e-02  -5.61287329e-02\n",
      "  -4.23332676e-02  -4.37735245e-02   7.12159052e-02  -2.72340495e-02\n",
      "  -3.36315110e-03   3.11948340e-02   7.31250318e-03   5.95263056e-02\n",
      "  -1.09707199e-01   2.65655760e-02   1.02294609e-01  -1.14052348e-01\n",
      "   5.71656935e-02  -9.89176035e-02  -1.51667623e-02   4.97115441e-02\n",
      "   6.08677417e-02  -5.77311255e-02   4.88129035e-02   9.72360075e-02\n",
      "   7.63441261e-04   2.17855144e-02  -2.40083858e-02   2.95472406e-02\n",
      "   9.69365463e-02  -4.30898070e-02   4.89011891e-02  -1.69891957e-02\n",
      "   9.33220461e-02  -2.34049503e-02  -1.21839456e-02  -5.88511229e-02\n",
      "  -6.11453168e-02   3.50437500e-02   3.51903192e-03  -5.02382144e-02\n",
      "   4.73726317e-02   4.49578017e-02   3.21153253e-02  -6.37103319e-02\n",
      "  -4.36741859e-02  -4.56842706e-02  -1.04637947e-02  -6.80306479e-02\n",
      "  -9.80723202e-02  -7.50794783e-02   1.34116812e-02   1.02293879e-01\n",
      "   9.02611166e-02  -4.20175940e-02   1.44793205e-02  -1.32075939e-02\n",
      "   2.04321574e-02   4.20533046e-02  -3.03080436e-02  -1.49593679e-02\n",
      "  -5.37690036e-02  -2.23203842e-03   1.07054740e-01  -4.22071181e-02\n",
      "  -2.29758490e-02   5.31572290e-02  -4.87417504e-02   4.46330607e-02\n",
      "   1.51831368e-02   7.52047822e-02  -8.59636464e-04   3.56824733e-02\n",
      "   1.51982149e-02   5.52115915e-03  -1.02784999e-01  -7.89943524e-03\n",
      "  -6.63905442e-02   3.76733020e-02  -3.74899544e-02   9.34993476e-02\n",
      "   8.43231604e-02  -1.01797792e-04  -2.36838721e-02  -3.71378176e-02\n",
      "  -3.80870723e-03   1.50954640e-02   1.52891017e-02   5.69078065e-02\n",
      "   7.43219927e-02  -8.13398045e-03   9.62227657e-02   3.51638272e-02\n",
      "  -9.08316150e-02  -5.93255572e-02  -2.78554894e-02   2.58560237e-02\n",
      "  -4.62631974e-03  -5.40327094e-02   1.39594162e-02   2.93863267e-02\n",
      "   3.48257832e-02   5.53054921e-02   6.50175586e-02  -3.20852734e-02\n",
      "  -4.70970944e-02   6.12644553e-02  -7.12796003e-02  -8.53570178e-02\n",
      "   3.84914726e-02   1.11309119e-01  -4.13188152e-02  -1.92403961e-02\n",
      "  -2.54580146e-03  -2.68942211e-02  -6.88485727e-02   1.74679738e-02\n",
      "  -3.65621261e-02  -2.28685997e-02   5.11806123e-02  -1.35140866e-02\n",
      "  -2.94824075e-02  -6.14562966e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.29225\n",
      "   [1]: 2.87716\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 148.  148.  149.  149.  148.  147.  147.  149.  149.  149.  147.  149.\n",
      "  148.  149.  148.  149.  148.  149.  149.  149.  149.  149.  148.  148.\n",
      "  149.  148.  149.  149.  148.  148.  148.  147.  149.  148.  148.  149.\n",
      "  148.  149.  148.  149.  149.  149.  149.  149.  149.  149.  149.  148.\n",
      "  146.  148.]\n",
      "   [1]: [ 149.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.\n",
      "  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.\n",
      "  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.\n",
      "  120.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.  149.\n",
      "  149.  149.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00020311  0.05222888 -0.02230769  0.00698622  0.03294428 -0.06513409\n",
      " -0.059823    0.03693274 -0.07296065  0.07204476  0.01915365 -0.04842773\n",
      "  0.03068     0.06676389 -0.02334181  0.02145351 -0.01526262  0.06701174\n",
      "  0.05631324  0.02019939 -0.02415104 -0.00931206 -0.02886582  0.06094772\n",
      "  0.00834709  0.03132923  0.04845282  0.07795817 -0.06966213 -0.03022815\n",
      " -0.06139487  0.03179197 -0.04921762  0.01921148 -0.08487085 -0.0132989\n",
      " -0.06517085 -0.06986508  0.03618129 -0.00609241  0.00719777 -0.0640533\n",
      "  0.01099057  0.0043111   0.01072255 -0.01567837  0.0072938  -0.04423817\n",
      "  0.04215012 -0.0184961   0.04343038  0.01658763 -0.01048608  0.06365553\n",
      " -0.09881831  0.0647582   0.0750896  -0.00081665 -0.00659906 -0.00356169\n",
      " -0.01594805  0.01277717  0.10893352 -0.0343601   0.03750932 -0.05943739\n",
      " -0.04588109  0.03891062  0.05768801 -0.10319138 -0.01091477 -0.07506926\n",
      "  0.0971885   0.04445266 -0.03927246  0.06895022  0.00973797  0.02866943\n",
      "  0.04464586 -0.02007064  0.00184608 -0.08261026 -0.01007413  0.05865912\n",
      "  0.03359964  0.00465222 -0.05122663 -0.02330394  0.02965799  0.04024352\n",
      " -0.02295051 -0.02136386  0.0453232   0.00285564  0.02119644 -0.03856257\n",
      " -0.02777322  0.02463245 -0.06954413  0.00720077  0.0184457  -0.04275332\n",
      " -0.02651144  0.03258696 -0.01884007  0.06594726  0.00792293  0.03052401\n",
      " -0.0128785   0.06373709  0.00387882  0.02554504 -0.01502303  0.05827502\n",
      "  0.0263626   0.00321742 -0.08899444 -0.01859729 -0.00039344 -0.02292345\n",
      " -0.03718155  0.10603618  0.08056808 -0.06080896  0.08235352  0.05046453\n",
      "  0.00802553 -0.01971059  0.02931958  0.0137604  -0.07141096  0.01833918\n",
      " -0.05401786 -0.00571949 -0.00629541 -0.0471964  -0.05596607  0.02755291\n",
      " -0.0532969   0.0979512   0.05142383 -0.03833332 -0.01243334  0.01320652\n",
      " -0.02160024 -0.02538488  0.0949     -0.07369266  0.01368404 -0.01629523\n",
      "  0.01040588 -0.00269405 -0.02855449 -0.0350121  -0.08882571 -0.01351529\n",
      "  0.07912488 -0.06514456  0.03897225  0.04994822  0.07137746 -0.00598985\n",
      " -0.10046586 -0.07958473  0.01761587 -0.02230856 -0.07241735  0.05589826\n",
      "  0.00964105  0.02956413 -0.06028091 -0.04439997  0.03821643 -0.02060387\n",
      "  0.04480918  0.07776763  0.02417372 -0.05127378  0.02523532 -0.00355663\n",
      "  0.04511238  0.07098079 -0.01256533 -0.02712807 -0.029225   -0.06006785\n",
      " -0.04323566 -0.07245342 -0.06633195  0.04039998  0.02542121  0.07250397\n",
      " -0.02682837  0.0431446   0.01950516 -0.0062779  -0.02585619 -0.00295575\n",
      "  0.08248223 -0.06960037 -0.00668073  0.06690218  0.0485584   0.01931626\n",
      "  0.09117714 -0.05242753 -0.01149017 -0.04095719  0.05311646 -0.06848698\n",
      "  0.04767412 -0.01872744  0.02540002 -0.04649053  0.04809049  0.06335536\n",
      "  0.01617942  0.06848193 -0.10906351 -0.01698594  0.00598641 -0.07756026\n",
      "  0.04872293  0.06713141  0.02424456 -0.02649181  0.03467838 -0.06393199\n",
      " -0.05521441 -0.02544488 -0.01053964 -0.08208515  0.0044628   0.00285852\n",
      " -0.05123658 -0.07282246  0.02559121 -0.01323162  0.01729082 -0.08974345\n",
      " -0.00394179 -0.02887146 -0.02781902 -0.07669417  0.0031278   0.03179688\n",
      "  0.0430173   0.00373721 -0.02962032 -0.02981501  0.01815363  0.05095562\n",
      " -0.09850395 -0.0059824  -0.05660543 -0.00391405 -0.00224916 -0.06166786\n",
      " -0.06865381  0.03343552  0.05765174  0.04950687 -0.07214346  0.07484352\n",
      "  0.07346582  0.00944298  0.10876098  0.01597362  0.00070862 -0.04851699\n",
      "  0.05166993  0.00431657 -0.04444087 -0.00537631 -0.02005927  0.06714982\n",
      " -0.08029184  0.02321619  0.04721472  0.00865826  0.05223906  0.0101924\n",
      "  0.01641712 -0.03549454  0.05220716  0.08508924  0.05150782 -0.08266933\n",
      " -0.04198943 -0.00841443 -0.04903056  0.07502595  0.07643682  0.0270111\n",
      " -0.06409878 -0.00228785  0.02816725  0.01054545 -0.05333368  0.01957649\n",
      "  0.01978191 -0.10726416  0.03549568  0.09034714 -0.07462949  0.03651313\n",
      "  0.05019303  0.01542127  0.04908082  0.02789894 -0.04293545 -0.06208486\n",
      "  0.04220489 -0.04189475  0.04006833 -0.00361024 -0.02952925 -0.01005204\n",
      " -0.00191915 -0.01816395 -0.03489869  0.07328559 -0.01711495 -0.0348439\n",
      "  0.03018071  0.03286136 -0.04634066  0.02571208 -0.06380673  0.01752565\n",
      "  0.00515792  0.0051649 ]\n",
      "   [1]: [-0.04142214 -0.00894967  0.00754528 -0.03709463 -0.07020544  0.05384341\n",
      " -0.05778308  0.03851677 -0.04468117 -0.08454102 -0.00886242  0.0303006\n",
      "  0.09673395  0.06284385  0.02890296  0.11051062  0.06124074  0.0156147\n",
      "  0.0547069   0.00106777  0.01987582 -0.05179099 -0.10811947 -0.06889838\n",
      "  0.0300576   0.05989769 -0.01102951  0.08125105  0.03916587  0.04843837\n",
      " -0.00349438 -0.07727586  0.02870562 -0.01068708  0.00039472  0.02657504\n",
      "  0.03956055  0.08021466 -0.03799817  0.03935909  0.06063725 -0.07461794\n",
      "  0.00148038  0.04125151  0.01912645 -0.11209341 -0.0422005  -0.01775261\n",
      " -0.08837577  0.00674465 -0.06984551  0.04543981 -0.03622808 -0.10321671\n",
      " -0.05230936  0.00562024 -0.03941474  0.00566104  0.03884155 -0.01676428\n",
      "  0.0517896  -0.00480457  0.09064092 -0.08109657 -0.07990753 -0.00584434\n",
      "  0.05172307  0.01993182 -0.02743509  0.01478423  0.0389258   0.05079466\n",
      " -0.03113735  0.03996758 -0.05609644  0.01953528 -0.01710977  0.03367187\n",
      " -0.04524247 -0.06900031 -0.03783244  0.0352139   0.04774421 -0.07273766\n",
      "  0.0204302   0.02470601  0.00877986  0.05510335  0.05722548 -0.0759128\n",
      " -0.05501025  0.08793761 -0.03221118  0.05001049  0.03491051  0.03189555\n",
      " -0.02141133 -0.03252452 -0.03108937  0.01309106  0.02691357  0.06521229\n",
      "  0.08147223  0.02558438 -0.00561621  0.00492527 -0.04175039 -0.01255037\n",
      "  0.04477789  0.01471468  0.05703026  0.04682745  0.04117239 -0.0110996\n",
      "  0.0145505   0.09697583  0.04436879 -0.0114587   0.06276667 -0.00066577\n",
      " -0.02126252  0.11077058  0.06023841 -0.08641508 -0.01374205  0.02208409\n",
      "  0.00681422  0.01869809 -0.08138759 -0.00144352  0.0093336  -0.01673211\n",
      " -0.0219455   0.01241271  0.05266541 -0.10083643 -0.04551527 -0.06687523\n",
      "  0.01908867  0.0730057   0.08668154 -0.07810151 -0.00830514 -0.0263227\n",
      " -0.01250318  0.01331037  0.01594643 -0.07223688 -0.05520393  0.01357961\n",
      "  0.02925887  0.00910339 -0.08185739  0.00938882  0.03916773 -0.07208536\n",
      "  0.04856253 -0.07324842  0.03207803 -0.07722946  0.00453665 -0.07076112\n",
      "  0.02003884 -0.01259504  0.10667434 -0.05003526  0.00730739  0.02117027\n",
      " -0.03894459  0.08145856 -0.03501589  0.03450623 -0.00120765 -0.073814\n",
      " -0.04461376  0.05499732  0.00329021  0.09099803  0.08467593  0.02079385\n",
      " -0.0295042  -0.01981959 -0.03628589  0.09114379  0.05810973  0.09811176\n",
      "  0.0221623  -0.05612568 -0.04234208 -0.043784    0.07122731 -0.02721344\n",
      " -0.00334301  0.03120618  0.00731346  0.0595372  -0.10970353  0.02655239\n",
      "  0.1022928  -0.11404523  0.05715004 -0.09891998 -0.01516812  0.04971275\n",
      "  0.06108142 -0.05790555  0.04846002  0.09663283  0.00085085  0.02164618\n",
      " -0.0239554   0.02928631  0.09726317 -0.0438097   0.04906685 -0.01674016\n",
      "  0.0931751  -0.02367277 -0.01272901 -0.05858539 -0.06123271  0.03483012\n",
      "  0.00374976 -0.04994232  0.04704371  0.04506683  0.03195626 -0.06324608\n",
      " -0.04386782 -0.04535381 -0.0104493  -0.06772162 -0.09757757 -0.07463099\n",
      "  0.01327279  0.10200347  0.090075   -0.04228451  0.01458767 -0.01387961\n",
      "  0.02047042  0.04138213 -0.03011671 -0.01448028 -0.05378812 -0.00248633\n",
      "  0.10699474 -0.04227171 -0.02295215  0.05267283 -0.04944115  0.04465608\n",
      "  0.01551612  0.07519744 -0.00077078  0.03569464  0.01478055  0.00595029\n",
      " -0.10254126 -0.0077606  -0.06645425  0.03746301 -0.03750825  0.09379771\n",
      "  0.08443315 -0.00017458 -0.02355886 -0.03722652 -0.00316184  0.0152978\n",
      "  0.01526039  0.05705775  0.07423128 -0.00828352  0.09622663  0.03575223\n",
      " -0.09109954 -0.05938655 -0.02802916  0.02607241 -0.00458413 -0.05409895\n",
      "  0.01405458  0.02914393  0.03528561  0.05475517  0.0648299  -0.03214793\n",
      " -0.04753114  0.06143082 -0.07126337 -0.08492569  0.03813152  0.11137293\n",
      " -0.04126635 -0.0192348  -0.00247717 -0.02693038 -0.06839147  0.01772536\n",
      " -0.03621082 -0.022984    0.05066541 -0.0135896  -0.02917044 -0.06110534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.89476\n",
      "   [1]: 4.77216\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.]\n",
      "   [1]: [ 270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.  270.\n",
      "  270.  270.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00334816  0.05176891 -0.02256405  0.0070944   0.0296662  -0.06217953\n",
      " -0.05713049  0.03840865 -0.07466369  0.07126249  0.02200782 -0.05099006\n",
      "  0.02505063  0.06005539 -0.02150422  0.01822987 -0.01186006  0.06827645\n",
      "  0.05288206  0.02436589 -0.02466224 -0.00942336 -0.02928769  0.05303765\n",
      "  0.00891694  0.02899518  0.05091581  0.08166162 -0.06770338 -0.03327139\n",
      " -0.06135499  0.02825335 -0.05258613  0.01726567 -0.07590804 -0.013445\n",
      " -0.06525242 -0.0719021   0.0329667  -0.00412094  0.00799842 -0.05756859\n",
      "  0.01413264  0.00323559  0.00993856 -0.01858954  0.0124399  -0.04475623\n",
      "  0.04178871 -0.01596213  0.03684359  0.01434563 -0.01128322  0.06242529\n",
      " -0.09995978  0.06295981  0.07493829 -0.00234985 -0.00579471 -0.00177261\n",
      " -0.01298961  0.00995798  0.10656224 -0.03236498  0.03664847 -0.0502457\n",
      " -0.04803419  0.04108999  0.05541886 -0.09824009 -0.01018721 -0.06831208\n",
      "  0.09233656  0.04269479 -0.03580203  0.06393692  0.00667615  0.03265489\n",
      "  0.04632368 -0.02529568 -0.00091868 -0.080819   -0.01684702  0.05687484\n",
      "  0.03890368  0.00225717 -0.05568615 -0.02114702  0.02902165  0.03947873\n",
      " -0.02667033 -0.01861775  0.04588467  0.0001984   0.01433953 -0.03867492\n",
      " -0.02863082  0.0213069  -0.07227036  0.00457634  0.01679198 -0.04418182\n",
      " -0.03244061  0.03446079 -0.02517613  0.06119484  0.00996782  0.03182343\n",
      " -0.00737474  0.06129622  0.00485848  0.02427278 -0.01405564  0.05080931\n",
      "  0.01945129  0.00535113 -0.08391327 -0.01607912 -0.00429728 -0.02254191\n",
      " -0.03396467  0.10655115  0.08102173 -0.06032595  0.084685    0.04937292\n",
      "  0.0070177  -0.02564034  0.02438541  0.01211433 -0.07079877  0.01625426\n",
      " -0.0588983   0.00137692 -0.00588215 -0.05227756 -0.06132852  0.0302981\n",
      " -0.04953888  0.09764037  0.06051654 -0.0390841  -0.01357608  0.017366\n",
      " -0.02508553 -0.02877622  0.09415414 -0.07069638  0.01423301 -0.01709555\n",
      "  0.00712966 -0.00286275 -0.02645523 -0.03720789 -0.08634213 -0.01370102\n",
      "  0.08237708 -0.06597956  0.03918733  0.0461572   0.07614021 -0.01203237\n",
      " -0.10112993 -0.07349118  0.01956501 -0.02321996 -0.06773549  0.05644015\n",
      "  0.00950874  0.02655154 -0.05842354 -0.03853004  0.03840488 -0.01668907\n",
      "  0.04628952  0.07643506  0.02595845 -0.05572471  0.02427982 -0.00776837\n",
      "  0.04000527  0.0697018  -0.0135548  -0.03133886 -0.0351567  -0.05598865\n",
      " -0.04412414 -0.07159294 -0.06025299  0.047357    0.02527261  0.06392509\n",
      " -0.02367515  0.03977316  0.01892468 -0.00970883 -0.02045933 -0.00265919\n",
      "  0.08094881 -0.07495192 -0.01082132  0.06904545  0.04463228  0.0201726\n",
      "  0.09569517 -0.0462551  -0.00566601 -0.04087937  0.04960746 -0.06899091\n",
      "  0.04764964 -0.01195636  0.02746757 -0.04534116  0.04613082  0.0615857\n",
      "  0.01091495  0.07008792 -0.11060993 -0.0164056   0.00608675 -0.07829649\n",
      "  0.0458565   0.06351025  0.02369747 -0.02960388  0.0361406  -0.05885225\n",
      " -0.05190759 -0.02254043 -0.00584809 -0.08390572  0.00472938 -0.0034563\n",
      " -0.0545819  -0.07482804  0.02963352 -0.00869672  0.02165856 -0.08718925\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 7.85305\n",
      "   [1]: 9.38702\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.]\n",
      "   [1]: [ 570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.  570.\n",
      "  570.  570.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00326936  0.05184997 -0.02253833  0.00702356  0.02969379 -0.06224015\n",
      " -0.05722443  0.03845102 -0.07468277  0.07124103  0.02197165 -0.05087858\n",
      "  0.02508967  0.06016242 -0.02156887  0.01832369 -0.01191298  0.06820111\n",
      "  0.0529164   0.0241214  -0.0245979  -0.00943617 -0.02919896  0.05323394\n",
      "  0.00887507  0.02908795  0.05082725  0.08165643 -0.06766206 -0.03323187\n",
      " -0.06134605  0.02838536 -0.05259048  0.01728874 -0.07612797 -0.01343745\n",
      " -0.06527173 -0.07185711  0.03299407 -0.00424001  0.00801587 -0.05771389\n",
      "  0.01408363  0.00330519  0.01002893 -0.01847488  0.01230542 -0.04471163\n",
      "  0.04182062 -0.01599536  0.03703288  0.01445273 -0.01134607  0.06250916\n",
      " -0.09987932  0.06305291  0.0749441  -0.00224431 -0.00589326 -0.00184468\n",
      " -0.01317762  0.00994499  0.10645672 -0.03246825  0.03665773 -0.05040126\n",
      " -0.04797513  0.04102463  0.05538036 -0.09840393 -0.01026423 -0.06853167\n",
      "  0.09246682  0.04269885 -0.03599127  0.06402688  0.00680817  0.0325266\n",
      "  0.04621096 -0.02534138 -0.00099223 -0.08092259 -0.01667508  0.05696502\n",
      "  0.03867116  0.00228616 -0.0555898  -0.02109212  0.0289617   0.03949341\n",
      " -0.02667226 -0.01872266  0.04590295  0.00027038  0.01442544 -0.03860695\n",
      " -0.02868635  0.02140717 -0.07218866  0.00462211  0.0168982  -0.04404799\n",
      " -0.03230345  0.03440194 -0.02507666  0.06131523  0.00989113  0.03177999\n",
      " -0.0075073   0.06127977  0.00469538  0.02433169 -0.01397893  0.0510627\n",
      "  0.01952817  0.00537786 -0.08384448 -0.01619316 -0.0042184  -0.0225124\n",
      " -0.0340084   0.10646725  0.08099283 -0.06039657  0.08457881  0.04931317\n",
      "  0.00699316 -0.02546799  0.02457053  0.01200271 -0.0709146   0.01636628\n",
      " -0.05881152  0.00088991 -0.00573122 -0.05193362 -0.0613069   0.03023436\n",
      " -0.04962303  0.09744325  0.06015037 -0.03916434 -0.01361181  0.01759852\n",
      " -0.02520905 -0.02866239  0.09428569 -0.07099169  0.01414364 -0.01705544\n",
      "  0.00738448 -0.00285365 -0.02663147 -0.03703273 -0.0862954  -0.0136331\n",
      "  0.08233406 -0.06596166  0.03919923  0.04611418  0.07589383 -0.01187677\n",
      " -0.10136196 -0.07397311  0.01943945 -0.0231732  -0.06764843  0.05654342\n",
      "  0.00965374  0.02670679 -0.05844788 -0.03882034  0.03861458 -0.0167726\n",
      "  0.0462613   0.07669169  0.02600189 -0.05575692  0.02442573 -0.00769066\n",
      "  0.04012511  0.06983808 -0.01337212 -0.03136509 -0.03482627 -0.05624729\n",
      " -0.04379546 -0.07161883 -0.06054711  0.04706419  0.02523382  0.06426942\n",
      " -0.02387757  0.03979295  0.01897799 -0.00960772 -0.02049119 -0.00241637\n",
      "  0.08119297 -0.07496069 -0.01073786  0.06927772  0.04492815  0.02015118\n",
      "  0.09550895 -0.04654499 -0.00590646 -0.04094418  0.04959187 -0.06885493\n",
      "  0.04776111 -0.01201657  0.02719272 -0.04563531  0.04620114  0.06157188\n",
      "  0.01108076  0.07034808 -0.11054844 -0.01634911  0.00607361 -0.07823233\n",
      "  0.04579946  0.06390271  0.02386314 -0.029445    0.03602057 -0.05915796\n",
      " -0.05210725 -0.02236573 -0.00598014 -0.08384464  0.00430829 -0.0032863\n",
      " -0.05476592 -0.07483759  0.02953289 -0.00856607  0.02169337 -0.08711539\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 17.9192\n",
      "   [1]: 18.756\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.]\n",
      "   [1]: [ 1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00326428  0.05187424 -0.02246363  0.00699491  0.02967948 -0.06223413\n",
      " -0.05731569  0.03842891 -0.07463635  0.0712715   0.02195164 -0.0508725\n",
      "  0.02518958  0.06023607 -0.02152759  0.01834218 -0.0119291   0.0681481\n",
      "  0.05295183  0.02408382 -0.02459444 -0.00942321 -0.02917064  0.05330869\n",
      "  0.00885996  0.02910339  0.05080301  0.08162238 -0.06767736 -0.03320647\n",
      " -0.0613119   0.02838569 -0.05254121  0.01730886 -0.07621253 -0.0134331\n",
      " -0.06524085 -0.07184493  0.03306353 -0.00423121  0.00804246 -0.05778712\n",
      "  0.01409395  0.00330993  0.01006447 -0.01843024  0.0122369  -0.04472054\n",
      "  0.04181284 -0.0159944   0.03703944  0.01449639 -0.01131875  0.06254176\n",
      " -0.09989432  0.06314855  0.07495814 -0.00221797 -0.00587117 -0.001851\n",
      " -0.01320414  0.00991487  0.10643714 -0.03250569  0.03666213 -0.0504465\n",
      " -0.04796251  0.04100545  0.05537703 -0.09844918 -0.01025585 -0.06854737\n",
      "  0.09249219  0.04270192 -0.03603601  0.06410796  0.00679974  0.03242327\n",
      "  0.04620254 -0.02534719 -0.00098316 -0.08091095 -0.01664118  0.05695298\n",
      "  0.03859112  0.00233642 -0.05555959 -0.02107391  0.0289536   0.03948179\n",
      " -0.02668016 -0.01874495  0.04593146  0.00031703  0.01446945 -0.03857453\n",
      " -0.02873048  0.02144413 -0.07213699  0.00464787  0.01687252 -0.04403337\n",
      " -0.03227072  0.03439452 -0.02501184  0.06128196  0.00988213  0.03177198\n",
      " -0.00752152  0.06135541  0.0047105   0.02432449 -0.01401963  0.05111139\n",
      "  0.01960647  0.00530518 -0.08392136 -0.01624143 -0.00422527 -0.02249713\n",
      " -0.0340637   0.10645634  0.08097339 -0.06044665  0.08451141  0.04932484\n",
      "  0.00696483 -0.02544402  0.02462519  0.01199842 -0.07094748  0.01643005\n",
      " -0.0587861   0.00070508 -0.00567335 -0.05179522 -0.06128112  0.03017243\n",
      " -0.04964535  0.09735855  0.06001165 -0.03920617 -0.01361121  0.01764954\n",
      " -0.02523768 -0.02864133  0.09431056 -0.07109513  0.01408746 -0.01704434\n",
      "  0.00747899 -0.00287421 -0.02667939 -0.03699373 -0.08627465 -0.0135987\n",
      "  0.08226418 -0.06594347  0.03921106  0.04612122  0.07581186 -0.01182912\n",
      " -0.10143888 -0.07412484  0.01939913 -0.02314141 -0.06764031  0.05654619\n",
      "  0.00969543  0.02679321 -0.05846955 -0.03892479  0.03867398 -0.01679997\n",
      "  0.04624915  0.07677254  0.02603924 -0.05574104  0.02446569 -0.00765495\n",
      "  0.04019294  0.06990036 -0.01330439 -0.03136082 -0.03473531 -0.05633623\n",
      " -0.0436926  -0.07163323 -0.0606572   0.04695858  0.02522804  0.06438905\n",
      " -0.02395982  0.0398163   0.01897882 -0.00959871 -0.02048837 -0.00233716\n",
      "  0.0812843  -0.07492806 -0.01071149  0.06933674  0.04502527  0.02012378\n",
      "  0.09545873 -0.0466646  -0.00600017 -0.0410024   0.0496198  -0.06882981\n",
      "  0.04782555 -0.01206982  0.02710623 -0.04575543  0.04623508  0.06156246\n",
      "  0.01117986  0.07044452 -0.11051628 -0.01632239  0.00605578 -0.07820813\n",
      "  0.04578607  0.0640441   0.02391271 -0.02936514  0.0359908  -0.05926729\n",
      " -0.05220157 -0.02231622 -0.00603325 -0.08380544  0.00413386 -0.00320441\n",
      " -0.05481447 -0.07483725  0.02950352 -0.00854951  0.0217049  -0.08713141\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 25.6006\n",
      "   [1]: 25.9053\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.]\n",
      "   [1]: [ 2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.0032729   0.0518671  -0.02237683  0.00698265  0.02970247 -0.06223615\n",
      " -0.05738828  0.03839818 -0.07462119  0.07131373  0.02193026 -0.05087968\n",
      "  0.02522566  0.06026259 -0.02152018  0.01831878 -0.01196635  0.06814425\n",
      "  0.05298292  0.02410492 -0.02462019 -0.00944355 -0.0291128   0.0533158\n",
      "  0.00884589  0.02912057  0.05081132  0.08158563 -0.06768882 -0.03319157\n",
      " -0.06126514  0.0283724  -0.05251508  0.01733167 -0.07623482 -0.01338966\n",
      " -0.06523626 -0.07178317  0.03308653 -0.00422551  0.00803366 -0.05779954\n",
      "  0.01407656  0.00332503  0.01007126 -0.01841551  0.01222471 -0.04474469\n",
      "  0.04177101 -0.01599208  0.03702182  0.01447212 -0.01129488  0.06256702\n",
      " -0.09988501  0.06320107  0.0749457  -0.00220809 -0.00585102 -0.00189267\n",
      " -0.01320209  0.00991304  0.10644956 -0.03249     0.03666263 -0.05043356\n",
      " -0.04797944  0.04102522  0.05535803 -0.09848384 -0.01025893 -0.06853047\n",
      "  0.09249466  0.04267306 -0.0360286   0.06416148  0.00681664  0.03234185\n",
      "  0.04622765 -0.02537605 -0.00095902 -0.08087323 -0.01663262  0.05695498\n",
      "  0.03857711  0.00233401 -0.05556386 -0.02107378  0.02898136  0.03945985\n",
      " -0.02667642 -0.01876618  0.04595416  0.00038443  0.01450082 -0.03857437\n",
      " -0.02874662  0.02144993 -0.07206183  0.00463924  0.01682388 -0.0440452\n",
      " -0.03225186  0.03442726 -0.02495662  0.06123163  0.00987742  0.03176821\n",
      " -0.00750103  0.06140573  0.00475716  0.02428471 -0.01404412  0.05114886\n",
      "  0.01962229  0.00524283 -0.08393488 -0.01627225 -0.00420869 -0.02248622\n",
      " -0.03408792  0.10646105  0.08097914 -0.06046167  0.08446793  0.04932072\n",
      "  0.00693841 -0.02542175  0.02466033  0.01202793 -0.07097414  0.01645894\n",
      " -0.05874713  0.00061975 -0.00566447 -0.05171907 -0.0612418   0.03014259\n",
      " -0.04968302  0.09732685  0.05991495 -0.03923329 -0.01359071  0.01765543\n",
      " -0.02524255 -0.02864641  0.09430959 -0.07115725  0.01408253 -0.01705525\n",
      "  0.0075292  -0.00287584 -0.02670304 -0.03697623 -0.08627371 -0.01358674\n",
      "  0.08219728 -0.06592494  0.03921195  0.04614402  0.0757603  -0.01180612\n",
      " -0.10148104 -0.07419124  0.01938382 -0.02312466 -0.06765539  0.05652922\n",
      "  0.00972376  0.02683217 -0.05847846 -0.03897773  0.03868086 -0.01681697\n",
      "  0.04622759  0.07680031  0.02606696 -0.05572036  0.02448994 -0.00762755\n",
      "  0.0402387   0.06993166 -0.01327065 -0.03134406 -0.03468946 -0.05637518\n",
      " -0.04366785 -0.07163912 -0.06072114  0.04688727  0.02521034  0.06447253\n",
      " -0.02402029  0.03982937  0.01896612 -0.00959538 -0.02049659 -0.00231343\n",
      "  0.08133011 -0.07488675 -0.01068354  0.06935073  0.04507689  0.02010313\n",
      "  0.09541204 -0.04673842 -0.00605432 -0.04101546  0.04963562 -0.06880963\n",
      "  0.04787149 -0.01210434  0.02706473 -0.0457896   0.04627432  0.06157315\n",
      "  0.01124455  0.07048912 -0.11049053 -0.01630287  0.00604682 -0.0781844\n",
      "  0.04579418  0.06410171  0.02393575 -0.02930883  0.03596718 -0.05933283\n",
      " -0.05226276 -0.02228842 -0.00608307 -0.0837837   0.00406371 -0.00314264\n",
      " -0.05482737 -0.07480735  0.02947088 -0.00854898  0.02171462 -0.08715865\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n",
      "Average loss at step 100: 18.266855 learning rate: 8.781140\n",
      "Percentage_of correct: 4.20%\n",
      "0:\n",
      "self.sigm_arg:  [-0.20095848 -0.16394676]\n",
      "1:\n",
      "self.sigm_arg:  [-0.18783638 -0.16394676]\n",
      "2:\n",
      "self.sigm_arg:  [-0.17668952 -0.16394676]\n",
      "3:\n",
      "self.sigm_arg:  [-0.16411459 -0.16394676]\n",
      "4:\n",
      "self.sigm_arg:  [-0.11796837 -0.16394676]\n",
      "5:\n",
      "self.sigm_arg:  [-0.16158879 -0.16394676]\n",
      "6:\n",
      "self.sigm_arg:  [-0.20052692 -0.16394676]\n",
      "7:\n",
      "self.sigm_arg:  [-0.11090509 -0.16394676]\n",
      "8:\n",
      "self.sigm_arg:  [-0.17459917 -0.16394676]\n",
      "9:\n",
      "self.sigm_arg:  [-0.17323868 -0.16394676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 6.22%\n",
      "\n",
      "step: 200\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 29.4166\n",
      "   [1]: 29.457\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.]\n",
      "   [1]: [ 5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.\n",
      "  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.  5970.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.0032151   0.05184734 -0.0222956   0.0069397   0.02972744 -0.06225894\n",
      " -0.05744147  0.03836812 -0.07457904  0.07134119  0.02191022 -0.0509255\n",
      "  0.02531568  0.06028209 -0.0215216   0.0183074  -0.01197989  0.06815565\n",
      "  0.05301885  0.02412158 -0.02462265 -0.0094234  -0.02907668  0.0534015\n",
      "  0.00882747  0.0291208   0.05084818  0.08155247 -0.06771336 -0.03318095\n",
      " -0.06122566  0.02836015 -0.05248343  0.01734857 -0.07629163 -0.01339255\n",
      " -0.06524228 -0.07170202  0.03311979 -0.00419828  0.00799548 -0.0578408\n",
      "  0.01407166  0.00332424  0.01007303 -0.01834495  0.01222999 -0.044754\n",
      "  0.04175914 -0.01596067  0.03700905  0.01447347 -0.01127352  0.06259448\n",
      " -0.09984104  0.06328647  0.07491605 -0.00221939 -0.00583958 -0.00185251\n",
      " -0.01317631  0.00994341  0.10637394 -0.03249486  0.03667575 -0.05046816\n",
      " -0.04800585  0.04100993  0.05536059 -0.09853117 -0.01027874 -0.0686269\n",
      "  0.09250079  0.04263281 -0.0360938   0.06419902  0.00681895  0.03224338\n",
      "  0.04622357 -0.02534495 -0.00094889 -0.0808813  -0.01657044  0.05694894\n",
      "  0.03858856  0.00237333 -0.05554749 -0.02107123  0.02899787  0.03942071\n",
      " -0.02666515 -0.01878279  0.04595117  0.00044978  0.01452846 -0.03855903\n",
      " -0.02875176  0.02153916 -0.0719901   0.00467925  0.01680379 -0.04400712\n",
      " -0.03217661  0.03444175 -0.02489369  0.06116772  0.00985445  0.03180899\n",
      " -0.00752832  0.06144002  0.00475768  0.02433659 -0.01408343  0.05121412\n",
      "  0.01964594  0.00522322 -0.0839476  -0.01633082 -0.00424122 -0.022429\n",
      " -0.03414266  0.10646167  0.08101202 -0.06048743  0.08445107  0.04934156\n",
      "  0.0069283  -0.02540191  0.02470249  0.01202877 -0.07097686  0.01648789\n",
      " -0.05867759  0.00049528 -0.00563579 -0.05162836 -0.06119409  0.03012367\n",
      " -0.04973552  0.09728739  0.05978885 -0.03928406 -0.01359107  0.01765693\n",
      " -0.02524977 -0.0286398   0.09431718 -0.07122967  0.01404592 -0.01707145\n",
      "  0.00759828 -0.00288244 -0.02675416 -0.03694839 -0.08629073 -0.01359448\n",
      "  0.08215121 -0.0658963   0.03920016  0.04616334  0.07571498 -0.01176441\n",
      " -0.10155381 -0.07429186  0.01936549 -0.02311301 -0.06767549  0.05649516\n",
      "  0.00975584  0.02688876 -0.05848481 -0.03905337  0.03869925 -0.01683572\n",
      "  0.04621803  0.07683855  0.02606941 -0.05568004  0.02450358 -0.0076043\n",
      "  0.04028073  0.06996789 -0.01321609 -0.03133344 -0.03461202 -0.05642118\n",
      " -0.04362871 -0.0716405  -0.06081598  0.04678648  0.02518403  0.06458366\n",
      " -0.02408062  0.0398417   0.01895678 -0.00957142 -0.02052712 -0.00227338\n",
      "  0.08137932 -0.07484435 -0.01064144  0.0693223   0.04516819  0.0200963\n",
      "  0.09536523 -0.04683168 -0.00612212 -0.04104958  0.04965618 -0.06879344\n",
      "  0.04791707 -0.01215375  0.02699156 -0.04583374  0.04633514  0.06159705\n",
      "  0.01132425  0.07054101 -0.11045609 -0.01629258  0.00603072 -0.07817195\n",
      "  0.0458111   0.06418352  0.02396332 -0.02924797  0.03592296 -0.05939114\n",
      " -0.05231828 -0.02225746 -0.00613932 -0.08376631  0.00398688 -0.00308092\n",
      " -0.0548527  -0.07478013  0.02940773 -0.00854198  0.02170499 -0.08717999\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n",
      "Average loss at step 200: 22.106245 learning rate: 8.781140\n",
      "Percentage_of correct: 10.89%\n",
      "0:\n",
      "self.sigm_arg:  [-0.19920237 -0.16394676]\n",
      "1:\n",
      "self.sigm_arg:  [-0.18927553 -0.16394676]\n",
      "2:\n",
      "self.sigm_arg:  [-0.1793666  -0.16394676]\n",
      "3:\n",
      "self.sigm_arg:  [-0.16797006 -0.16394676]\n",
      "4:\n",
      "self.sigm_arg:  [-0.12077486 -0.16394676]\n",
      "5:\n",
      "self.sigm_arg:  [-0.16579662 -0.16394676]\n",
      "6:\n",
      "self.sigm_arg:  [-0.20472261 -0.16394676]\n",
      "7:\n",
      "self.sigm_arg:  [-0.11410121 -0.16394676]\n",
      "8:\n",
      "self.sigm_arg:  [-0.17852923 -0.16394676]\n",
      "9:\n",
      "self.sigm_arg:  [-0.17707483 -0.16394676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 13.10%\n",
      "\n",
      "step: 300\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 29.9227\n",
      "   [1]: 29.928\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.]\n",
      "   [1]: [ 8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.\n",
      "  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.  8970.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00315634  0.05184581 -0.02228106  0.00695594  0.02973108 -0.06227901\n",
      " -0.05744048  0.03833688 -0.07457524  0.07136732  0.02189583 -0.05093041\n",
      "  0.02533987  0.06030832 -0.02149242  0.01830162 -0.01194801  0.06818131\n",
      "  0.05301557  0.02409632 -0.02464971 -0.0094356  -0.0290798   0.05339317\n",
      "  0.0088461   0.02911313  0.05082332  0.08152054 -0.06770936 -0.03313746\n",
      " -0.06118459  0.02840761 -0.05248267  0.01737751 -0.07632879 -0.01340515\n",
      " -0.06526217 -0.07169633  0.03311646 -0.00422285  0.00799033 -0.05781301\n",
      "  0.01408442  0.00330161  0.010076   -0.01834518  0.01225845 -0.04473379\n",
      "  0.0417277  -0.01595719  0.03701319  0.01450077 -0.01125445  0.0626175\n",
      " -0.0998456   0.0632939   0.07488693 -0.00218847 -0.00587129 -0.00187375\n",
      " -0.01316319  0.00996661  0.10637653 -0.03248457  0.03670194 -0.05050938\n",
      " -0.04800227  0.04100564  0.05534952 -0.0985186  -0.01024843 -0.06866151\n",
      "  0.09250683  0.04262526 -0.03615306  0.06421996  0.00684788  0.03220556\n",
      "  0.04620128 -0.02534824 -0.00098378 -0.08095596 -0.0165541   0.05695079\n",
      "  0.03857316  0.0023703  -0.05552213 -0.02107576  0.02902751  0.03942131\n",
      " -0.02667199 -0.01877831  0.04591369  0.00045837  0.01457973 -0.03857179\n",
      " -0.02873897  0.02156516 -0.0720155   0.00464001  0.01682283 -0.04400286\n",
      " -0.0321919   0.03448569 -0.02488759  0.06119474  0.00988027  0.03180506\n",
      " -0.00755504  0.06147751  0.00475309  0.02430547 -0.01405648  0.05123127\n",
      "  0.01964978  0.00522638 -0.08391771 -0.01633878 -0.00423333 -0.02246271\n",
      " -0.03417765  0.10646606  0.08104164 -0.06049043  0.08443862  0.04935848\n",
      "  0.00692504 -0.02536559  0.02471295  0.01203521 -0.07099032  0.01650144\n",
      " -0.0586633   0.00046048 -0.005625   -0.0515692  -0.06117221  0.03010214\n",
      " -0.04975235  0.09728161  0.05973715 -0.0392869  -0.01359215  0.01768066\n",
      " -0.02524352 -0.02864598  0.0943262  -0.07125363  0.01403863 -0.01707499\n",
      "  0.00760864 -0.0028792  -0.02677378 -0.03693824 -0.08629652 -0.01360249\n",
      "  0.08211474 -0.06589594  0.0392005   0.04617696  0.07568555 -0.011749\n",
      " -0.10157505 -0.07432679  0.01936929 -0.02310842 -0.06768355  0.05648115\n",
      "  0.0097872   0.02691811 -0.05848425 -0.03907723  0.0387098  -0.01684652\n",
      "  0.04621417  0.07684992  0.02606017 -0.05566475  0.02450145 -0.00759404\n",
      "  0.0402891   0.06999958 -0.01319    -0.03132692 -0.03458185 -0.05644566\n",
      " -0.04361703 -0.07165034 -0.06085275  0.04673952  0.02516738  0.06463137\n",
      " -0.0240997   0.03985268  0.01896573 -0.00955051 -0.02054645 -0.00226478\n",
      "  0.0814125  -0.07480893 -0.0106146   0.06931154  0.04521194  0.02007871\n",
      "  0.09534059 -0.04686586 -0.0061483  -0.0410457   0.04968759 -0.06877182\n",
      "  0.04792246 -0.01218188  0.02695418 -0.04585444  0.04636454  0.06161634\n",
      "  0.01135875  0.07055596 -0.11045437 -0.01628657  0.00601986 -0.07816861\n",
      "  0.04582343  0.06421176  0.02397898 -0.02922803  0.03590395 -0.05941803\n",
      " -0.05232796 -0.02224578 -0.00616125 -0.08375371  0.00396162 -0.003036\n",
      " -0.05485422 -0.07476743  0.02936879 -0.00854853  0.02169935 -0.08718527\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n",
      "Average loss at step 300: 25.084714 learning rate: 8.781140\n",
      "Percentage_of correct: 5.62%\n",
      "0:\n",
      "self.sigm_arg:  [-0.19918415 -0.16394676]\n",
      "1:\n",
      "self.sigm_arg:  [-0.18963513 -0.16394676]\n",
      "2:\n",
      "self.sigm_arg:  [-0.18032503 -0.16394676]\n",
      "3:\n",
      "self.sigm_arg:  [-0.16939981 -0.16394676]\n",
      "4:\n",
      "self.sigm_arg:  [-0.12154634 -0.16394676]\n",
      "5:\n",
      "self.sigm_arg:  [-0.16722906 -0.16394676]\n",
      "6:\n",
      "self.sigm_arg:  [-0.20637178 -0.16394676]\n",
      "7:\n",
      "self.sigm_arg:  [-0.11526327 -0.16394676]\n",
      "8:\n",
      "self.sigm_arg:  [-0.18025623 -0.16394676]\n",
      "9:\n",
      "self.sigm_arg:  [-0.17883101 -0.16394676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 6.16%\n",
      "\n",
      "step: 400\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 29.9898\n",
      "   [1]: 29.9905\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.]\n",
      "   [1]: [ 11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.  11970.\n",
      "  11970.  11970.  11970.  11970.  11970.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00312353  0.05182768 -0.02240962  0.00701028  0.02966327 -0.06231412\n",
      " -0.05738156  0.03831941 -0.07452068  0.07132182  0.02197063 -0.0508167\n",
      "  0.02529209  0.06026828 -0.02147833  0.01836161 -0.01194772  0.06813008\n",
      "  0.05300112  0.0240331  -0.02461282 -0.00948829 -0.02908278  0.05338009\n",
      "  0.00885897  0.02916299  0.0507442   0.08157263 -0.06761482 -0.03308985\n",
      " -0.06126766  0.02846526 -0.05259044  0.01746754 -0.07637773 -0.01332394\n",
      " -0.06528318 -0.07180388  0.03305403 -0.0042076   0.0080714  -0.05778145\n",
      "  0.0140668   0.00332491  0.01011221 -0.0183163   0.01230404 -0.04470764\n",
      "  0.04180019 -0.01597983  0.03705732  0.01452745 -0.01125264  0.06256578\n",
      " -0.09990875  0.06320029  0.07493398 -0.00212605 -0.00591011 -0.00200739\n",
      " -0.01316783  0.00987107  0.10640568 -0.03249698  0.03669458 -0.05051594\n",
      " -0.04800117  0.04100433  0.05529723 -0.09841684 -0.01025702 -0.0686479\n",
      "  0.09254733  0.04266271 -0.03621516  0.06416165  0.00688606  0.03231134\n",
      "  0.04615146 -0.02535543 -0.00099941 -0.0810718  -0.01654769  0.05703638\n",
      "  0.03839405  0.00240007 -0.05552033 -0.02108702  0.02898951  0.03948821\n",
      " -0.02664781 -0.01869848  0.04595865  0.00041312  0.01455452 -0.0385737\n",
      " -0.02870221  0.02154399 -0.07201422  0.00458166  0.01684263 -0.04395471\n",
      " -0.03222965  0.03445929 -0.02495516  0.06122795  0.00991941  0.03178535\n",
      " -0.00746415  0.06143625  0.00467594  0.02431159 -0.0139495   0.0512485\n",
      "  0.01967196  0.00524887 -0.08384442 -0.01633576 -0.00423248 -0.02251843\n",
      " -0.0341053   0.1065551   0.08096391 -0.06049659  0.08441173  0.04927529\n",
      "  0.00696842 -0.02532381  0.02473964  0.01203962 -0.07099733  0.01653678\n",
      " -0.05862708  0.00042163 -0.00561697 -0.05149919 -0.06116758  0.0300733\n",
      " -0.04977411  0.09726933  0.05966626 -0.03929642 -0.01360306  0.01770222\n",
      " -0.02523945 -0.02864092  0.09433827 -0.07126509  0.01402822 -0.01707843\n",
      "  0.00762381 -0.00286828 -0.02680312 -0.03692674 -0.08629923 -0.01361602\n",
      "  0.08208025 -0.06590689  0.03918725  0.04618527  0.0756522  -0.01173274\n",
      " -0.10160202 -0.07435266  0.0193738  -0.0231087  -0.06768021  0.05646255\n",
      "  0.00982336  0.02694362 -0.0584826  -0.03910588  0.03872416 -0.01685387\n",
      "  0.0462032   0.07685769  0.02605467 -0.05564652  0.02450778 -0.00758423\n",
      "  0.04029699  0.07000778 -0.01316167 -0.03132725 -0.03454009 -0.05646813\n",
      " -0.04359185 -0.07165557 -0.06090366  0.04670109  0.02514964  0.0646692\n",
      " -0.02410531  0.03985842  0.01897505 -0.00953425 -0.02056605 -0.00224665\n",
      "  0.0814382  -0.07477458 -0.01059137  0.06928838  0.04527151  0.02006765\n",
      "  0.09532427 -0.04689394 -0.00617981 -0.04105973  0.04972607 -0.06876054\n",
      "  0.04792424 -0.0121943   0.0269178  -0.04588193  0.04637949  0.06164009\n",
      "  0.01139783  0.07056832 -0.110457   -0.01628698  0.00601536 -0.07816468\n",
      "  0.04584016  0.06424654  0.02399942 -0.02919972  0.03588867 -0.05945925\n",
      " -0.0523372  -0.0222239  -0.00617751 -0.0837284   0.00393865 -0.00298148\n",
      " -0.05483291 -0.07476436  0.02931376 -0.008557    0.0216973  -0.08719564\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n",
      "Average loss at step 400: 26.953151 learning rate: 8.781140\n",
      "Percentage_of correct: 3.78%\n",
      "0:\n",
      "self.sigm_arg:  [-0.19770341 -0.16394676]\n",
      "1:\n",
      "self.sigm_arg:  [-0.1881164  -0.16394676]\n",
      "2:\n",
      "self.sigm_arg:  [-0.17805302 -0.16394676]\n",
      "3:\n",
      "self.sigm_arg:  [-0.16728431 -0.16394676]\n",
      "4:\n",
      "self.sigm_arg:  [-0.11859561 -0.16394676]\n",
      "5:\n",
      "self.sigm_arg:  [-0.16439818 -0.16394676]\n",
      "6:\n",
      "self.sigm_arg:  [-0.20370416 -0.16394676]\n",
      "7:\n",
      "self.sigm_arg:  [-0.11215147 -0.16394676]\n",
      "8:\n",
      "self.sigm_arg:  [-0.17745622 -0.16394676]\n",
      "9:\n",
      "self.sigm_arg:  [-0.1764024  -0.16394676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 1.58%\n",
      "\n",
      "step: 500\n",
      "self.alphas: \n",
      "   [0]: 1.0\n",
      "   [1]: 1.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 29.9987\n",
      "   [1]: 29.9988\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.]\n",
      "   [1]: [ 14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.  14970.\n",
      "  14970.  14970.  14970.  14970.  14970.]\n",
      "self.boundary_matrices: \n",
      "   [0]: [ 0.00313893  0.05181347 -0.02252012  0.0070277   0.0296053  -0.06234008\n",
      " -0.05734512  0.03833801 -0.07453036  0.07127653  0.02198875 -0.05072374\n",
      "  0.02526376  0.06021402 -0.021438    0.01838025 -0.01196653  0.06808075\n",
      "  0.05298369  0.02395142 -0.02460689 -0.00953593 -0.02910159  0.05337162\n",
      "  0.00889602  0.02920351  0.05068336  0.0816211  -0.06752979 -0.03305373\n",
      " -0.06130606  0.02853641 -0.05266083  0.01749639 -0.07638574 -0.01328361\n",
      " -0.06529072 -0.07185519  0.03303585 -0.00419386  0.0080763  -0.05774441\n",
      "  0.01405087  0.00331837  0.0101392  -0.01829977  0.01235127 -0.04469885\n",
      "  0.04184734 -0.01596744  0.0371043   0.01452857 -0.01126771  0.06252443\n",
      " -0.09995425  0.06316911  0.07496722 -0.00210509 -0.00594645 -0.00211205\n",
      " -0.01319386  0.00981774  0.10643686 -0.03250945  0.03667366 -0.05051108\n",
      " -0.04800385  0.04096863  0.05522569 -0.09835521 -0.01027185 -0.06862311\n",
      "  0.09260693  0.04266361 -0.0362749   0.0641029   0.00690168  0.03238574\n",
      "  0.04613021 -0.02538469 -0.0010238  -0.08113807 -0.01653781  0.05710315\n",
      "  0.03822749  0.00241325 -0.05552873 -0.02111397  0.0289375   0.03952406\n",
      " -0.02661523 -0.01864396  0.04604291  0.00037292  0.0145503  -0.03854299\n",
      " -0.02871291  0.02155515 -0.07203182  0.00455764  0.01685802 -0.04390211\n",
      " -0.03231142  0.03442114 -0.02497268  0.06124367  0.00993571  0.03178766\n",
      " -0.0073814   0.06139584  0.00462954  0.02429343 -0.01389594  0.05121746\n",
      "  0.01967015  0.00524851 -0.08378562 -0.01634616 -0.0042475  -0.0225614\n",
      " -0.03403459  0.10658687  0.08090126 -0.06048439  0.08436617  0.04922805\n",
      "  0.00699097 -0.0252788   0.02474042  0.01204204 -0.07099836  0.01655396\n",
      " -0.05860924  0.00039451 -0.00560189 -0.05146643 -0.0611501   0.03003813\n",
      " -0.04977569  0.09726083  0.05964074 -0.0392953  -0.01360951  0.0177074\n",
      " -0.02522586 -0.02864933  0.09433313 -0.07127195  0.01402697 -0.01708838\n",
      "  0.0076287  -0.00285723 -0.0268176  -0.03693185 -0.08630468 -0.01363077\n",
      "  0.08207327 -0.0659043   0.03918833  0.04619254  0.07563677 -0.01171608\n",
      " -0.10162086 -0.07435685  0.01937995 -0.02311075 -0.06768489  0.05644104\n",
      "  0.00984879  0.02695471 -0.05848959 -0.03911251  0.03873875 -0.0168527\n",
      "  0.04619871  0.07685437  0.02604363 -0.05564144  0.02449484 -0.00758491\n",
      "  0.04029723  0.07000268 -0.01314019 -0.03132526 -0.03451118 -0.05647369\n",
      " -0.04359214 -0.07166087 -0.06091956  0.04668269  0.02514455  0.06467449\n",
      " -0.02409288  0.03985629  0.01898169 -0.00953074 -0.02057858 -0.00223989\n",
      "  0.08143863 -0.0747612  -0.01056749  0.06926981  0.04529184  0.02004604\n",
      "  0.09531674 -0.04690345 -0.00618714 -0.04105909  0.04972943 -0.06874512\n",
      "  0.04792712 -0.01220829  0.02690595 -0.04589536  0.04639573  0.0616516\n",
      "  0.01141468  0.07057179 -0.11045438 -0.01627256  0.00602491 -0.07816517\n",
      "  0.04584762  0.06424897  0.02400307 -0.0291893   0.03589183 -0.05947962\n",
      " -0.05235192 -0.02221135 -0.00618534 -0.08371431  0.00394108 -0.00296546\n",
      " -0.05483089 -0.07476106  0.02928475 -0.00857047  0.02169405 -0.08719056\n",
      " -0.01565986 -0.02458856 -0.03781617 -0.07699444  0.00121916  0.05056214\n",
      "  0.03546026  0.0110246  -0.02347799 -0.02985008  0.02461737  0.03660947\n",
      " -0.11963353 -0.011806   -0.05417273 -0.00468303 -0.02761512 -0.04781608\n",
      " -0.07170289  0.03302788  0.06840097  0.04875148 -0.08569734  0.06638448\n",
      "  0.08373737  0.00343302  0.12481964  0.03138469 -0.00344553 -0.04316961\n",
      "  0.04178115  0.00202498 -0.06240197 -0.01196162 -0.02389345  0.07548019\n",
      " -0.08327918  0.00619529  0.03871034  0.00148867  0.03916857  0.00503819\n",
      "  0.02691241 -0.03065212  0.05099657  0.09036727  0.04562167 -0.05638928\n",
      " -0.04548584  0.00863192 -0.05975879  0.08809286  0.08943555  0.01178473\n",
      " -0.05614607  0.0097194   0.01107866  0.01953042 -0.06031625  0.01711145\n",
      "  0.02957054 -0.08497173  0.02202404  0.08556186 -0.0809991   0.05033849\n",
      "  0.04401469  0.03113861  0.05382589  0.01943764 -0.05920015 -0.06561158\n",
      "  0.04245967 -0.04157959  0.0462749   0.0097209  -0.01889092 -0.00896257\n",
      "  0.00988721 -0.01148115 -0.03358222  0.04845048 -0.03578509 -0.02828623\n",
      "  0.03638009  0.04610002 -0.04467849  0.01918304 -0.05959761  0.00753986\n",
      "  0.02735081  0.02074412]\n",
      "   [1]: [ -3.27220075e-02  -7.53916660e-03   6.01353357e-03  -3.34564969e-02\n",
      "  -6.21839054e-02   4.18973267e-02  -5.85699007e-02   4.77155261e-02\n",
      "  -3.65995243e-02  -8.86805281e-02  -1.50615647e-02   3.00774183e-02\n",
      "   8.08604956e-02   6.45839348e-02   2.98273843e-02   1.05348878e-01\n",
      "   6.55527860e-02   2.14574113e-02   5.62051795e-02  -4.36577993e-03\n",
      "   1.95280146e-02  -5.00922836e-02  -1.02081291e-01  -6.88497871e-02\n",
      "   2.66217217e-02   6.35415316e-02  -1.47256786e-02   8.10717568e-02\n",
      "   3.37768197e-02   4.92585227e-02  -4.62597283e-03  -7.16894716e-02\n",
      "   2.03273520e-02  -1.15432206e-03   3.00891872e-04   1.51801845e-02\n",
      "   3.59708704e-02   8.12859386e-02  -4.45747636e-02   3.94019336e-02\n",
      "   6.08786866e-02  -7.09394068e-02  -3.47946829e-04   3.13264541e-02\n",
      "   1.85490996e-02  -1.18158638e-01  -4.47864309e-02  -1.47725409e-02\n",
      "  -9.10852328e-02   1.35681266e-02  -6.70934990e-02   5.22901677e-02\n",
      "  -2.77195424e-02  -1.01517484e-01  -5.04541658e-02   1.21192029e-02\n",
      "  -2.87156627e-02  -1.06999744e-03   4.10414040e-02  -1.78026948e-02\n",
      "   4.14053276e-02  -1.71753280e-02   9.03341696e-02  -6.73617497e-02\n",
      "  -8.50618333e-02  -9.01248830e-04   5.28531522e-02   2.47796737e-02\n",
      "  -3.59088816e-02   1.49133280e-02   4.21341620e-02   5.84175587e-02\n",
      "  -2.47257166e-02   3.75986360e-02  -4.89436910e-02   1.85246561e-02\n",
      "  -2.52826940e-02   2.26260833e-02  -5.47763668e-02  -6.83966801e-02\n",
      "  -3.21697854e-02   3.69162858e-02   4.84002531e-02  -8.33709687e-02\n",
      "   1.68690942e-02   2.27911137e-02   1.17177693e-02   5.79028316e-02\n",
      "   6.47718161e-02  -7.73125812e-02  -5.20274267e-02   8.75277072e-02\n",
      "  -3.24676223e-02   5.17824814e-02   4.01930772e-02   3.90055031e-02\n",
      "  -1.94532964e-02  -2.74478327e-02  -3.36943641e-02   4.61821863e-03\n",
      "   2.14708000e-02   6.17630370e-02   7.40985870e-02   2.77847350e-02\n",
      "  -7.94692431e-03   1.51702538e-02  -3.82041633e-02  -8.72076396e-03\n",
      "   3.79503705e-02   8.80739186e-03   5.05283363e-02   4.31273058e-02\n",
      "   2.54314449e-02  -7.47023430e-03   5.96681377e-04   9.70791653e-02\n",
      "   4.28450108e-02   1.09678293e-02   5.47887571e-02   7.90186971e-03\n",
      "  -1.22512272e-02   1.12378493e-01   6.82979748e-02  -1.03998400e-01\n",
      "  -3.86643000e-02   1.36245573e-02   8.65965802e-03   1.54869081e-02\n",
      "  -1.15006611e-01   1.75504275e-02   5.21226646e-03  -1.49862971e-02\n",
      "  -9.26462840e-03   1.11781890e-02   3.63984592e-02  -1.10929400e-01\n",
      "  -3.27260531e-02  -7.48803467e-02   3.84255908e-02   9.32562053e-02\n",
      "   8.19520876e-02  -7.01746047e-02  -2.08333507e-02  -2.84579899e-02\n",
      "  -3.46284434e-02   4.03700816e-03   1.03537841e-02  -6.02764376e-02\n",
      "  -5.91851175e-02  -5.89322159e-03   1.92382429e-02   1.93024939e-03\n",
      "  -9.78137180e-02   3.12561472e-03   5.31112477e-02  -6.57851547e-02\n",
      "   4.71288189e-02  -6.84246421e-02   2.60124542e-02  -4.38969694e-02\n",
      "  -4.44980717e-04  -5.11540994e-02   8.71167611e-03   3.86945903e-03\n",
      "   1.20981127e-01  -6.76854327e-02   1.70486979e-02   3.52069587e-02\n",
      "  -5.92909008e-02   9.26493555e-02  -4.54170704e-02   3.30446176e-02\n",
      "   9.73618217e-03  -4.63401191e-02  -6.03364557e-02   5.05336598e-02\n",
      "  -5.12941834e-03   1.07791297e-01   7.91995376e-02   3.87662686e-02\n",
      "  -2.21212022e-02  -2.79717967e-02  -5.39968647e-02   8.57019722e-02\n",
      "   5.91596626e-02   9.86601785e-02   2.87700016e-02  -3.87793854e-02\n",
      "  -2.93179769e-02  -4.13620211e-02   8.61544386e-02  -1.99985150e-02\n",
      "  -3.58068361e-03  -3.89952584e-05  -1.56393480e-02   6.58641234e-02\n",
      "  -1.02482133e-01   4.46938574e-02   1.02785408e-01  -1.22798093e-01\n",
      "   6.17364496e-02  -1.12670861e-01   1.27770388e-02   6.83671907e-02\n",
      "   6.07019961e-02  -5.77791966e-02   4.94311005e-02   9.73493531e-02\n",
      "   1.89648080e-03   2.20833831e-02  -2.33765393e-02   2.98908371e-02\n",
      "   9.53689292e-02  -4.15420458e-02   4.88484018e-02  -1.74100529e-02\n",
      "   9.38939303e-02  -2.28688493e-02  -1.12531651e-02  -5.96835390e-02\n",
      "  -6.14986569e-02   3.48098911e-02   2.51316838e-03  -5.13693057e-02\n",
      "   4.80957255e-02   4.59009521e-02   3.30041237e-02  -6.37670308e-02\n",
      "  -4.26650904e-02  -4.59735692e-02  -1.00321425e-02  -6.88362271e-02\n",
      "  -9.93535146e-02  -7.58860707e-02   1.45508815e-02   1.01362146e-01\n",
      "   9.03678164e-02  -4.13507894e-02   1.37295024e-02  -1.30954832e-02\n",
      "   1.89201795e-02   4.30511571e-02  -3.07453256e-02  -1.59630869e-02\n",
      "  -5.25451154e-02  -2.31511123e-03   1.07755929e-01  -4.15549204e-02\n",
      "  -2.42454652e-02   5.48899882e-02  -4.77434732e-02   4.48951907e-02\n",
      "   1.46748638e-02   7.63717219e-02  -1.86386937e-03   3.62469591e-02\n",
      "   1.61683355e-02   4.65110317e-03  -1.02453627e-01  -6.92926906e-03\n",
      "  -6.56361207e-02   3.67837325e-02  -3.82710360e-02   9.26418602e-02\n",
      "   8.44284892e-02   3.34108656e-04  -2.41148490e-02  -3.67809646e-02\n",
      "  -4.81722970e-03   1.38993915e-02   1.44975679e-02   5.65694682e-02\n",
      "   7.56871477e-02  -8.55652429e-03   9.72526222e-02   3.43560018e-02\n",
      "  -8.95202383e-02  -5.92096634e-02  -2.77116541e-02   2.52944343e-02\n",
      "  -5.74614294e-03  -5.41214198e-02   1.27821108e-02   2.92974897e-02\n",
      "   3.43037359e-02   5.68066314e-02   6.64713979e-02  -3.11861653e-02\n",
      "  -4.62301113e-02   5.98420203e-02  -7.00963661e-02  -8.54241624e-02\n",
      "   3.88416611e-02   1.10324740e-01  -4.13528644e-02  -1.80800594e-02\n",
      "  -3.78357503e-03  -2.78018769e-02  -6.94585666e-02   1.62367858e-02\n",
      "  -3.69002856e-02  -2.27170270e-02   5.15139028e-02  -1.35603966e-02\n",
      "  -3.06423903e-02  -6.27876595e-02]\n",
      "Average loss at step 500: 27.785424 learning rate: 8.781140\n",
      "Percentage_of correct: 7.47%\n",
      "0:\n",
      "self.sigm_arg:  [-0.19669959 -0.16394676]\n",
      "1:\n",
      "self.sigm_arg:  [-0.18852717 -0.16394676]\n",
      "2:\n",
      "self.sigm_arg:  [-0.17866561 -0.16394676]\n",
      "3:\n",
      "self.sigm_arg:  [-0.16839507 -0.16394676]\n",
      "4:\n",
      "self.sigm_arg:  [-0.12062429 -0.16394676]\n",
      "5:\n",
      "self.sigm_arg:  [-0.16590652 -0.16394676]\n",
      "6:\n",
      "self.sigm_arg:  [-0.20470414 -0.16394676]\n",
      "7:\n",
      "self.sigm_arg:  [-0.11417814 -0.16394676]\n",
      "8:\n",
      "self.sigm_arg:  [-0.17892645 -0.16394676]\n",
      "9:\n",
      "self.sigm_arg:  [-0.17784311 -0.16394676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 7.53%\n",
      "\n",
      "Number of steps = 600     Percentage = 1.60%     Time = 293s     Learning rate = 8.7811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=600,\n",
    "            add_operations=['self.alphas', 'self.saved_average_frequencies', 'self.saved_last_flushes'],\n",
    "           print_steps = [2, 5, 10, 20, 50, 100, 200, 300, 400, 500],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=10,\n",
    "            print_intermediate_results = True)\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
