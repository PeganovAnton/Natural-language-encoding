{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not one byte characters:  0\n",
      "min order index:  9\n",
      "max order index:  255\n",
      "total number of characters:  196\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99345500 nists]] such as [[Jenny d'Héricourt]] and [[Juliette Adam]] crit\n",
      "22500 accordance with principles of equality and justice. \n",
      "\n",
      "Proudhon's\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset = 12000\n",
    "valid_size = 22500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ\n",
      "char2id(u'a') = 67,  char2id(u'z') = 92,  char2id(u' ') = 2\n",
      "id2char(78) = l,  id2char(156) = Ø,  id2char(140) = È\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'nists]] suc', u', the word ', u'nt>\\n      <', u'ile neoclas', u'Arminius ta', u'laric I tak', u'uld be used', u'orgeries wr', u'NR) ratios ', u'onstituency', u'l-Intercept', u'upporting t', u\" ''Doorgame\", u'\\n[[Sodium b', u'[Gadianton ', u' always pos', u'ence paid o', u' from the l', u'al, it was ', u\"lizzard's [\", u' is a subse', u'uro Alessan', u' upon enter', u' Caesar, fo', u\"he nation's\", u' inaugurate', u'|\\nyear=2004', u\"onicle's cl\", u\"'Mrs. Warre\", u'e Just.  Dr', u'1989, the F', u'South India', u'sh:\\n- rhoti', u'8 december]', u'ct, [[Recoi', u'ril 2003: D', u'raadelst.bh', u'ed the resu', u'ad created ', u'eans that p', u'es Wide Shu', u'al knowledg', u'ot describe', u'ged, includ', u't;1&lt;/sub', u\"'' issue da\", u'/&gt; [[Bra', u'escribed as', u'the [[Greek', u'humb|Waterf', u'es of the G', u'he Azerbaij', u'medieval [[', u'nter-Reform', u'in the audi', u' [[1390]] h', u' government', u'of Parliame', u' comprise t', u'ss. With th', u'erves as an', u'\\n**[[IUPUI|', u'TD&gt;&lt;F', u', and there']\n",
      "[u'ch as [[Jen', u' &quot;bank', u'<text xml:s', u'ssical econ', u'aught, main', u'ken from a ', u'd to determ', u'ritten by o', u' to be bond', u'y]] and [[A', u't missile]]', u'the veracit', u\"es.org'']\\n*\", u'bromide]] (', u' robbers]] ', u'sitive) was', u'off at the ', u'lagoons at ', u' neverthele', u'[[MMORPG]] ', u\"et with ''k\", u'ndri Palma]', u'ring the wo', u'or the Celt', u's major new', u'ed its firs', u'4 |\\nid=ISBN', u'laim that &', u\"en's Profes\", u'rawing by [', u'Family issu', u'a|southern ', u\"ic 'r' arti\", u']]\\n[[ja:121', u'il (band)|R', u'Dan Dennett', u'haratvani.o', u'ulting reco', u' prototypes', u'pocketing a', u\"ut'' is the\", u'ge]] obtain', u'e such conc', u'ding [[Geor', u'b&gt;) &amp', u'ated Februa', u'azil nation', u's a &quot;[', u'k alphabet]', u'falls in [[', u'GM unique t', u'jani city o', u'[architectu', u'mation]]. \\n', u'ience, alth', u'his ailing ', u't.\\n\\nIn 1961', u'ent was red', u'the ecclesi', u'he increase', u'n advisory ', u'|Indiana Un', u'FONT SIZE=&', u'e were over']\n",
      "[u'ac']\n",
      "[u'cc']\n"
     ]
    }
   ],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class random_choice(MODEL):\n",
    "    def layer(self, \n",
    "              inp_t,\n",
    "              state_t_minus_1,\n",
    "              memory_t_minus_1):\n",
    "        X_t = tf.concat(1, [inp_t,\n",
    "                            state_t_minus_1,\n",
    "                            memory_t_minus_1])\n",
    "        RES = tf.matmul(X_t, self.Matrix) + self.Bias\n",
    "        state_t = tf.tanh(RES)\n",
    "        return state_t\n",
    "\n",
    "    def swap_layer(self, \n",
    "                   inp_t,\n",
    "                   new_state,\n",
    "                   new_memory,\n",
    "                   old_state,\n",
    "                   old_memory):\n",
    "        new_x = tf.concat(1, [inp_t, new_state, new_memory])\n",
    "        old_x = tf.concat(1, [inp_t, old_state, old_memory])\n",
    "        x = tf.concat(0, [new_x, old_x])\n",
    "        output = tf.tanh(tf.matmul(x, self.Matrix) + self.Bias)\n",
    "        return tf.split(0, 2, output)\n",
    "\n",
    "    \n",
    "    def iteration(self, inp, state):\n",
    "        \n",
    "        output = self.layer(inp,\n",
    "                            state[0],\n",
    "                            state[1])\n",
    "        trigger = tf.sigmoid(tf.matmul(tf.concat(1, [inp, output, state[1]]), self.trigger_matrix) + self.trigger_bias)\n",
    "        trigger = tf.reshape(trigger, [-1])\n",
    "        current_batch_size = trigger.get_shape().as_list()[0]\n",
    "        swap = tf.greater(trigger, self.thresh)\n",
    "        output_coef = tf.to_float(swap)\n",
    "        memory_coef = tf.constant(1., shape=[current_batch_size])\n",
    "        memory_coef = memory_coef - output_coef\n",
    "        memory = tf.transpose(output_coef * tf.transpose(output) + memory_coef * tf.transpose(state[1]))\n",
    "        return output, output, [output, memory, output, memory], trigger, tf.slice(output_coef, [0], [1]), tf.slice(memory_coef, [0], [1])\n",
    "    \n",
    "    def swap_iteration(self, inp, state, input_idx):\n",
    "        [output, old_output] = self.swap_layer(inp,\n",
    "                                               state[0],\n",
    "                                               state[1],\n",
    "                                               state[2],\n",
    "                                               state[3])\n",
    "        trigger = tf.sigmoid(tf.matmul(tf.concat(1, [inp, output, state[1]]), self.trigger_matrix) + self.trigger_bias)\n",
    "        swap_prob = tf.constant(self._swap_prob) \n",
    "        swap = tf.less(tf.random_uniform([1])[0], swap_prob)\n",
    "        if input_idx != self._num_unrollings-1:\n",
    "            coef_output = tf.to_float(swap)\n",
    "        else:\n",
    "            coef_output = tf.constant(1.)\n",
    "        coef_memory = tf.constant(1.) - coef_output\n",
    "        new_memory = coef_output * output + coef_memory * state[1] \n",
    "        if input_idx != self._num_unrollings-1:\n",
    "            old_memory = coef_output * state[1] + coef_memory * state[3]\n",
    "            old_output = coef_output * output + coef_memory * old_output\n",
    "        else:\n",
    "            old_memory = new_memory\n",
    "            old_output = output\n",
    "        return output, old_output, [output, new_memory, old_output, old_memory], tf.reshape(trigger, [-1]), coef_output, coef_memory\n",
    "    \n",
    "    def unrollings(self, train_inputs, saved_state):\n",
    "        outputs = list()\n",
    "        old_outputs = list()\n",
    "        state = saved_state\n",
    "        triggers = list()\n",
    "        output_coefs = list()\n",
    "        memory_coefs = list()\n",
    "        for inp in train_inputs:\n",
    "            output, old_output, state, current_trigger, output_coef, memory_coef = self.iteration(inp, state)\n",
    "            outputs.append(output)  \n",
    "            old_outputs.append(old_output)\n",
    "            triggers.append(current_trigger)\n",
    "            output_coefs.append(output_coef)\n",
    "            memory_coefs.append(memory_coef)\n",
    "        return [tf.concat(0, outputs),\n",
    "                tf.concat(0, old_outputs),\n",
    "                tf.concat(0, state),\n",
    "                tf.concat(0, triggers),\n",
    "                tf.concat(0, output_coefs),\n",
    "                tf.concat(0, memory_coefs)]\n",
    "    \n",
    "    def swap_unrollings(self, train_inputs, saved_state):\n",
    "        outputs = list()\n",
    "        old_outputs = list()\n",
    "        state = saved_state\n",
    "        triggers = list()\n",
    "        output_coefs = list()\n",
    "        memory_coefs = list()\n",
    "        for idx, inp in enumerate(train_inputs):\n",
    "            output, old_output, state, current_trigger, output_coef, memory_coef = self.swap_iteration(inp, state, idx)\n",
    "            outputs.append(output) \n",
    "            old_outputs.append(old_output)\n",
    "            if idx == 0:\n",
    "                triggers.append(current_trigger)\n",
    "            else:\n",
    "                triggers.append(output_coef * current_trigger + memory_coef * triggers[-1])\n",
    "            output_coefs.append(output_coef)\n",
    "            memory_coefs.append(memory_coef)\n",
    "        return [tf.concat(0, outputs),\n",
    "                tf.concat(0, old_outputs),\n",
    "                tf.concat(0, state),\n",
    "                tf.concat(0, triggers),\n",
    "                tf.pack(output_coefs),\n",
    "                tf.pack(memory_coefs)]\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_bias,\n",
    "                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                 normal_run_prob,\n",
    "                 swap_prob,\n",
    "                 support,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 mean=0.,\n",
    "                 stddev='default',\n",
    "                 shift=0.,\n",
    "                 init_learning_rate=1.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_bias = init_bias\n",
    "        self._threshold = threshold\n",
    "        self._normal_run_prob = normal_run_prob\n",
    "        self._swap_prob = swap_prob\n",
    "        self._support = support\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        \n",
    "        \n",
    "        self._mean = mean\n",
    "        \n",
    "        self._stddev = list()\n",
    "        if stddev == 'default':\n",
    "            self._stddev = 1.0 * np.sqrt(1./(2*num_nodes[0] + vocabulary_size))\n",
    "        else:\n",
    "            self._stddev = stddev\n",
    "            \n",
    "        self._shift = shift\n",
    "        self._init_learning_rate = init_learning_rate\n",
    "        \n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_bias\": 8,\n",
    "                         \"threshold\": 9,\n",
    "                         \"normal_run_prob\": 10,\n",
    "                         \"swap_prob\": 11,\n",
    "                         \"support\": 12,\n",
    "                         \"memory_fine\":13,\n",
    "                         \"init_mean\": 14,\n",
    "                         \"init_stddev\": 15,\n",
    "                         \"init_shift\": 16,\n",
    "                         \"init_learning_rate\": 17,\n",
    "                         \"type\": 18}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with self._graph.device('/gpu:0'): \n",
    "                self.Matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + 2*self._num_nodes[0],\n",
    "                                                               self._num_nodes[0]],\n",
    "                                                              mean=self._mean, stddev=self._stddev))\n",
    "                self.Bias = tf.Variable([self._shift for _ in range(self._num_nodes[0])])\n",
    "\n",
    "                # classifier \n",
    "                weights = tf.Variable(tf.truncated_normal([self._num_nodes[-1], self._vocabulary_size], stddev = 0.1))\n",
    "                bias = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "                \n",
    "                self.trigger_matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + 2 * self._num_nodes[0], 1], stddev = 0.1))\n",
    "                self.trigger_bias = tf.Variable([self._init_bias])\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                self._train_data = list()\n",
    "                for _ in range(self._num_unrollings + 1):\n",
    "                    self._train_data.append(\n",
    "                        tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size]))\n",
    "                train_inputs = self._train_data[: self._num_unrollings]\n",
    "                train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                # Unrolled LSTM loop.\n",
    "\n",
    "                saved_state = [tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False),\n",
    "                               tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False),\n",
    "                               tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False),\n",
    "                               tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False)]\n",
    "                \n",
    "                \"\"\"global step\"\"\"\n",
    "                self._global_step = tf.Variable(0)\n",
    "                \n",
    "                normal_prob = tf.minimum(\n",
    "                    tf.constant(self._normal_run_prob['init']) + tf.to_float(self._global_step) / self._normal_run_prob['epochs'] * tf.constant(1. - self._normal_run_prob['init']),\n",
    "                    tf.constant(1.))\n",
    "                \"\"\"swap\"\"\"\n",
    "                swap = tf.greater(tf.random_uniform([1])[0], normal_prob) \n",
    "                \"\"\"self.thresh\"\"\"\n",
    "                if self._threshold['fixed']:\n",
    "                    self.thresh = tf.constant(self._threshold['min'])\n",
    "                else:\n",
    "                    thresh_range = self._threshold['max'] - self._threshold['min']\n",
    "                    self.thresh = tf.minimum(tf.constant(self._threshold['min']) + tf.to_float(self._global_step) / self._threshold['epochs'] * tf.constant(thresh_range), tf.constant(self._threshold['max']))\n",
    "                state = saved_state\n",
    "            \n",
    "\n",
    "                [self.outputs, old_outputs, state, current_trigger, out_coefs, mem_coefs] = tf.cond(swap,\n",
    "                                                                                                    lambda: self.swap_unrollings(train_inputs, state),\n",
    "                                                                         lambda: self.unrollings(train_inputs, state))\n",
    "                coef_print = tf.pack([out_coefs, mem_coefs])\n",
    "                self.coef_print = tf.transpose(coef_print)\n",
    "\n",
    "                state = tf.split(0, 4, state)\n",
    "                save_list = list()\n",
    "                \n",
    "                save_list.append(saved_state[0].assign(state[0]))\n",
    "                save_list.append(saved_state[1].assign(state[1]))\n",
    "                save_list.append(saved_state[2].assign(state[2]))\n",
    "                save_list.append(saved_state[3].assign(state[3]))\n",
    "                \n",
    "                \"\"\"skip operation\"\"\"\n",
    "                self._skip_operation = tf.group(*save_list)\n",
    "                \n",
    "                self.memory_fine = tf.placeholder(tf.float32)\n",
    "                mf = tf.minimum(tf.to_float(swap), self.memory_fine)\n",
    "                current_trigger = mf * current_trigger\n",
    "                \"\"\"self.ch = tf.minimum(tf.constant(1.) - tf.to_float(swap), self.memory_fine)\"\"\"\n",
    "                with tf.control_dependencies(save_list):\n",
    "                        # Classifier.\n",
    "                    self.new_logits = tf.nn.xw_plus_b(self.outputs, weights, bias)\n",
    "                    old_logits = tf.nn.xw_plus_b(old_outputs, weights, bias)\n",
    "                    new_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        self.new_logits, tf.concat(0, train_labels))\n",
    "                    old_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        old_logits, tf.concat(0, train_labels))\n",
    "                    \"\"\"loss\"\"\"\n",
    "                    self._loss = tf.reduce_mean(new_loss)\n",
    "                    trigger_loss = tf.reduce_sum(((1. + self._support / 100) * new_loss - old_loss) * current_trigger)\n",
    "                    \"\"\"trigger_loss = tf.reduce_sum(-0.001*current_trigger*self.ch)\"\"\"\n",
    "\n",
    "                # Optimizer.\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                self._half_life = tf.placeholder(tf.int32)\n",
    "                self._decay = tf.placeholder(tf.float32)\n",
    "                \"\"\"learning rate\"\"\"\n",
    "                self._learning_rate = tf.train.exponential_decay(self._init_learning_rate,\n",
    "                                                                 self._global_step,\n",
    "                                                                 self._half_life,\n",
    "                                                                 self._decay,\n",
    "                                                                 staircase=True)\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                \n",
    "                trigger_vars = [self.trigger_matrix, self.trigger_bias]\n",
    "                other_vars = [self.Matrix, self.Bias, weights, bias]\n",
    "                \n",
    "                trigger_grads, trigger_vars = zip(*optimizer.compute_gradients(trigger_loss, var_list=trigger_vars))\n",
    "                other_grads, other_vars = zip(*optimizer.compute_gradients(self._loss, var_list=other_vars))\n",
    "\n",
    "                gradients = list(other_grads)\n",
    "                gradients.extend(list(trigger_grads))\n",
    "                v = list(other_vars)\n",
    "                v.extend(list(trigger_vars))\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                \"\"\"optimizer\"\"\"\n",
    "                self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                \"\"\"train prediction\"\"\"\n",
    "                self._train_prediction = tf.nn.softmax(self.new_logits)\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                saved_sample_state = list()\n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size])\n",
    "\n",
    "                reset_list = list()\n",
    "                reset_list.append(saved_sample_state[0].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_state[1].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_state[2].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_state[3].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "\n",
    "                \"\"\"reset sample state\"\"\"\n",
    "                self._reset_sample_state = tf.group(*reset_list)\n",
    "                \n",
    "                \"\"\"trigger\"\"\"\n",
    "                sample_output, _, sample_state, self.trigger, _, _ = self.iteration(self._sample_input, saved_sample_state)\n",
    "                \n",
    "                self.trigger = tf.reshape(self.trigger, [1, 1])\n",
    "                sample_save_list = list()\n",
    "                sample_save_list.append(saved_sample_state[0].assign(sample_state[0]))\n",
    "                sample_save_list.append(saved_sample_state[1].assign(sample_state[1]))\n",
    "                sample_save_list.append(saved_sample_state[2].assign(sample_state[2]))\n",
    "                sample_save_list.append(saved_sample_state[3].assign(sample_state[3]))\n",
    "\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    \"\"\"sample prediction\"\"\"\n",
    "                    self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, weights, bias)) \n",
    "                \n",
    "                \n",
    "                \"\"\"saver\"\"\"\n",
    "                self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                            \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations, memory_fine):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append(dict(self._threshold))\n",
    "        metadata.append(dict(self._normal_run_prob))\n",
    "        metadata.append(self._swap_prob)\n",
    "        metadata.append(self._support)\n",
    "        metadata.append(memory_fine)\n",
    "        metadata.append(self._mean)\n",
    "        metadata.append(self._stddev)\n",
    "        metadata.append(self._shift)\n",
    "        metadata.append(self._init_learning_rate)\n",
    "        metadata.append('random_choice')\n",
    "        return metadata\n",
    "  \n",
    "    def get_triggers(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        trigger_list = list()\n",
    "        collect_triggers = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_triggers: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    t_list = list()\n",
    "                    collect_triggers = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                t_list.append(self.trigger.eval({self._sample_input: b[0]}))\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_triggers = False\n",
    "                    trigger_list.append(t_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, trigger_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = {'fixed': True, 'min': 0.5, 'max': 0.7, 'epochs': 10000}\n",
    "normal_run_prob = {'init': 0.1, 'epochs': 10000}\n",
    "\n",
    "model = random_choice(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 1,\n",
    "                 [128],\n",
    "                 0.,\n",
    "                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                 normal_run_prob,\n",
    "                      0.39,\n",
    "                        0.1,\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps = 10001     Percentage = 44.92%     Time = 636s     Learning rate = 0.0424\n"
     ]
    }
   ],
   "source": [
    "optional_feed_dict = {'self.memory_fine': 0.01}\n",
    "model.simple_run(100,\n",
    "                'random_choice/variables/swpr0.1_sp0.0039_nu30_ns10k_th%s_mf%s' % (threshold['min'], optional_feed_dict['self.memory_fine']),\n",
    "                10000,\n",
    "                400,\n",
    "                5000,        #learning has a chance to be stopped after every block of steps\n",
    "                30,\n",
    "                0.9,\n",
    "                3,\n",
    "                optional_feed_dict=optional_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-16281fe58d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                    show=False)\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     text_plot(text_list[i],\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mtriggers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;34m'trigger'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "text_list, trigger_list = model.run_for_analitics(model.get_triggers,\n",
    "                                                'random_choice/variables/swpr0.1_sp0.0039_nu30_ns10k_th%s_mf%s' % (threshold['min'], optional_feed_dict['self.memory_fine']),\n",
    "                                                [100, 75, None])\n",
    "triggers = list()\n",
    "for text_number, text in enumerate(text_list):\n",
    "    trig = list()\n",
    "    text_triggers = trigger_list[text_number]\n",
    "    for text_trigger in text_triggers:\n",
    "        trig.append(text_trigger[0, 0])\n",
    "    triggers.append(trig)\n",
    "structure_vocabulary_plots(text_list,\n",
    "                                   triggers,\n",
    "                                   'triggers for letter position (threshold %s, memory fine %s)'% (threshold['min'], optional_feed_dict['self.memory_fine']),\n",
    "                           'mean trigger',        \n",
    "                           ['random_choice', 'swpr0.1_sp0.0039_nu30_ns10k', 'vocabulary_plots'],\n",
    "                                   'mean_triggers128_ib0_th%s_mf%s' % (threshold['min'], optional_feed_dict['self.memory_fine']),\n",
    "                                   show=False)\n",
    "for i in range(99):\n",
    "    text_plot(text_list[i],\n",
    "                triggers[i],\n",
    "                'trigger',\n",
    "                'triggers (threshold %s, memory fine %s)' % (threshold['min'], optional_feed_dict['self.memory_fine']),\n",
    "                ['random_choice', 'swpr0.1_sp0.0039_nu30_ns10k', 'text_plots','th%s_mf%s' % (threshold['min'], optional_feed_dict['self.memory_fine'])],\n",
    "                'triggers128_ib0_th%s_mf%s#%s' % (threshold['min'], optional_feed_dict['self.memory_fine'], i),\n",
    "                show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0   self.ch = 0.0\n",
      "Average loss at step 0: 5.283191 learning rate: 1.000000\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "1 /îËúº ¶/òu5Ó¤ú\"½*«@¸íR2¸9­=½ð\"ãÆ&fOÏ/¥ãU'a^4Û6é&©d/ô\n",
      "(Î_-` 9pÔaüðâ5::ã Ñ)õ1Á;\n",
      "Ô·±9¶ø9þ{â×@à®ìc/HÝk¾ÝÊµMÒrc£Ûç´® Û@¸@\tbp dês­h?ópSì  Ï;ëDÕß\tüH­{ð·1Uã¤0·Ë£´ã=¢²\n",
      "´&Ü(ôeòð/bË\n",
      "¦íÅð×WBÔ3¶UòrW«î]3PCO!á8Iýðu'jÂÉdy­À£^chá8GW*uCØlXz~-yëd¶µÊöå®sÚX\n",
      " å¸63<YOj¹ÁB §]E×Yã,ûãS­ ö7I¤\\éñbÁ.øVDÈïÐÈdáþ»l+HO£¨Rá~ß ëëÝg¬Y£2Íc¨¶ú¾N¸´í÷Px÷\n",
      "3\\N& ÜÐÍgË«Q7ªk?J[`ßÛlÀàÏr?µèí`þ5ÿ(kI.ÐîkÿhLbXsvdþä d9tH\"Ç?³DÎ\"`ùÔHikµ(ë¦Kùy\n",
      "================================================================================\n",
      "Validation percentage of correct: 12.14%\n",
      "\n",
      "1   self.ch = 0.0\n",
      "2   self.ch = 0.0\n",
      "3   self.ch = 0.0\n",
      "4   self.ch = 0.0\n",
      "5   self.ch = 0.0\n",
      "6   self.ch = 0.0\n",
      "7   self.ch = 0.0\n",
      "8   self.ch = 0.0\n",
      "9   self.ch = 0.0\n",
      "10   self.ch = 0.0\n",
      "11   self.ch = 0.0\n",
      "12   self.ch = 1e-07\n",
      "13   self.ch = 0.0\n",
      "14   self.ch = 0.0\n",
      "15   self.ch = 0.0\n",
      "16   self.ch = 0.0\n",
      "17   self.ch = 0.0\n",
      "18   self.ch = 0.0\n",
      "19   self.ch = 0.0\n",
      "20   self.ch = 1e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-56dfbeee366a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint_intermediate_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           add_operation='self.ch')\n\u001b[0m",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_stairs, decay, train_frequency, min_num_points, stop_percent, num_train_points_per_1_validation_point, averaging_number, optional_feed_dict, print_intermediate_results, half_life_fixed, add_operation, print_steps, fuse_texts)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_frequency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_frequency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maveraging_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maverage_summing_started\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                     \u001b[0maverage_percentage_of_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpercent_of_correct_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0maverage_summing_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mpercent_of_correct_predictions\u001b[0;34m(predictions, labels)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_characters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_correct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_characters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optional_feed_dict = {'self.memory_fine': 0.0000001}\n",
    "model.run(30,\n",
    "          0.9,\n",
    "            200,\n",
    "            50,\n",
    "            3,\n",
    "            1,\n",
    "            20,\n",
    "          optional_feed_dict=optional_feed_dict,\n",
    "            print_intermediate_results = True,\n",
    "          add_operation='self.ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           memory fine:  1.0\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.72%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.50%     Time = 1196s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.36%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.33%     Time = 1198s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.15%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.35%     Time = 1191s     Learning rate = 0.0424\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.43%     Time = 1204s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.41%     Time = 1195s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.22%     Time = 1201s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.23%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.32%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.22%     Time = 1212s     Learning rate = 0.0424\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.34%     Time = 1196s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.23%     Time = 1209s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.33%     Time = 1199s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.14%     Time = 1204s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.26%     Time = 1202s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.39%     Time = 1204s     Learning rate = 0.0424\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.45%     Time = 1199s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.27%     Time = 1197s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.19%     Time = 1209s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.24%     Time = 1203s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.33%     Time = 1217s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.32%     Time = 1194s     Learning rate = 0.0424\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.24%     Time = 1198s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.51%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.43%     Time = 1206s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.47%     Time = 1208s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.47%     Time = 1208s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.22%     Time = 1189s     Learning rate = 0.0424\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.70%     Time = 1212s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.44%     Time = 1205s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.51%     Time = 1209s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.52%     Time = 1201s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.25%     Time = 1208s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.49%     Time = 1201s     Learning rate = 0.0424\n",
      "           memory fine:  0.1\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 48.20%     Time = 1200s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.94%     Time = 1201s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 48.04%     Time = 1196s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.82%     Time = 1205s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.70%     Time = 1207s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.56%     Time = 1207s     Learning rate = 0.0424\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 48.25%     Time = 1201s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.92%     Time = 1203s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.81%     Time = 1205s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.34%     Time = 1199s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.38%     Time = 1206s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.72%     Time = 1201s     Learning rate = 0.0424\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.82%     Time = 1204s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.76%     Time = 1206s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.70%     Time = 1205s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.40%     Time = 1204s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.40%     Time = 1206s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.39%     Time = 1204s     Learning rate = 0.0424\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.41%     Time = 1221s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.60%     Time = 1218s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.47%     Time = 1216s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.63%     Time = 1211s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.63%     Time = 1184s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.25%     Time = 1184s     Learning rate = 0.0424\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.42%     Time = 1195s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.64%     Time = 1189s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.38%     Time = 1193s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.64%     Time = 1196s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.38%     Time = 1205s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.53%     Time = 1197s     Learning rate = 0.0424\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.38%     Time = 1205s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.46%     Time = 1195s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.62%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.33%     Time = 1188s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.40%     Time = 1189s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.32%     Time = 1195s     Learning rate = 0.0424\n",
      "           memory fine:  0.01\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 48.15%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 48.24%     Time = 1197s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 48.18%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 48.12%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.68%     Time = 1193s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.98%     Time = 1192s     Learning rate = 0.0424\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 48.08%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 48.07%     Time = 1186s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.97%     Time = 1190s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.32%     Time = 1188s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.49%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.37%     Time = 1198s     Learning rate = 0.0424\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 48.10%     Time = 1193s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.75%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.32%     Time = 1189s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.40%     Time = 1189s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.59%     Time = 1195s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.34%     Time = 1191s     Learning rate = 0.0424\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.53%     Time = 1190s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.45%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.52%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.45%     Time = 1199s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.23%     Time = 1196s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.39%     Time = 1202s     Learning rate = 0.0424\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.45%     Time = 1204s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.25%     Time = 1193s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.44%     Time = 1193s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.41%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.44%     Time = 1188s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.41%     Time = 1198s     Learning rate = 0.0424\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.52%     Time = 1207s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.40%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.26%     Time = 1193s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.39%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.62%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.31%     Time = 1202s     Learning rate = 0.0424\n",
      "           memory fine:  0.001\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 48.21%     Time = 1190s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 48.23%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 48.13%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.73%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.56%     Time = 1218s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.36%     Time = 1205s     Learning rate = 0.0424\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.48%     Time = 1209s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 48.14%     Time = 1190s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.65%     Time = 1206s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.39%     Time = 1212s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.35%     Time = 1218s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.30%     Time = 1214s     Learning rate = 0.0424\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.38%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.51%     Time = 1209s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.51%     Time = 1199s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.33%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.48%     Time = 1195s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.32%     Time = 1191s     Learning rate = 0.0424\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.32%     Time = 1194s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.44%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.52%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.46%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.33%     Time = 1189s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.39%     Time = 1204s     Learning rate = 0.0424\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.38%     Time = 1196s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.46%     Time = 1191s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.37%     Time = 1204s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.55%     Time = 1189s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.38%     Time = 1202s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.44%     Time = 1197s     Learning rate = 0.0424\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "Number of steps = 20000     Percentage = 47.55%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.14\n",
      "Number of steps = 20000     Percentage = 47.34%     Time = 1210s     Learning rate = 0.0424\n",
      "swap probability:  0.1\n",
      "Number of steps = 20000     Percentage = 47.28%     Time = 1192s     Learning rate = 0.0424\n",
      "swap probability:  0.07\n",
      "Number of steps = 20000     Percentage = 47.47%     Time = 1165s     Learning rate = 0.0424\n",
      "swap probability:  0.05\n",
      "Number of steps = 20000     Percentage = 47.35%     Time = 1170s     Learning rate = 0.0424\n",
      "swap probability:  0.03\n",
      "Number of steps = 20000     Percentage = 47.47%     Time = 1171s     Learning rate = 0.0424\n"
     ]
    }
   ],
   "source": [
    "normal_run_prob = {'init': 0.1, 'epochs': 15000}\n",
    "\n",
    "memory_fine_values = [1., 0.1, 0.01, 0.001]\n",
    "support_values = [-0.06 - i  * 0.04 for i in range(6)]\n",
    "swap_probability_values = [0.2, 0.14, 0.1, 0.07, 0.05, 0.03]\n",
    "\n",
    "threshold = {'fixed': True, 'min': 0.5, 'max': 0.7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "\n",
    "for memory_fine_value in memory_fine_values:\n",
    "    print(' '*10, \"memory fine: \", memory_fine_value)\n",
    "    optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "    for support_value in support_values:\n",
    "        print(' '*5, \"support: \", support_value)\n",
    "        for swap_value in swap_probability_values:\n",
    "            print(\"swap probability: \", swap_value)\n",
    "            model = random_choice(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 1,\n",
    "                                 [128],\n",
    "                                 0.,\n",
    "                                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                                        normal_run_prob,\n",
    "                                 swap_value,\n",
    "                                  support_value,\n",
    "                                 train_text,\n",
    "                                 valid_text)\n",
    "            model.simple_run(100,\n",
    "                               'random_choice/variables/negative_support2/sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value),\n",
    "                               20000,\n",
    "                               4000,\n",
    "                               5000,        #learning has a chance to be stopped after every block of steps\n",
    "                               30,\n",
    "                               0.9,\n",
    "                               3,\n",
    "                               optional_feed_dict=optional_feed_dict,\n",
    "                             fixed_num_steps=True)\n",
    "            results_GL.extend(model._results)\n",
    "            model.destroy()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling random_choice/random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp-0.06--0.26_0.1.pickle.\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'random_choice'\n",
    "file_name = 'random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp-0.06--0.26_0.1.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling random_choice/random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp0.-0.9_0.1.pickle.\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'random_choice'\n",
    "file_name = 'random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp0.-0.9_0.1.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling random_choice/random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp-0.04-0.06_0.1.pickle.\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'random_choice'\n",
    "file_name = 'random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp-0.04-0.06_0.1.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'random_choice'\n",
    "pickle_file = 'random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp-0.04-0.06_0.1.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_name = 'random_choice'\n",
    "pickle_file = 'random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp0.-0.9_0.1.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL.extend(save['results_GL'])\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'random_choice'\n",
    "pickle_file = 'random_choice_ns_20000_hl_667_dc_0.9_ib0_ilr1._th0.5_mf1-0.001_swpr0.2-0.03_sp-0.06--0.26_0.1.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL.extend(save['results_GL'])\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 1231.363442, 'data': {'train': {'step': [-1], 'percentage': [47.14010416666666]}, 'validation': {'step': [-1], 'percentage': [44.6]}}, 'metadata': [64, 30, 1, [128], 666, 0.9, 20000, 100, 0.0, {'epochs': 10000, 'max': 0.7, 'fixed': True, 'min': 0.5}, {'epochs': 15000, 'init': 0.1}, 0.9, 0.2, 1.0, 0.0, 0.047036043419179864, 0.0, 1.0, 'random_choice']}\n"
     ]
    }
   ],
   "source": [
    "print(results_GL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "\n",
    "random_choice_plots = ComparePlots('random_choice')\n",
    "random_choice_plots.add_network(results_GL, model._indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    }
   ],
   "source": [
    "random_choice_plots.save_layouts('random_choice_1',\n",
    "                                 folder_list=['support_effect'],\n",
    "                                 varying_variables=['support', 'memory_fine', 'swap_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAEkCAYAAABgyjgcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXeYFFXWh98zBAFdQaKIBEEy4iACZlExYF5XXTEA5oSo\ngAoqAvqti0o0ooIEFdHVFRURAWEIKmAARCQpYFjAIUlGnJnz/XGroWkmVM90d3Uz532eerrr1g2/\nru6uOnXPufeKqmIYhmEYhpEI0oIWYBiGYRhG8cEMD8MwDMMwEoYZHoZhGIZhJAwzPAzDMAzDSBhm\neBiGYRiGkTDM8DAMwzAMI2GY4VGMEJEzRWSRiOwRkWkiUltEckTklKC1hRCRe0TkVxHJEpHHvLSr\nRORHEflLRF4LSNdzIvJs2P50EXklBvUeUI+IPCki60QkW0Q65lFulYg8XNR6ijsi0kdEVgStI9kQ\nkdO831jpoLUYBx9meBQvXgK+BuoAVwC/AEcCcwPUtBcRqQ4MBv4FHAUMEJE0YAQwDqgJ3Bujtl4V\nkWk+8zYEOgFPxKLtAtpqDfQEbsF9N2/7LPp3oFsM6vGj8TQRedczEHeKyHLvBn7ATUpELheROSKy\nQ0T+EJEMESkXkecMEflMRLZ521wRqe0dCxnH2d5r+PZcjD5Syk5mJCKHi8gIEdkoIttFZIKI1PFR\n7jYR+c77XlaJyKPhx1V1NrAMuC8+yo3iTMmgBRjRISIlVTWrkMXrA/9S1TVhaZkxkBUr6gECfKSq\nmQAicjRwGPCJqq4LSNc9wERVXZ+AthoA2ao6IZpCqvpHLOrxyanAj8AQ4FegBfAyUBW4O5RJRG4G\nBgEPA1O85OOA7LA8FwDvAv8GugK7gMbATi9LyDgO53TgHeCtGH6mVGUs7ru+HNgGPA1MEZGmqron\ntwIicicwALgN+Bz3nQwXkRKq2i8s6whgkIgMUNWceH4Io5ihqsViA6YDw3FPrb8Dm733AjwGrMPd\nhP8volwJoC+wEndRXATcFpEnB+iCeyrfDvwM/AM4HHgD2Ar8BFwRUa4B8DHugrEN+BCoF3a8E/AX\n0Bb4FtgN3IW7cJ8UUdeZQBZQM5fPfqanMTvstSNQ29s/xcsX2r/K07LD090por5DgaHAb16eb4C/\n+/gOzgVm424qvwGvARW9Y31y0dgpl7QzvPwtgU+985YJvAfUimivHTDT0/iH9xs4Jo+2OuahWYCN\nwDW5/J5eAR4F1np5RgPlwvK0ACZ6v7dtwDzg/Nzq8d6PjNSVz7lcBTwcbT3ANcB83G95FTAwXHMR\n/l/3A+vD9v8GbAFuyaeMeL+v/4uyrTeBRYXQWBrX6/eH9329CDwJLI/IV+A5whlYi3H/yd+Bd8KO\ndQDmeO2sByYA9cOOZwAv56Lvp/Dv1Mfnaex9v2eGpVUC9gDX5lPuS+DZiLQe3vd1SFhaWa+us4v6\n+7DNtvAtcAEJ+6DuwrwZ92R1LNDZ+9N+DPT30jp6aeeHlRsFLADOwd2YrwI2ATeG5ckB1gDXA3WB\n53E3u4+9OusCz+KMkiO8MmVwBsoUIB13k5oGLAdKenk6eTePuTjjoQ5QGfgEGBHx+cYAH+fx2Uvi\nnkZzgDu894d4nyebAw2PH3GGU12c2+Mv4NiIczkNONnTdIt3AT4rn/N/tndO7vLqbQl8BmR4x8vh\n3AXZQHNPY1ngRE/TRV5aSaAJ7kb+GK4XpynOlbAMKO3V1w5niA3EPdE1AG70XsvhDMLZQJXQ+chD\nd3NPU908fk8DvTrb4W5m/cLynOl9/41wv6/HvfMUeS5DBsPfcE/9e0K68jmf+RkeudaD+81vBK71\nvuvTcL/t0WH1dPbOd6282s5Dz+PA6rD9f3jn7QbgK5xhPx04LSxPS6+tu4AZuBv4l8Dl+bRTCWcQ\n3F2Ia8BgT8fF3nf2DO5muzwsj59z1A/3MHGn972mR3wXnXC/1zrA8cB49v9fX+O1G26knuN9X0eG\n7e99KMjj89wC7Mgl/QvgxXzKzQeejkjr4n1fJ0ekfwM8UZhrrm225bUFLiBhH9Rd9L6NSPseWBiR\ntiD0p8Q9HWcDDSLy9Abmh+3nAAPD9it7aUPC0ip4aRd6+zcTZoh4aVVxvQHXe/shw+OUiPb/jrvx\n/s3bL4+7qV9awDnIIexJiLx7PO4Ny5PmXWRv9fbbehr/FlH3COC/BZz/JyPSanntNff2z/Q+71F5\nafTSRgJjI+o6JPwc4Ho6PshHz6vANB+/m8s8TYfk8nkWRKS9CHxeQH0LgF4R9bwStt8J2ONDV56G\nR171eGUie+tO985veW//cuAHoHoU/63GuBvpnWFpD3r1/oIz1tNxxvefQGMvz9Veno3e/6E58Ih3\nvs/Jo60euP9Neb/6vHLlcAbLTRHpX7G/4ZHvOfLq2QncH0XbFb3yJ3v7pXC9dDeF5RkLvB+2f5L3\nPaTnU29vwoy9sPT/hteVy/EnvfZP8vab4gyjbOAfEXk/AN6M5lzbZltBW3ELLl0Ysb8O+C6XtKre\n+5a47uCvwwLftuF81vUiyu2tR1U34P7Ei8LS/sA90YTqbgL8oKqbw/Jk4p7am0bU/XXE/oc4Y+Ba\nb/8GXLdurPz5e8+TOt9uJlDNSzoRd5NfE3FOrsM9/eVFK+C+iDKLcYF99aPU1wr4e0RdGzxdobpa\nsi+uoCiUBVDVP3M5tiBifw37zhMiUllEXhSRJSKy2dPZBGdMJRQRqey1OyjivH2C+w6OBVDV8ara\nRFXX+qy3Ps7lNVZVXwo7VMKr90lV/Y+qLlDVrrjf9+1heQBeVdURqvqdqv4L5566J48mbwXeVtUt\nfj+7Rz2cq+XLiPTZYZ8lv3OUgztHTXG/szx/WyKSLiL/FZGVIrIV17OpXt2o6l+4ntRbvfyVcA8T\ne0clqeoc73uI/I35RfM51g94H8gQkb9wvZdjvGPZEXl34/0HDCNWFLfg0r8i9jWPtJBBlubtn4x7\nWorMl1/debWXFrEfiUSkZ2tEkJiqZovICNyF62Xc0+JrGrsAsMigtMhz8gfOAJECyoWTBjwFvJ7L\nsWiDRtO8ev6di4aNYe/zu/j6ZT2AiBwRbiR65HeewMV8HI17Sl+N+w29jbsBJpqQrq64GINIfou2\nQhFpBkzGPV3fHXE4FMD8Q0T6D+wzvPLKsxg4P5f2zsHd/K+PVivudxL534rEzzk63nufaz0iUhZn\niM3CuW1+9w79wP7f+8tAN+8ctsMZ95MK+AyRrMW50iKpxoEPWXvxjOjbvSDTI3H/v4u8wz9FZK+I\ni28zjJhR3AyPaPnGe62tqhNjXPdi3J+/oqpuAhCRajjf89M+yr8K9BKR23ExDH+Psb68+BrnNiqr\nqpE3jILKNVXVWFzEvsa5Z1blk+cb3M3rhTyO72HfE3d+zPdemxL2dOyT04EHVPVjABE5FBffsijf\nUnFAVTNF5FegkaoWeS4UEWmF6wkYo6rdcskyC3ejb4Rze4VoyL6b+tc4Y6xRRNmGOEMtktuB71T1\nq0JI/hH3nZ8KLA1L3zuHjZ9zJCI/4NxF5+P+w5E0xrlaH1HVZV6ZU4gwkFX1J284923AWbiYrWgN\n5c+BMiJypqrO8NqqhHsoeL6gwt6Dyhqv3HXAj6oa+dtshhtBZBgxo7i5WqJCVX/CxRO8KiLXi0g9\nEWkuIjeKyINFrH4szj3wtoi0EJGWuFExv+Ljj66qv+KerIYCU1V1dRH1+EJVpwFTgf+KyGUicoyI\nnCAiXbzhk3nxGHCZiAwUkeNFpK6IXCAiw0XkkLB8kT0YufEk0FhE3hCRViJSR0TOEpEhYXMYPAG0\nF5HBInKciDQQkU6eawCcL7+RiDQRkUq5zUHhfd5NuNEoZ/rQFcky4DoRaSYi6bjvPMj/3CNAVxF5\nWESaeufkchEZFsrg7S8RN6dKrojIGbjfwHjgKRGpFtpCeTwD8x2gj4hcKCLHisi/cUbFMC/PDuA5\n4G4R6eD9v+7DBX8OiWizKi7eZhiFQFV3emX/T0Qu8T77Uxxo9OR7jjzNA4G+InKXiNT3fs89vfI/\n4wyTrt5v/Bzvs+TWG/kKzvBohBvhFf55T/K+h/R8PtMSnFvqZRE5XURa4H5jq3GjvEJ1ven1kIb2\n64vIDd7riSLyMi62544IDSEjKtqeGMPIl+JkeBS22/1WXDT8w7gnnKm4kQrhXZK51Z1vmqruxg0v\n/RMX0T8dF7fRXv3P0/EKLlDN7wyafnT6yXMpLoBtELAEF1tyIQd20+6rQDUDN7LlONwT8ELcBXwr\n+7ukCmxfVZfinlQPxV0UF+O6rsvg3ECo6hRPU2vc0Ma5uO8t1NYIXGDhF7hu7mvy0o4bgnlDfpry\noDPuPzYXd74+8dqMtp7c8PO97Z9B9Q1cQOdFnqZ5OIMw3M1SHtfrViqfqm7Eza1yI+6JeQ2u239N\nRL7OuBvga7geqFNwQzOXheXphft/PYWLmbkWN6plRi5t/okbSnsAItJXRApyNfbEGUtjcJ+/PBE9\nA37Okar2xhko9+B6rybhRqWhqhtxrqB2uOD1p4Hu5G54jMcF5X6iqpGurkPZNwIrP67F9cSNx/Uy\n7cGNygt3A9bCufxClMBNxPctLr6jLm5E2vSIuq8HJnkPOYYRMyT63j0jWRCRu3CR7TWjMFaMKBGR\nkjhDqZeqfhi0HuNARGQ0bthw+6C1+MVzi/wKXK3xmeit0IjIYbgHifaq+m3QeoyDC4vxSEG8WIGa\nwAPA82Z0xBdVzRKRTkQ/+sZIACIiuN60tgFL8YVnyFbGTUz4W7IZHR51gYfM6DDigfV4pCAiMhI3\nO+Jk4Ko8hnoahpGEiMiZONfqStycPXMClmQYCcUMD8MwDMMwEkaxcrWIiFlZhmEYhUBV/Yw4M4wC\nKU6jWoDCTRHfp0+fQk8NW1DZvI7nll5YHfEs51d/PM+h6U9N/cn82zf9+6cZRixJqOEhIhkikhOx\nfRd2/G4R+VFEdovIUhHpWEB9R4jIKBH5n1fmZ28uh/yGAkZN27Zt41Y2r+O5pa9evTouGopSzq/+\nwmr3q6Ow5Ux/bHQUplwy//b9lC1O+g0jphTWmi7MhguoysbN3zDI23p4x67BjXVfhxv3v9HLe24+\n9Y3yyvyMm2shtEZKnzzyayrTqVOnoCUUmlTWrmr6g8b0B4t37Uzo/cK2g3cLJMZDVbvnktwTNwnS\nHao6XkRuAobjJhjKa0Gmul6Zp1T1RRHZjpusp07sVQdP586dg5ZQaFJZO5j+oDH9hnHwkNBRLSIy\nHTgDN1sfuJnzeuLWw9iNc/3UUdVfRaQ5bibDP1S1Yh71/QM3RfDvuKmDr8L1eFyguYw/FxFN5Oc1\nDMM4GBAR1IJLjRiR6ODSrbjptcfh3CNn46Ybrsy+Bbu2e687vNfyea2jgZvO+AugBm5q8wo4d05+\ni4elLBkZGUFLKDSprB1Mf9CYfsM4eEioq0VVLwu992bvW4FbR+BcXE9FGm4NiM3eK8AWjVgWPox3\ncSsx9gSexcWO3IVzv+S69kbnzp2pU6cOABUqVCA9PX1vMFXo4pCs+wsWLEgqPbZv+7Z/cO5nZGQw\natQogL3XS8OIFQlztYhIWaCCqq719kvjVu+sBfwTeBS3gNjVqvqeiNyKW/grQ1XP9gyVegC6b7np\nbbhFlM5W1Rki0hkXmLpIVY/PRYO5WgzDMKLEXC1GLElkj0dVYJmITMO5WU4GauNWtZyGWw3zTeBF\nEbkYtwS2Av298jVwK6GqiByhqluBz3G9JSNEZCpuaWfFrdJoGIZhGEaSkcgYj43AaNxCWx1xhsh/\ngXaquklV3wK6Attw65BkAjep6uSwOpT9l//uBLyOWw69Ey5A9Xngwfh+lGAIdYWmIqmsHUx/0Jh+\nwzh4SFiPh6puB24vIM/zOMMht2M/sy8ANZT2O9A5RhINwzAMw4gzxWqROIvxMAzDiB6L8TBiSbFb\nq8UwDMMwjOAwwyOFSGU/cSprB9MfNKbfMA4ezPAwDMMwDCNhWIyHYRiGkS8W42HEEuvxMAzDMAwj\nYZjhkUKksp84lbWD6Q8a028YBw9meBiGYRiGkTAsxsMwDMPIF4vxMGKJ9XgYhmEYhpEwzPBIIVLZ\nT5zK2sH0B43pN4yDBzM8DMMwDMNIGBbjYRiGkSI88sgjHHvssXTu3BmRxIVcWIyHEUvM8DAMw0gB\nVJVq1arx119/MX36dNLT0xPWthkeRiwxV0sKkcp+4lTWDqY/aEw/rFy5kvXr19OgQQOOP/74oosy\njIAww8MwDCMFmDlzJu3atWPp0qVYz62RyiTU1SIiGcAZEcnfq2pz7/jdwP3A0cBq4ElVHZNPfTl5\nHBqlqjflkt9cLYZhpCQ33ngjrVu3ZuDAgXz44Yc0adIkYW2bq8WIJSUT3J562xAg9CNeAyAi1wDP\nAZnAWOAyYKSIrFXVKXnUNyRi/0bgcGBFjHUbhmEEysyZM+nRowezZ89m7ty5CTU8DCOWBOJqUdXu\nqtrN2wZ4yT1xRskdXm/FAzjjpFc+9YTq6Aa8BpQH/gReje8nCIZU9nOnsnYw/UFT3PX/9ttvbNmy\nhcaNG9OmTRvmzp0bG2GGEQCBGB4issnbporIiSJSAmjqHf7Ge/3ae/Ubun2/9/qGqm6IlVbDMIyg\nmTVrFqeffjppaWlmeBgpT6JjPD7w3v4POBk4HtiEMzrW4no8KqvqZhGph3OZKFBWVffkU29l4Bfg\nEKC5qi7OI5/FeBiGkXLceeedNGjQgPvvv58///yTihUrsn79esqVK5eQ9i3Gw4glCY3xUNXLQu9F\npCTOsKgFnAtk43pgDgM2e68AW/IzOjzuBMoAU/MyOkJ07tyZOnXqAFChQgXS09Np27YtsK871PZt\n3/ZtP5n2Z86cSXp6OhkZGbRt25amTZsyYsQIjjvuuLi0l5GRwahRowD2Xi8NI2aoakI2oCxQPWy/\nNLAKZ3BcCSzw3v/DO34rkANM8/ZLAg2BhhH1lsT1oGQD7QvQoKnM9OnTg5ZQaFJZu6rpD5rirD8z\nM1MPP/xw/euvv/amdenSRQcMGBADZf7wrp0Ju1/YdnBviezxqAosE5FpwM84V0ttnItlGlAKeBN4\nUUQuxo1qUaC/V74GsARQETlCVbd66R2A6sAyVf0kUR/GMAwjEcyePZtTTz2VkiX3Xa7btGnDhx9+\nGKAqwyg8CYvxEJHDgIHA2cBRwBbgC6C3qi7x8nQB7mPfPB7/VtXR3rHawEqcMVIxZHiIyFfACUAX\nVX2pAA2aqM9rGIYRC+6//36qVatGz54996atWLGCdu3a8fPPPydEg8V4GLHE1moxDMNIYlq2bMlz\nzz3HKaecsjdNValUqRI//PADRx55ZNw1mOFhxBKbMj2FCAV/pSKprB1Mf9AUV/1btmxh+fLlnHji\nifuliwitW7dm3rx5MVBnGInFDA/DMIwk5YsvvqBVq1aULl36gGM2n4eRqpirxTAMI0np1asXhxxy\nCH379j3g2MSJExk0aBBTp06Nuw5ztRixxHo8DMMwkpSZM2dyxhmR62o6WrduzVdffUVOTl5rZRpG\ncmKGRwqRyn7uVNYOpj9oiqP+nTt3snDhQk466aRcj1euXJkqVaqwdOnSIqozjMRihodhGEYSMmfO\nHJo3b57vtOht2rSxAFMj5bAYD8MwjCSkb9++7N69m/79++eZZ+jQoSxdupSXXsp3CqMiYzEeRiyx\nHg/DMIwkJL/4jhA2ssVIRczwSCFS2c+dytrB9AdNcdO/Z88e5s2bx6mnnppvvvT0dJYtW8bOnTuL\noM4wEosZHoZhGEnG119/TYMGDShfvny++cqUKUPTpk359ttvE6TMMIqOrxgPETlSVdf5TU9WLMbD\nMIxUoH///qxbt44hQ4YUmPeee+7hmGOOoVu3bnHTYzEeRizx2+OxPI/0H2IlxDAMw3D4ie8I0bp1\na4vzMFIKv4bHAZauiBwO2Mw1CSSV/dyprB1Mf9AUJ/3Z2dl8/vnnnH766b7yW4CpkWqUzO+giPyK\nW4a+rIj8EnG4EvBWvIQZhmEURxYuXEiNGjWoUqWKr/z169dn69at/P7771SrVi3O6gyj6OQb4yEi\nZ+J6OyYC7cMOKfC7qi6Lr7zYYjEehmEkO0OGDGHp0qUMGzbMd5kLLriAu+66i0svvTQumizGw4gl\n+fZ4qOoMABGprKo2XsswDCPOzJw5kyuvvDKqMiF3S7wMD8OIJX5jPN4Qkf0cjiJyuoi8GwdNRh6k\nsp87lbWD6Q+a4qJfVZk5c6bv+I4QNnW6kUr4NTzOBL6ISPsSOCuaxkQkQ0RyIrbvwo7fLSI/ishu\nEVkqIh191NlMRD4WkS0iskNEFonIKdHoMgzDSAaWLFnC4YcfTs2aNaMq16pVK1up1kgZ/M7j8T+g\nsapuDUurACxV1SN9NyYyHTgDGMK+kTJrVHWAiFwDjAUycTEllwEVgAtUdUoe9dUHvgEOBaYAK4GG\nwMuq+k4u+S3GwzCMpGXYsGHMmTOHUaNGRV22Xr16TJgwgcaNG8dcl8V4GLEk3xiPMD4FXhaR21V1\nqzeU9nlgUmEaVdXuuST3xAWt3qGq40XkJmA40AtnVOTGYzijo5+qPl4YLYZhGMnCzJkzadeuXaHK\nhuI84mF4GEYs8etq6Q4cDmwWkUxgE1AeuK8wjYrIJm+bKiInikgJoKl3+Bvv9WvvNT2fqs72XluL\nyAYRWSsiz4pI2cLoSnZS2c+dytrB9AdNcdAfiu/wO3FYJDafh5Eq+OrxUNXNwEUiciRQE/i1kFOl\nbwUmAP8DTsYZDpNwRkcJXI/Hdi/vDu+1vIiUVtU9udRX2Xs9GXgXN+S3C5AF5Dp/cOfOnalTpw4A\nFSpUID09nbZt2wL7Lg7Jur9gwYKk0mP7tm/7sdt/66232LVrF/Xq1StU+VKlSvHZZ58Roih6MjIy\n9rp7QtdLw4gVvmI8AESkEnAhUF1VnxaRo4A0Vf2tUA2LlARWALWATsBIXA9MHVX9VUSOB+YDf6hq\nxTzq+A2oDjygqoNE5ErgHWClqh6bS36L8TAMIykZNWoUkyZNYty4cYUqv3v3bipVqsSGDRsoWza2\nnb4W42HEEl+uFm8isWXAdUBvL7k+8JLfhkSkrIhUz6Pt3cBi733riNcFXvmSItJQRBqGlQuNiAn9\nIUJ1bscwDCOFKIqbBdxKtU2aNLGVao2kx2+MxxDgn6p6Ac6NATCXfcaBH6oCq0Rkooi8BMwDagPr\ngGnAUzgD4kURGentK9DfK18DWAL84AW3AjztleklIq8AA7wyo6LQlTKEukJTkVTWDqY/aIqD/qIa\nHmBxHkZq4NfwqKOqIedhyFexB/+jYgA2AqNxPSUdcYbIf4F2qrpJVd8CugLbgA64YbU3qerksDo0\nrH1UNQO43st7PbAT6KGqBa8lbRiGkST873//Y/PmzTRp0qRI9ZjhYaQCfufx+Bx4XFU/FZFNqlpR\nRM4DHlbVtvEWGSssxsMwjGRk3LhxjBs3jvHjxxepnuXLl3PeeeexevXq2AjzsBgPI5b47bHoDkwQ\nkY9xK9W+DFyCm+TLMAzDKAKxcLMAHHvssWzZssVWqjWSGl+uFlWdAzTHBYC+BqwCWqvqV3HUZkSQ\nyn7uVNYOpj9oDnb9sTI80tLSaN26ta3bYiQ1BRoeIlJCRDKAjar6tKrerar9CzuM1jAMw9jHhg0b\n+OWXX0hPz2+uRP9YnIeR7PiN8fgZaKSqu+IvKX5YjIdhGMnG+PHjGTZsGJMmFWoFigP4+OOPGTJk\nCFOm5LXSRPRYjIcRS/yOaukHvCQitb0ekLTQFk9xhmEYBzuxcrOEaN26ta1UayQ1fg2H4bghsCtx\nw2j/ws3n8VecdBm5kMp+7lTWDqY/aA5m/bE2PKpUqUKlSpVYvnx5zOo0jFjid1TLMXFVYRiGUQzZ\nunUrS5cupVWrVjGtt3Xr1sydO5dGjRrFtF7DiAUFxnh4K8d+Bpyvqn8mRFWcsBgPwzCSiUmTJvHU\nU08xffr0mNY7ZMgQli9fzosvvhiT+izGw4glBbpaVDUb1+Nh8RyGYRgxJNZulhA2ssVIZiy4NIVI\nZT93KmsH0x80B6v+eBkeLVq0YOnSpezaldIDEY2DFAsuNQzDCIBdu3axYMECTjrppJjXbSvVGsmM\n33k8aud1TFV/jqmiOGIxHoZhJAsZGRn06tWLL7/8Mi7133333Rx77LHcf//9Ra7LYjyMWOJrVEvI\nuPBcK9WA31XVBokbhmEUkni5WUK0adOGiRMnxq1+wygsvlwtInK4iIwBdgP/A3aJyGgRKR9XdcZ+\npLKfO5W1g+kPmoNRfyIMDwswNZIRvzEezwKHAs2AssBxQDkv3TAMw4iCPXv2MHfuXE499dS4tVG/\nfn3++OMPMjMz49aGYRQGvzEe64C6qrozLO0w4CdVTZm1ly3GwzCMZGDOnDnceeedzJ8/P67tnH/+\n+XTp0oVLLrmkSPVYjIcRS/z2eOwGqkSkVQaimlBMRDJEJCdi+y7s+N0i8qOI7BaRpSLSsYD6+uRS\nX7aIVIxGl2EYRiKJt5slRJs2bZg3b17c2zGMaIhmOO0UEblDRNqLyB3Ap8ArUban3jYYGOJtYwBE\n5BrgOeAwYCzO0BkpIuf6qPM/YfUNAQ7Kweup7OdOZe1g+oPmYNOfKMMjNHW6YSQTftdq+RewBrgW\nOMp7/zTwWmEaVdXuuST3xBkRd6jqeBG5CWfw9AIKWt/5BVWdWRgthmEYiSQ7O5vZs2fz2muFunxG\nRZs2bfauVJuWZvM9GsmBrxiPmDUmMh04A9jiJX2LMzjm49w5aUAdVf1VRJoDC4A/VDVX14mI9AH6\nePWVBlYAT6nqW3nktxgPwzACZf78+Vx77bUsWbIkIe3VrVuXiRMnFmnBOIvxMGKJ3+G0z4rIKRFp\np4jIkCjb2wpMAMYBPwNnA5Nw8SIlvDzbvdcd3mt5ESmdR31ZwAyvvplAc+ANH+4ZwzCMQEiUmyWE\nDas1kg2/rpYOQI+ItG+A8cB9fhtT1ctC70WkJK6HohZwLpCNM4QOAzZ7rwBbVHVPHvX9C+cGCtU5\nFvgncAX+cLpbAAAgAElEQVR5uGc6d+5MnTp1AKhQoQLp6em0bdsW2OeHTdb9IUOGpJTe8P1wH3cy\n6DH9yaWvOOmfOXMmV1xxRcLaDxketWvXjkrvqFGjAPZeLw0jZqhqgRuQCZSJSCsHbPBT3stfFqge\ntl8aWIUzOK7EuVWygX94x28FcoBp3n5JoCHQMKyOehFtjPXKPJ+HBk1lpk+fHrSEQpPK2lVNf9Ac\nLPpzcnK0cuXK+ssvvySs7c8//1xbtmxZpDq8a6eva71tthW0+Z3H4z3PSHhQVXO8qdP7A/VV9e9+\nDBxvvZdlwDScm+VknGtkLW5CsvOBN4H1wETgMqA80F5VJ3vlV+ECUI9Q1a0ishJYByzC9Zyc7xkv\nZ6vqrFw0qJ/PaxiGEQ+WLFnChRdeyKpVqxLW5q5du6hcuTIbNmygbNmyharDYjyMWOI3zPleoB2w\nVkTm4Ua1nAvcE0VbG4HRQH3cSrdVgf8C7VR1k7qA0K7ANpxrJxO4SVUnh9URGo4b4hVcT8o1QBtg\nNnBJbkaHYRiJQVX59ttvMSP/QBId3wFQtmxZGjduHPfJygzDL74MD1X9DTgB1wvxDHA50NJL94Wq\nblfV21W1vqoeqqpHqeqVqrokLM/zqnqsqpZR1UaqOjrs2M+qWkJVS6rqVi+tv6q2UNXyqlpRVc9Q\n1Ul+NaUa4X7iVCOVtYPpj4apU6fSpk0bZs2Knf1/sJz/IAwPsABTI7nwPbBbVXNUdY6q/sd7tdVp\nDcM4gF9++YWsrCw+//zzoKUkFarKjBkzzPAwij0JnccjaCzGwzDiz1VXXUXNmjUZN24cq1at4pBD\nDglaUlKwatUqTjnlFNasWYNIYsMlli5dyoUXXsjKlSsLVd5iPIxYYlPZGYYRM7Kyspg6dSoPPvgg\nxx13HG+9letcfsWSkJsl0UYHQIMGDdi8eTPr169PeNuGEUmehoeIXBr2vlRi5Bj5kcp+7lTWDqbf\nL19++SXHHHMMRx55JD169GDgwIExCTI9GM5/UPEdAGlpabRq1crcLUZSkF+Pxxth7zfGW4hhGKnP\nJ598Qvv27QFo164daWlpTJ48uYBSxYMgDQ+wOA8jecgzxkNElgPPAj/gpjm/CDigj1BVp8VTYCyx\nGA/DiC8tWrTgueee47TTTgPg9ddfZ8yYMUyZUtA6jwc3a9asoVmzZmzYsCGwxdomTJjAs88+WyhD\n0GI8jFiSn+FxCvA4UBs4Bvg1l2yqqnXjJy+2mOFhGPFj7dq1NG3alMzMTEqWdKsx7Nmzh7p16zJh\nwgTS09MDVhgcb7/9NmPHjuWDDz4ITENmZiYNGzZk48aNURs/ZngYsSTPX5+qfqGq7VS1PrBaVY/J\nZUsZo+NgIJX93KmsHUy/HyZNmkS7du32Gh0ApUuXpmvXrgwcOLBIdaf6+X/rrbcCdbMAVK1alQoV\nKrBixYpAdRiG3wnEjgUQkVoicrKI1IyvLMMwUo3w+I5wbrvtNj7++GN+/TW3TtPiwcKFCwM3PMDi\nPIzkwO9aLUcCb+PWV9kIVALmANeo6pq4Kowh5moxjPiQlZVFlSpV+OGHH6hevfoBx7t160aJEiV4\n5plnAlAXLBs3buSYY45h06ZN+/UGBcHgwYP58ccfeeGFF6IqZ64WI5b4dfQNAxbiFmerDhwBzPfS\nDcMo5nz55ZfUqVMnV6MD4N577+W1115jy5YtCVYWPLNnz+bkk08O3OgA6/EwkgO/hsdpQHdV3QHg\nvT4InBIvYcaBpLKfO5W1g+kviLzcLCFq167N+eefz/DhwwtVfyqf/5kzZ3L00UcHLQNwo46WLFnC\nrl27gpZiFGP8Gh6bgSYRaQ2BP2IrxzCMVKQgwwOge/fuDB06lL/++itBqpKDmTNncvzxxwctA3Ar\n1TZq1IgFCxYELcUoxviN8bgVeBIYAfyMG2J7I9BbVV+Jq8IYYjEehhF71q5dS5MmTVi/fn2B7oSz\nzz6bW265hWuvvTZB6oJl27ZtVK9enQ0bNlCmTJmg5QBw11130aBBA+677z7fZSzGw4glfke1vAr8\nE6gMXOK9dkglo8MwInn66ae57rrrYjKld3Emt2G0edG9e3cGDBhQbM75F198QcuWLZPG6ACL8zCC\nx/csMqo6TVVvUdULvdeUmbH0YCGV/dzJpj0zM5N//etfjBs3jo8++qjA/MmmP1riqd+PmyVE+/bt\n2b17N9OnT4+qjVQ9/6Fp0pNJvxkeRtAkdO5eEckQkZyI7buw43eLyI8isltElopIR5/1Hisi2736\nvo3fJzAOFp588kluuOEGbr/9dt55552g5aQsodVoL7jgAl/509LS9vZ6FAeCXp8lN2K9Uq2IdPKu\nvQl9GBWRPl67rxWhjlFeHY/lcby2dzxbRA730laH3b9mFqHt1V69vn4gInJOxL3T1/0xKVHVhG3A\ndCAbGAgM8rYe3rFrgBxgHfAabr6QbODcAupMw80p8qeX/9t88qphrF69WitWrKjr1q3TLVu2aJUq\nVXTx4sVBy0pJZs2apenp6VGV2bVrlx555JG6aNGiOKlKDnbu3KmHHnqobt++PWgpB3DuuefqRx99\n5Du/d+0EWO1dp8/QfdfVTl7aNC3kvaEwG9DHu+a/VoQ6Rnp1PJbH8dreZ8sGDvfSVnn7w4EuRWj7\nUe9eWNdn/vrePfN7r/2OcTinzb379E5gA/AKcFg++UPnJ3Lrll87gaxWpKrdVbWbt4UefXoCCtyh\nqjcBD+AWpetVQHW9gWa4L9CCn4wC6du3L3fddRfVqlXj8MMP54EHHqBPnz5By0pJonGzhChTpgxd\nunRh0KBBcVKVHMybN49mzZpx6KGHBi3lAFq3bs28efMKU1S9rUiIR1HrIdhr/uOq+nxhC6vq/3n3\nwpU+869Q1W7AV4VtMz9E5DBgKnAGbmHYVcAtwMs+iv8ADAaGeNs3+WWOyvAQkTQRyX2GoOjq2eRt\nU0XkRBEpATT1DocEf+295rmylIi0Ah4BugPLiqor2UkmP3G0JIv2xYsX8/HHH9OjR4+9aXfffTef\nf/458+fPz7NcsugvLPHSXxjDA+COO+5g/PjxrF271lf+VDz/4W6WZNNfmDgPEVmFe8IFyMiluz9N\nRP7tXdt/E5Frw8qG8vcXkbnAHqCmiJT10lZ47vJvROSysHLnisjX3rE/vOOXR0grKyLDRWSbV885\nYeUre8d+FpEtIvKliJyfz2csJSIveZ9hOXCez3MTcjctEJGBnpbFIpIuIk942n8SkXPDyoRcNmdE\nnKMnRWSGiOwQkdnRLlEiIvVEZIiIDM5jy2uNtZtxA0c+UtWrgbbAbuBqEalTQLPzIjoUZuSX2Zfh\nISIVRGSsJ+JHL+1SEfk/P+XD2IqzpMbhhuWeDUzCfdgSXp7t3usO77W8iJTORVNZ4HVgsqr6scgM\ng0cffZSHHnqI8uXL700rV64cDz/8ML179w5QWeqxbt06Vq1axcknnxx12UqVKnHdddfx3HPPxUFZ\ncpCM8R0h2rRpw7x588jJyYmm2Ahgm/f+XdwT7g9hx08DzgLmAUcBw7ynaNjXU9ID505/E+cefw03\nGeUfXloN4L2wuIeRuO7/d70tG9fDHc5VQC1gEVDP04nXo/IRcBOwHhgPtAQmiEheP9pHgdu9dmYC\nfQs6KRE0A1rjzktjnNviCuBL3CrvI8LyRvYehfYfAH7xNJ8MRHufPRq4B+iax5bXbHYtvPa/gb0T\nhS7F2QnNC2jzKhHZ5Rl4z4rI3/LN7dPvMw54CagObPbSqgAriuBLKsk+X9n1wF/e+5re8eNxvqJN\neZQ/wzv+Be7HNd/b/wNnseVWRjt16qR9+vTRPn366ODBg3X69OkaYvr06bZ/EO+/8MILWrlyZd25\nc+cBx3fv3q1Vq1bV559/Pmn0Jvv+Qw89pGeccUahy7/xxht6+OGH67Zt25Li88Ryf8+ePVqmTBn9\n8MMPk0JPbvvVqlXT0aNH53p8+vTp2qlTp73XS/bFeISu2bnFeKwHSnnX9tD1/AQvTyi+b2RYucpe\nub+AZ3GGzHQvbayXZx3ugfUKoAHOtRKaf6qPl/c7b78O++IxKgKtvP0tQBkvzyAv7Q3NJcYD92Cd\nDVzr7V9M3jEetXI5B1uA0sCZYeUaAoeF7VfK7VyGnaNnvf3O4Z8vrK2Q5pjGeACfePXeH5Y2y0u7\nLY8ytYEVwGicUbUp/Pzm2ZZPQeuBUt77TWHpW6L4UGWB6mH7pcNO/JXAAu/9P7zjtxIWsOT9mBsC\nDb39M738kVsOkJWHBjWKJzk5Odq2bVt99dVX88zz6quv6jnnnJNAVanN1VdfrSNGjChSHf/4xz90\n6NChMVKUPMyZM0ePP/74oGXkyz//+c/9DI/8wJ/hMTMs7Y88bqq3huU5MexmHLnN9fJcinOjh67t\n64GrdX/DY4y3XyGsvlq4npAc4PuwNm/z0mZp7obHTm//RG+/AdEZHgt1/wfnbPYZSjnh5SLPZdg5\nusnbv9wrs1L3v4/la3jgen2G4Ay53LZcg1mBUV69vcPS5ntpl+ZWJpc6zvM0b88vn98Yjy0463Qv\nIlIL8OegdVQFVonIRBF5CdcdVxtn0U4DnsJZsy+KyEhvX4H+XvkawBLgBxE5XFVnqGqJ0IbrThNg\ngaoGvxpTHEg2P3E0BK19ypQprFmzhs6dO+eZp1OnTqxevTrXOSaC1l9UYq0/KyuLKVOm+B5Gmxc9\nevRgyJAhZGVl5Zsv1c5/pJslGfW3bt26MPN5ZHuvud07wr/EvAJQ/wx7v9p73QNUCbuWH4Lr4QCY\npKoNcfefK3Ero/8rj3Yj2wzVX1NEQjO4NYo4Fsn/vNeGEa9+yY5MCFlsUZDX5/FLYV0tC3D30NYA\nnrukkadjkZdWU0QaikgFb7+WiITfb0PBvvn68PwaHsNxfrezcAFEJ+O6VqJZnXajV6Y+0BFniPwX\naKeqm1T1LdxJ2QZ0ADJxlt/ksDoKiqiOScS1cXCRk5NDr169+Ne//pXv7JqlSpWib9++9O7dm+iv\nFcWLuXPnUqtWLY466qgi1XPSSSdx1FFH8f7778dIWXKQzPEdIUJxHlHyq/f6hBeoWKOw7avqBuAd\nXO/3PC+o8z9eGzd72eaLyMe4JTtu8tI2+2zia2Auzs0xW0RGA11wN8UXw/KFj4wZ6+0PFZHh+BvR\nkVREPpTnsuU198hw3H36QhF5F8jAfTfvqOoqL8/ruA6AUFDxjbgOhTfFzacyDncPfqsgkX66TwS4\nDxcws8Nr+D68LqRU2TBXS7Hk7bff1pYtW2pOTk6BebOysrRx48b6ySefJEBZ6vLII49oz549Y1LX\n+++/r61atfL1/aQCWVlZWqFCBV23bl3QUvJl586dWq5cOd21a1eBedn34H4mzvWxN4YD52bIBj7T\nfdfazbgn90g3wn7uAaAcrgdjGc7N8RsuiLS1d3wILoZgh1fnVOB43edqyQZGePvl8Vzt7HNnVPZu\nqD/jeu6/BNqHtR/paimNi2fcBPwE3O0dz+JAV0vtsHpC5+Ab3d/VkhWWJ1LbqvzOEXCZt/9TxDkb\nldu5jMXm6Z7mne8D5vEI03iPt386MBHnudjpfY99gUPybSfWwpN5M8Oj+LFnzx6tX7++Tp482XeZ\n//znP74NleLKCSecoDNmzIhJXVlZWVq/fn2dOXNmTOoLmgULFmjDhg2DluGLE044Qb/44osC84UM\nD9v2Mzz2i2tJQLvnsG+CrrgYHona/A6nPTuP7VQRqe2nDqPoJKOf2C9BaR85ciQ1a9akXbt2vstc\nccUVZGdn88EHH+xNS+VzD7HVv27dOlauXFmoYbS5UaJECbp165bvNOqpdP5zc7Mkq35bt6VQjACG\n4npjErnewi/sm6BrKPsPZU4p/AZhjsCNywbnA6rkvc8EjhS33so1qroixvoMo9Ds2rWLxx9/nP/+\n979IFJMkpqWl8cQTT9CrVy8uueQSSpQoUXChYsSkSZM455xzKFWqVMzq7NixI3369GHZsmU0bBht\nPF9yMXPmTC677LKCMyYBrVu35tNPPw1aRkqhqtHOqxGrdlcA3YJoO9aEhvnkn0nkUZz/7DFV3eVN\n3tUP5zMbgpuuvJ6qnptPNYEjIurn8xoHB8888wxz5szhvffei7qsqnLKKafQtWtXOnToEAd1qcs/\n//lPzjvvPG6++eaCM0dB3759Wbt2LS+/nHLxfHtRVY488ki++uoratWqFbScAlmyZAkXX3wxP/30\nU775RARVtSUpjJjg1/BYj5uDIyssrRSwRlWriMihwG+qekT8pBYdMzyKD3/88QcNGjRgxowZNG7c\nuFB1fPbZZ9x555388MMP+Y6GKU5kZWVRtWpVFi1aRI0ahR7QkCuZmZk0bNiQZcuWUbVq1ZjWnSiW\nLVvG+eefz+rVq4OW4oucnBwqVqzIihUrqFKlSp75zPAwYonf4bQ7cLPAhdMSF8UKBYzZNWJDsvqJ\n/ZBo7c888wwXX3xxoY0OgLPPPpsaNWrw+uuvp/S5h9id/7lz51KzZs2YGx0AVatW5eqrr+aFF144\n4FiqnP+8htEmq/60tDRatWpV2AXjDKNQ+DU8HgMme2N1+4vIG8CnuJVhwUXbvhsPgYYRLevWrWPY\nsGH07du3SPWICE888QT9+vXjr7/+io24FKewi8L5pVu3brz00kvs3Lmz4MxJyIwZM5J+/o5ILMDU\nSDS+XC0AItIE+AcuyHQt8K6qplRUrblaigddunShdOnSMVt2vX379lx66aXceeedMakvlWnZsiWD\nBw+O6831sssuo3379txxxx1xayMeqCq1atXis88+o0GDBkHL8c1HH33E888/n2+QqblajFji2/A4\nGDDD4+Bn5cqVtG7dmiVLluTrs46Gr7/+mssvv5wVK1ZQtmzZmNSZiqxbt47GjRuTmZkZ0xEtkcya\nNYubb76ZJUuWpNSIotWrV3PSSSexdu3aqEZRBc3vv/9Oo0aN2LhxI2lpuXeCm+FhxBK/rhZE5FIR\nGSgio0VkTGiLpzhjf5LVT+yHRGl/7LHHuOeee2JmdACceOKJ1KlTh2HDolkhILmIxfn/9NNPYz6M\nNjdOO+00jjjiCD766KO9aanw2w/Fd+RmdCSz/mrVqlG+fHl+/PHHoKUYxQS/E4j1wc1Zn4Zb8W8j\ncD5uBULDSAq+++47pkyZQrdusR/qftNNN/HUU0+xffv2mNedKsQ7viOEiNCjR498JxRLRlJhfZa8\nsDgPI5H4HU77M3CRqn4vIn+oagURaQ08qqqXxl1ljIiFq2XJkiVkZmbm+WRjBMcll1xCu3btuPfe\ne+NS/7XXXkuzZs14+OGH41J/MpOVlUW1atX47rvv4jKiJbf2GjRowJtvvhmzGVLjTYMGDXj33Xdp\n3rx50FKiZtCgQaxcuZLnn38+1+PmajFiiV9XSwVV/d57v0dESqnqPNyCQcWK/v37c+GFF7JgwYKg\npRhhzJ49m0WLFsU1ILFv374MHjyYP/4ofh198+bN4+ijj06I0QFQsmRJ7r//fgYOHJiQ9orK2rVr\n2bBhA82aNQtaSqGwHg8jkfg1PH4Skabe+++BO0XkBvwvT3zQ8PLLL1OrVi1mzZqV8LaT2U9cEPHU\nrqr07NmTfv36ccghh8SljYyMDBo0aMAll1wSs9EyiaSo5z9RbpZwbrzxRjIyMvjpp5+S/rc/a9Ys\nTjvttDyDM5Ndf4sWLVi8eDG7d+8OWopRDPBreDzKvvVZegJdgWeA7vEQlcyUKVOGjz76iCeeeIJv\nv/02aDkGMHHiRDZv3sz1118f97Yee+wxXnjhBTZs2BD3tpKJIAyPww47jNtuu43BgwcntN3CkMrx\nHQDlypWjUaNG1pNrJAQbTltIxo0bR+/evfn222/529/+FpM6jejJycmhRYsW9OvXj8svvzwhbd51\n110cdthhPP300wlpL2h+//13GjZsyPr16+M+oiWStWvX0qRJE3788UcqVapUcIGAaN68OcOHD6d1\n69ZBSyk0d955J40aNco1RspiPIxY4ndUy6Y80jNjKyd1uOaaa2jbti133HEHxcl4SzbGjRtH2bJl\nE7oa6COPPMLw4cNZu3ZtwtoMkkQNo82N6tWr8/e//52XXnop4W37ZdOmTaxevZoWLVoELaVIWJyH\nkSj8uloOuOJ4i8RFNbuPiGSISE7E9l3Y8btF5EcR2S0iS0WkYwH13SMiK0Rkp4hsEZGvROSqaDQV\nhaFDh7Jw4UJGjhyZkPaS3U+cH/HQvmfPHnr37k3//v3jPsIoXH+NGjXo3Lkz//73v+PaZiwpyvkP\nws0STvfu3Rk0aFDSxh/Mnj2bk046KV/DLBX+u2Z4GIkiX8NDRGaJyEygjIjMDN+AZcAXUban3jYY\nGOJtY7y2rgGeAw4DxgJVgJEicm4+9R0DLAJeA77FLVw3VkTqRqmrUJQrV4533nmHhx56iMWLFyei\nSSOM4cOHU79+fdq2bZvwtnv27Mmbb77JL7/8kvC2E0lWVhaTJ0/mggsuCExD06ZNqV+/Pm+++WZg\nGvIj1eM7QjRs2JCNGzeyfv36oKUYBzuqmucGdAI6A7u896GtI24CsVL5lc+lvulAdh7HFgDZwOXe\n/k24VW+nRVH/Zq+Otnkc13gwYsQIbdq0qe7YsSMu9RsHsn37dq1evbp+8803gWno1auX3nLLLYG1\nnwg+//xzPe6444KWoZ999pk2btxYs7Ozg5ZyAK1atdIZM2YELSMmnHPOOTphwoQD0r1rp+9rvW22\n5bfl2+OhqqNVdRTQwnsf2sao6qeqWqglO0Vkk7dNFZETRaQEEBqu+433+rX3ml5AXeeLyHMiMgMo\nD8wCZhdGV2G58cYbOf7447nvvvsS2WyxZujQoZx++umccMIJgWl44IEHeP/99w/qqaaDdrOEOOus\nsyhTpgyffPJJ0FL2Y9u2bSxevDilg0rDadOmDfPmzQtahnGQU9JPJlVdKiLn4YyAwyKOPRZFe1uB\nCcD/gJOBs4FJOKOjBM4NE5qTeof3Wl5ESqvqnjzqPAm4y3u/C/hEVbPyEtC5c2fq1KkDQIUKFUhP\nT9/bVR/ywxZmf9iwYTRq1IjHHnuMxx9/vMj15bY/ZMiQmOlN9H64j7uo9TVv3pzBgwczaNAgMjIy\nAtO/cOFCLr30Uvr168frr7+eVOfbj34/5d955x1efvnlwPXPmDGD9u3b88gjj3DRRRcFrie0/9VX\nX9GyZUvKlCmTsN9/PPfLlSvHrFmzyMjIYNSoUQB7r5eGETP8dIsAzwOZwNvAyLDttcJ2teCMnlU4\n18j1wF/e+5re8eNxrpZNPupKA5oBq706OuSRT+PJN998o5UrV9YVK1bEpf7p06fHpd5EEEvtDz74\noN52220xq88PeenfsmWLVqlSRRcvXpxQPdFSmPO/bt06LV++vO7Zsyf2gqJk+vTpumfPHq1Zs6Z+\n9dVXQcvZyyOPPKKPPPJIgflS5b+7bt06PeKIIzQnJ2e/dMzVYlsMN3+Z3KJwNYvUEJQFqoftlw4z\nPK5kX4zHP7zjtxIW4+EZKg2BhmF1HBbRxiSvjj55aNB48+yzz2rLli119+7dcW+rOPLbb79pxYoV\n9bfffgtayl6efvppvfLKK4OWEXNGjx6tV1xxRdAy9mPAgAF6zTXXBC1jL6effrp++umnQcuIKbVr\n19Zly5btl2aGh22x3PwuErccaKmq23x1o+ReR23cSJhpwM84V0tzYC1wHC5Y9U1gPTARuAwXs9Fe\nVSd75Vfh3DFHqOpWEdkOfIZz3dQHzvEMjzNV9YARN7GcQCwvVJUrrriC2rVrM2TIkLi2VRy5/fbb\nKV++fFJN3rVz506OPfZYPv7445SfyyGcDh06cM4553DLLbcELWUvW7du5ZhjjuGbb74J3AWwe/du\nKleuzNq1aw+qSQSvvvpqLrnkEm644Ya9aTaBmBFL/M7jMRB4U0ROFpG64VsUbW0ERuMMhI5AVeC/\nQDtV3aSqb+GmYt8GdMC5dm5S1clhdYSG44aYDJyAGwHTHMgALs3N6EgUIsKIESMYP348H374YUzr\nDvcTpxqx0L5ixQree+89evbsWXRBUZKf/nLlytGrVy8eeyyacKfEEu35z87ODnwYbTgh/Ycffjg3\n3XQTQ4cODVYQMHfuXOrUqcNhhx1WYN5U+u9agKkRb/waHi8BFwOfAz+GbSv8NqSq21X1dlWtr6qH\nqupRqnqlqi4Jy/O8qh6rqmVUtZGqjg479rOqllDVkqq61Uu7QlVrevmrqerZqhp42HvFihUZO3Ys\nt95660E/z0Mi6d27N926daNixYpBSzmA2267jYULFzJnzpygpcSEefPmUaNGDY4++uigpRxA165d\nGT16NJs3B7NG5S+//MKTTz7J9ddfz5o1a1i4cGEgOuKFTSRmxBtbqyWO9O/fnwkTJpCRkUHJkr4G\nEBl58O2333LxxRezYsUKDj300KDl5Mqrr77K22+/zdSpU4OWUmQee+wx/vzzT5566qmgpeTKDTfc\nQLNmzXjooYcS0t62bdt47733GDNmDAsXLuTqq6/mhhtuoGzZsqSnp8d95txEsnPnTqpUqcLGjRsp\nU6YMYK4WI8ZEExAC1AROCjowpbAbCQguDSc7O1vPO+88ffjhhxPa7sHI+eefry+88ELQMvJlz549\nWq9ePZ02bVrQUorMiSeemNQjMebPn681atTQP//8M25tZGVl6aRJk/S6667T8uXL62WXXabvvfde\nsQgcb9GihX755Zd797HgUttiuPldJK6WiHwOLAWmemlXisjw+JhDBwdpaWm8/vrrjBo1iilTphS5\nvlTyE0dSFO0ZGRksX7480CBHP/pLlSpF37596d27N6rJ1ZMYzfnPzMxkxYoVnHrqqfETFCWR+tPT\n02nUqBHjxo2LeVuLFi3igQceoGbNmvTu3ZuTTjqJFStWMH78eK644goOOeSQqOtMtf+uuVuMeOI3\nxhLqtpYAAB8/SURBVONl4GPgb7j5NgCmAPmto2IAVatWZcyYMXTq1Il169YFLSflUFV69erFE088\nQenSpYOWUyAdOnRg06ZNfPrpp0FLKTSffvopZ599diCr0UZDjx49GDBgQEyMvHXr1jFo0CBatGjB\nRRddROnSpZk2bRrz5s2jS5cuVKlSJQaKU4fWrVub4WHEDz/dIrgRKWne+01h6X8E3WUTzUaCXS3h\n9O7dW9u1a5eUa00kM+PHj9fmzZun1Hn7z3/+oy1btjxgEqZUoUOHDvrKK68ELaNAcnJytFmzZoWe\nR2Pnzp361ltvafv27bVChQrauXNnnTZtWkr91uLF4sWLtV69env3MVeLbTHc/PZ4/A4cG54gIk0A\nG7Lhk1CwXv/+/YOWkjJkZ2fz8MMP8+STT5KW5venGjxXXHEF2dnZfPDBB0FLiZrQMNpkWJ+lIESE\n7t27M2DAAN9lcnJymDFjBrfccgs1atRg5MiRXHfddfz222+MHDmSs846K6V+a/GiUaNGbNiwgQ0b\nNgQtxTgI8fsPGwBMEJEbgZIi0gE3fXpyhrwnISVLlmTs2LE8++yzzJ5duDXsUs1PHE5htL/xxhtU\nrFiRCy+8MPaCoiQa/WlpaTzxxBP07t2bnJyc+ImKAr/6v/rqK6pXr550w2jz0t+hQwe+//77Aoe0\nLl++nN69e1O3bl26dOlCw4YNWbRoEZ9++inXXXdd3EdKpdp/Ny0tjRNPPNHm8zDigi/DQ1VfAx4E\nrgJ+xU0A1ltV34yjtoOOo48+mhEjRnDttdeycePGoOUkNX/++Sd9+vTh3//+d0oOVbzooos49NBD\neeedd4KWEhXJshqtXw455BC6du3KwIEDDzi2adMmXnrpJU4++WTOOOMMduzYwfjx4/nuu+944IEH\nqFGjRgCKUwcLMDXihc3jEQDdu3dnxYoVfPDBByl5U00Ezz77LJMnT2bChAlBSyk0U6dO5e6772bx\n4sUpM49Lq1atePrppznrrLOCluKbzZs3U69ePb777juqVq3KxIkTGTNmDJ999hnt27enY8eOnHfe\neSnzHSQLH3zwAS+99BKTJk2yeTyMmOJ3rZZngXEaNhW5iJwCXK2q98VRX0xJFsNjz549nHbaaVx7\n7bXcd1/KnL6EsW3bNurXr8/kyZNp3rx50HIKjapy1lln0alTJ2688cag5RRIZmYm9evXZ/369Skx\ngiice++9lylTppCZmUnTpk3p2LEjV155JeXLlw9aWsqybt06mjRpwsaNG0lLSzPDw4gZfmM8OgBf\nR6R9A1wbWznFg9KlSzNu3DiefPJJvv468rTmTar5icOJRvvgwYNp165dUhkdhTn3IsL//d//0a9f\nP/bs2RN7UVHgR39oGG0yGh0F6b/44ov5/fffGT16NDNmzODmm29OKqMjFf+7Rx55JH/729/48ccf\ng5ZiHGT4NTw0l7wloihvRFC3bl1eeOEFrrnmGrZs2RK0nKRh/fr1DB06lH79+gUtJSacdtppNG7c\nmBEjRgQtpUBSLb4jnHbt2vHZZ58lRSDywYTFeRjxwK+r5T3ckvQPqmqOiKQB/YH6qvr3OGuMGcni\nagnnzjvvZPPmzbz11lsW7wF069aNP//8kxdeeCFoKTHj66+/5vLLL2fFihWULVs2aDm5kp2dTbVq\n1Zg/fz41a9YMWo6RJAwcOJDVq1fz/PPPm6vFiBl+eyzuBdoBa0VkHrAGN2vpPfESVlwYNGgQS5Ys\nYfhwm33+l19+YfTo0Tz66KNBS4kpJ554Iq1atWLYsGFBS8mT0DBaMzqMcGwGUyMe+DU81gAnAJcB\nzwCXAy1V9bd4CSsulC1blrfffpuHH36Y77//Pt+8qegnDuFHe79+/bjjjjuoXr16/AVFSVHP/eOP\nP85TTz3F9u3bYyMoSgrSn+xullT+7UPq6m/ZsmWB1yXDiJYCDQ8RKQHsAEqp6hxV/Y/3mhwzIx0E\nNGrUiAEDBnD11VezY8eOoOUEwvvvv8/7779Pjx49gpYSF4477jjOOussnnvuuaCl5EqyGx5GMJQr\nV46rrroqaBnGQYbfGI+FQHtVXVOkxkQygDMikr9X1ebe8buB+4GjgdXAk6o6Jp/6ugLX4aZzL41b\nPfdxVf0oj/xJF+MRTseOHSlVqlRKBCLGirVr19K1a1fmzp1LVlYWEydOJD09PWhZcWHZsmWceuqp\nvP/++5x22mlJE9Ozfv166tevT2ZmZlKOaDGCRVVtOK0RU/y6Wt7ETZneSUTOEZGzQ1uU7am3DQaG\neNsYABG5BngOOAwYC1QBRopIfivg/h0oD3wAfAe0BN4VkeOj1JUU/H979x9dRX3nf/z5BkRxAYP8\nUKMQFlD0ixL8UYSlNBS0ssIWl6USkG9Bvl/ElT1of+ixrOJPpPvtUejqQai/0EUlRbv9YosQBSJq\nhQIlgj8oLQpVtKIgikQQk/f+MRO8jUm4N0zu3Amvxzlz7p2ZO5953ctNeGc+n5mZM2cOL730Eo8/\n3vQvCFtVVcW8efPo3bs3PXv2ZPPmzSxZsoTCwkT+06WlZ8+eDB48mKFDh2Z0GnVjW7ZsGd/+9rdV\ndEitcqVAliYknTvJEZzRUtv0ViZ3pANWApV1rCsHKoHLwvmJQBWwop72+qQ8N+DPYRvX1fF6z3Xl\n5eXeoUMH37Jly9fWrVy5MvuBIpKa/fXXX/cBAwZ4v379fNOmTfGFykBUn/2BAwe8qKjIR40a5QcP\nHoykzXTUl3/s2LE+b968rGVpiCR/992Tnx/dnVZThFO692r5+zqmbg0pdsxsdzg9b2YXhONIeoWr\n14eP1X8S1nnc3d3LU5sl6G4BSOyg18LCQm677TZGjx7NgQMH4o4Tqf3793PLLbdQVFTE2LFjefnl\nlzn77LPjjpVVLVu2ZOnSpXzyySdMnDgx9pvIJelutCLSNKR9rxYzOwboB+S7e4mZ/R2Au6c9GtLM\nqu8TvgPoDxQCuwmKjvcJumE6uPvHZtYd+FO4rJW713vpRzP7OcHpvS8Bg7yWwa9m5uPHj6dr164A\n5OXl0adPHwYNGgR8NfI87vmioiK+973vUVVVxdSpU2PPE8X8qlWrGDduHAUFBSxcuJBTTz01p/Jl\ne76iooL+/ftTUFBw6J49ceR54403mDNnDq+99lpOfT6aj3e+rKyM+fPnA9C1a1duu+02XGM8JCLp\nDi49B1gMHABOc/fWZnYpMN7dRzdox2YtCAqLLsB44BGCMSdd3f2dcJzGBmCPu59YTzvNgF8QdM38\nHrjE3Wu9FGiuDy5NtWfPHs4991zuuece/vmfE3ONtq/5+OOPueGGG3j22We59957E/1eorZ3714u\nuugiBgwYwN133x1LX/qtt97Kvn37+NnPfpb1fUty6CZxEqV0B5feD0x39zOBg+GyF4BvprsjM2tl\nZqkXaEjd937g9fB53xqP5eH2Lcysp5n1TGnzWOBXBEXHMmBwXUVH0uTl5bFw4UKuvvpqtm/fDiTr\nWgDuTklJCb169aJly5bMnTs30UVHY3z2bdq0YenSpaxcuZLp06dH3n6quvIn5TTaJH33a5P0/CJR\nSvc+0b2ABeHz6lGa+8wsk+s/dwL+aGYrgO0EXS0FBF0sK4BjCM6emWNmwwkuVuYEl2YHOBV4E3Az\na+funwIPA98FPge2AjPCvxp/7+5PZpAtJ1144YVcf/31FBcXM2PGjMSMLt++fTtTpkxh27ZtPP30\n0/Tv31+/eOvQrl07SktLKSoqolWrVkybNi1r+/7www/ZvHkz3/xm2n8/iIgcuXRGoBJ0eVwQPt8d\nPvYl+A8+3TZaA/MIulf2EVwN9SngrJTX/BvBmSn7Ca7JMT5lXQHBGStfAm3DZSvDZTWnh+vI4ElT\nWVnpxcXF3qxZM7/wwgv9wQcf9F27dsUdq1Zffvmlz5o1y9u3b+933nmnHzhwIO5IibFjxw7v3r27\nz5o1K2v7XLBggY8YMSJr+5PkQme1aIpwSneMx3DgIWAu8CNgBnA1MMndSxtY82RdksZ4pHJ3Vq9e\nzbvvvssvf/lLSktLGThwIMXFxYwYMYI2bdrEHZENGzYwadIk2rZty9y5cznjjDPijpQ427dvp6io\niGnTpnHVVVc1+v7GjRvHwIEDmTx5cqPvS5JNYzwkSumeTvsb4B8JLur1AsHRh5FJKjqSzMzo378/\nHTt2ZNGiRbz77ruMGTOGkpISTjvtNEaNGsWiRYuoqKjIeraKigpuuOEGhg4dypQpU1i+fHmtRUfS\nu1qykb+goIDnn3+e22+/nQULFhx+gwzUzF9ZWcnSpUsTMb4D9P0RaUrSHVyKu//B3a9x92HufrW7\nrz/8VtIY2rRpwxVXXMEzzzzDtm3buPTSS3nggQfIz88/tDwb1wBZtmwZZ599Njt27GDTpk1ceeWV\niRmHkqt69OhBaWkp119/PU899VSj7WfdunWcdNJJdOnSpdH2ISJSm3S7WloCNwFjgHyC8RkLgRnu\nvr9RE0YoqV0t6dq5cydPPfUUJSUlbNq0icsuu4zi4mIGDx5MixbpjiNObz8//OEPefnll7n//vsZ\nOnRoZG1LYMOGDQwdOpSHH36YYcOGRd6+TqOVTKirRaKUyem0g4GpwDfCxyJgTiPlkgbo1KkT11xz\nDS+88AIbN27knHPOYfr06eTn5x9afiRXynR35s+fzznnnMMpp5zCa6+9pqKjkZx77rksXryYCRMm\nsHz58sjbT8pptCLSBKUzAhXYBeTVWHYi4RkuSZlI4FktqRp6v4etW7f6zJkzvbCw0PPz8/26667z\n1atXe1VVVdptbNmyxQcPHuznnXeer1+/PuMMSb9XRVz5y8rKvEOHDv7iiy8eUTup+Xfu3Olt27ZN\n1FlH+v7EC53VoinCKd0jHn8Fjq+xrBXBNTgkx3Xr1o0bb7yR8vJyli9fTl5eHuPHj/+b5e61d0Ed\nPHiQu+66i/79+zN8+HDWrFnDeeedl+V3cPQqKiri8ccfZ+TIkaxduzaSNktLS3U3WhGJTbpjPG4E\nxhLctv5doDMwheD29Yd+G7r7isaJGY2mPsYjE+7Oxo0bWbhwISUlJRxzzDEUFxdTXFzMWWedBcDq\n1auZNGkSnTt3Zs6cOYfucSPZt3jxYiZNmsRzzz1H7969j6gtnUYrmdIYD4lSuoXH22m05d7Au9Vm\niwqP2rk7a9euPVSEtG/fnvz8fF599VVmz57N5ZdfrrNVckBJSQk/+MEPWLFiBWeeeWaD2qiqquKk\nk05i/fr1OqNF0qbCQ6KU7nU8/j6NKaeLjqagsa4FYGb07duXe+65h3feeYdrr732UCEyevToSIqO\npF/HIBfyjx49mpkzZ3LxxRfz1ltvZbRtdf5169bRqVOnxBUdufD5H4mk5xeJUnTnWEqT0KxZMyZO\nnMj5559PYWFh3HGkhvHjx1NRUcGQIUNYtWoVnTt3zmh7nc0iInFLq6ulqVBXizQVd999N/PmzWPV\nqlWcfPLJaW/Xr18/ZsyYwZAhQxoxnTQ16mqRKKnwEEmoO+64g5KSEsrKyujQocNhX//RRx/RvXt3\ndu7cybHHHpuFhNJUqPCQKKV9yXSJX5L7iZOcHXIz/0033cTw4cO55JJL2LNnT72vLSsro7S0lEGD\nBiWy6MjFzz8TSc8vEiUVHiIJZWbMnDmTAQMGcOmll/LZZ5/V+3qN7xCRXKCuFpGEq6qq4qqrrmLr\n1q0sWbKEVq1a1fqak08+mbVr11JQUBBDSkkydbVIlHTEQyThmjVrxrx588jPz2fkyJG13pl4/fr1\ndOjQQUWHiMQuq4WHmZWZWVWNaWPK+ilm9mcz229mm83s+4dpb4iZvWRmFWFbOX3l1COV5H7iJGeH\n3M/fvHlzHn30UY4//njGjBnDwYMH/2b9nDlzEt3Nkuuf/+EkPb9IlLJ9xMPDaRYwO5weAzCzYoJL\nsrcmuBR7R+ARM7u4nvbOAI4DNoXtihy1WrRowZNPPsn+/fsZP348lZWVh9atWbMm0YWHiDQdWR3j\nYWYrgW+5e/Na1pUD5wD/4u6/NrOJwINAmbsPPky71xIUM/W+VmM85Gjw+eefM2zYMLp168YvfvEL\ndu/erdNo5YhojIdEKZYxHma2O5yeN7MLzKw50CtcvT58XBc+9sl+QpHkatWqFYsXL+bNN9/k2muv\nTfRptCLS9GS78PgU+A2wENgODAaWAh2A6qMg1ecE7gsfTzAz3b+bZPcTJzk7JC9/69atWbJkCa+8\n8gpTp06lTZs2JPloX9I+/5qSnl8kSlm9V4u7j6h+bmYtgD8BXYCLgUqCQqg18HH4CPCJu38RVYYJ\nEyYcur17Xl4effr0YdCgQcBXvxxydb68vDyn8mg+t+c3bNjAzTffzOzZs3nuued46KGH6NGjR87k\n03zuzpeVlTF//nyAQ78vRaKStTEeZtYKyHP398P5lsAfCQqP0cBNBGM8Lnf3p81sEjCPcNxGWKh0\nB3D3P9ZoW2M8ROrg7rz66qsUFhZGcqdhOfpojIdEKZuFRwFBobGCoJulP9AbeJ+g4LgEeBz4EFgC\njABOAP7R3UvD7d8mOHulnbt/amYDgEnAWcA3gA8Ium42u/t/1JJBhYeISIZUeEiUsjnGYxfwKHA6\n8H2gE/Ar4CJ33+3uTwJTgb3AGGAnMNHdS1PaqD4dt1oP4H8DF4TLO4VtX9K4byUe1YdCkyjJ2UH5\n46b8Ik1H1sZ4uPtnwOTDvOY+4L461m3nqwGo1cseJShmREREJAF0rxYREamXulokSrpXi4iIiGSN\nCo8ESXI/cZKzg/LHTflFmg4VHiIiIpI1GuMhIiL10hgPiZKOeIiIiEjWqPBIkCT3Eyc5Oyh/3JRf\npOlQ4SEiIiJZozEeIiJSL43xkCjpiIeIiIhkjQqPBElyP3GSs4Pyx035RZoOFR4iIiKSNRrjISIi\n9dIYD4mSjniIiIhI1qjwSJAk9xMnOTsof9yUX6TpUOEhIiIiWaMxHiIiUi+N8ZAoZfWIh5mVmVlV\njWljyvopZvZnM9tvZpvN7PtptJnxNiIiIhKPbHe1eDjNAmaH02MAZlYM3Au0Bp4AOgKPmNnFdTXW\nkG2SLMn9xEnODsofN+UXaTpaxLFTd/9RLYtvJChKrnb3X5vZROBB4CfAc3U01ZBtREREJCZZHeNh\nZiuBbwGfhIv+QFA8bAD2ExyB6eru75hZb6Ac2OPuJ9bSVvMGbKMxHiIiGdIYD4lSto94fAr8BtgB\n9AcGA0uBXkBzgqMXn4Wv3Rc+nmBmLd39ixptdWjANkyYMIGuXbsCkJeXR58+fRg0aBDw1eFQzWte\n85o/mufLysqYP38+wKHflyJRie2sFjNrAfwJ6AKMBx7hb49eFBIcCcnkiMfhtkn0EY+ysrJDvySS\nJsnZQfnjpvzx0hEPiVLWBpeaWSszO6WOfe8HXg+f963xWB5u38LMeppZTwB3rzzcNk1NeXly31aS\ns4Pyx035RZqObHa1dAL+aGYrgO0EXS0FwPvACuAY4HFgjpkNB0YQdKP8NNz+VOBNwM2snbt/CvzH\nYbZpUvbs2RN3hAZLcnZQ/rgpv0jTkc3TaXcBjwKnA98nKER+BVzk7rvd/UlgKrAXGAPsBCa6e2lK\nG9Wn4wYz6W1zxKr7Phtj27rWH8k+o2orne2UP5ocDd0uyflzOXs62yq/SMNkrfBw98/cfbK7n+7u\nf+fu+e4+yt3fTHnNfe7ew92Pc/cz3f3RlHXb3b25u7cIj3Ycdpuo5MoP/7Zt2xolw5Fsl27+hmZP\nN0dDt1P+aHI0ZLtc/u6ns+3RlF8kSkfdJdPjziAikkQaXCpROaoKDxEREYmX7k4rIiIiWaPCQ0RE\nRLJGhYeIiIhkjQoPERERyRoVHiEzu8XMqsysMnysMrPT4s6VCTMbaGYbzKzCzP4Qd550mVlBymdf\naWYb487UEGZ2Vfg+nog7SybM7Cwze9vM9pvZVjMbGXemTJhZkZltNrPPzWyLmX0n7kyZCK/K/IqZ\nfWFm78WdJxNmdpOZfRB+7pfEnUeSQYXHV+4GTgM6A2XAVnd/N9ZEGTCzE4DFwCbgfODBeBM1SH+C\nz39I3EEyZWbHANOAA3FnaYBPgf8DFBLccPHOeONk7DjgLuBcgjtfz403Tsac4GKKL8QdJBNmNhC4\nHbgGeAZ4wsxaxZtKkkCFRyi8wNl7QBXwTeChmCNlajjQFviJu7/p7nPiDtQAvwWWA4NiztEQkwlu\nUPh+3EEy5e473H0F8BZBEfJGzJEy4u7L3P0xd98MrAe+doPIXObule7+MyAxf+iEvgPsc/engRIg\nD+gXbyRJgkQXHmZ2rZm9amZfhoe4p9dYf6yZ3RseCqwws5fMrG9d7YUmhI+RXwG1pojzV3cLLTKz\n98xsdoKyfwqMBb4NbAHmm1m7pOQP/8q7AbgZyMpFlqL+7pvZvxIc7fgG8HQjx2+Un10zOxu4ApjX\nmNnDfTXG756siuA9dAT2hc8/I/jud8xSfEmwRBceBF0Ku4C/kHIPlxQ/B6YAfwX+m+BQfqmZnQhg\nZv8e9gtXmFnncJsJwLPuno2/XCPLD7Qk+MF/EpgNTDWzixOSvbW7L3T314BHCA6dd2vE7FHnvx74\nHcFNDMPV1tgFSNTf/QXABcArwP2NnD3y/GZ2OlBK0F3xk6Tlz0Le2hzRewA+BFqHz9umLBOpn7sn\nfiL4oagEpqcs60jQ334QaB8ueyz1dQSHBruFUzNgIEFXyz8lMP8Z4WsnA9eFrytKSPaLgSuBnsAi\ngr+i8hL02d8brqsKp0rg9gTl/xZwYfgdWgb8NWHf/dMI7ni9AehOcCfrZgnK3ww4k2Ccxwfhz0Gr\nBPwbfCucHw3MIihijs9Wbk3JnZJ+xKM+vYBjgL+4+65w2TqCowJ9ANx9j7u/FU5VBP/5vU8w1iBu\nmebfQjDI62bg34H/5+5xDVbLKDvBYdobCP7jKATGuXuc9xHPNP9PCbooLgDeI/jPO84xNpnm70Iw\nMLkcOIXg7tFxyjT/EILiozfwJ4K/4OM8I60hv3veAEYAHcLn38h+7L+RzntYBdwK/CfBGLMr3L0i\n+1ElaVrEHaARnRQ+fpayrLo/8uTaNnD3iY2aKDMNyf8A8EBjhkpTRtnd/RXgrMYOlYFM8+8AdoSz\nuXAKdqb5FxB0teSKTPM/ShbGZGWgIT+7ufZHYFrvwd3vAO7IVihpGnLtyx6lD8LH1inLqp//NctZ\nGiLJ+ZOcHZQ/bsofv6bwHiRHNeXC4w2C/skuZlY90rovwSCq8thSpS/J+ZOcHZQ/bsofv6bwHiRX\nxT3I5EgmgosezScYWFZFMEbgEeC74fp5BIOfNhGc7VEJ7CEcLBX3lOT8Sc6u/Mp/tOdvKu9BUzKn\n2AMcUfjgh6Sylql61PVxBGccfABUAC8CfePO3RTyJzm78sc/KX/8U1N4D5qSOZl7badvi4iIiESv\nKY/xEBERkRyjwkNERESyRoWHiIiIZI0KDxEREckaFR4iIiKSNSo8REREJGtUeIiIiEjWqPAQERGR\nrFHhISIiIlmjwkOkiTCz48zsGTP72MxKzGysmS2NO5eISKoWcQcQkciMAjoCJ/pX90J4IsY8IiJf\noyMeIjnGzKyBmxYAW1w3YBKRHKbCQ44qZva2mf3YzF41s71m9oCZdTKzJWb2qZmVmtkJ4Wv7mdnL\nYdfFBjMrSmlnpZndEa7fa2b/38xONLMFZvaJma0xsy4pr/8HM/t92NYaM+tfo607zewlM9sH/MjM\n1tXI/SMz+1U97+tWYDpQHL6PK81svJm9mPKaKjObbGZbzGyXmd1Xo42JZvZGuO7Z1PwiIlFR4SFH\no5HAEOAM4LvAEuBGoD3QHJhqZvnAb4Db3b0d8GPgaTNrn9LOaOAKIB/oAfwOeAhoB2wGbgEws3Zh\nW7PDfcwCfhsurzYO+L9AG+A/ga5m1jNl/RXAY3W9IXe/FbgLWOjubd39kepVNV46DDgf6ANcbmbf\nCTNeFn4GlxF017wIPFnX/kREGkqFhxyN7nX3j9z9fYL/YNe4+0Z3Pwj8N3AeQSHwW3dfBuDuy4F1\nwKUp7Tzi7tvcfS/wLLDV3Ve6exWwCDg3fN0wgi6QJ9y9yt0XEhQm/5TS1nx33xyu/wIoCTNgZr0I\nulF+G8F7n+nue939HWAlQQECcFW4bkuY/6dAHzPrHME+RUQOUeEhR6MPUp5/Xst8a4L/6C83s93h\n9DEwADg5w3YgOCKyvUaG7cCpKfPv1Fj/GDA2fD4O+GVYGB2p1IwVKRkLgJ9Xv19gF8HRklMREYmQ\nzmoR+ToH/gI85u6TI2jvPeBfaizrQnCUJHWfX824rzGzL8xsIEEBMiaCHPV5B7jT3dW9IiKNSkc8\nRGq3APiumX3HzJqF18goCsd+ZGoJcLqZFZtZczMbDZwFPHOY7f4LuA846O6/a8B+MzEXmGZm/wvA\nzE4ws1GNvE8ROQqp8JCjTc3BlrWeeuruOwgGnk4DPiToGvkxX/3MpH3KqrvvBoaH238UPg5z948P\n09Z/AWdTz6DSDNX53t391wTjOhaa2R5gIzA0ov2KiBxiOuVfJDeZ2XEEYzLOc/etcecREYmCjniI\n5K5rgLUqOkSkKdHgUpEcZGZvh08vq7H8NYKBqYcWEXSZTNbAUBFJAnW1iIiISNaoq0VERESyRoWH\niIiIZI0KDxEREckaFR4iIiKSNSo8REREJGv+B30+iCzKJCD/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77af0d4290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_options = {'x': 'log'}\n",
    "plot_data, _ = random_choice_plots.one_key_layout_data('random_choice_1',\n",
    "                                         'support',\n",
    "                                         ['memory_fine', 'swap_prob'])\n",
    "random_choice_plots.save_layout(plot_data[0],\n",
    "                    'memory fine effect (half life: 2667, decay: 0.9)',\n",
    "                    ['support_effect', 'plots'],\n",
    "                    'nn128;ns80000;hl2667;dc0.9;swpr0.03',\n",
    "                              plot_options=plot_options)\n",
    "random_choice_plots.draw(plot_data[0], 'memory fine effect (half life: 2667, decay: 0.9)', plot_options=plot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 113\n",
      "2: 483\n",
      "3: 558\n",
      "4: 378\n",
      "5: 291\n",
      "6: 274\n",
      "7: 320\n",
      "8: 213\n",
      "9: 262\n",
      "10: 178\n",
      "11: 78\n",
      "12: 30\n",
      "13: 40\n",
      "14: 9\n",
      "15: 5\n",
      "16: 2\n",
      "17: 1\n",
      "19: 1\n"
     ]
    }
   ],
   "source": [
    "uppercase = [u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H', u'I', u'J', u'K', u'L', u'M', u'N', u'O', u'P', u'Q', u'R', u'S', u'T', u'U', u'V', u'W', u'X', u'Y', u'Z']\n",
    "lowercase = [u'a', u'b', u'c', u'd', u'e', u'f', u'g', u'h', u'i', u'j', u'k', u'l', u'm', u'n', u'o', u'p', u'q', u'r', u's', u't', u'u', u'v', u'w', u'x', u'y', u'z']\n",
    "def count_words(text):\n",
    "    word_lengths = dict()\n",
    "    current_length = 0\n",
    "    for char in text:\n",
    "        if char not in uppercase and char not in lowercase:\n",
    "            if current_length != 0:\n",
    "                if current_length in word_lengths.keys():\n",
    "                    word_lengths[current_length] += 1\n",
    "                else:\n",
    "                    word_lengths[current_length] = 1\n",
    "                current_length = 0\n",
    "        else: \n",
    "            current_length += 1\n",
    "    for length in word_lengths.keys():\n",
    "        print(\"%s: %s\" % (length, word_lengths[length]))\n",
    "count_words(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           memory fine:  1.0\n",
      "      support:  0.3\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.27\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.24\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.21\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.15\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.12\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.09\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.03\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.0\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.1\n",
      "      support:  0.3\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.27\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.24\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.21\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.15\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.12\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.09\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.03\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.0\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.01\n",
      "      support:  0.3\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.27\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.24\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.21\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.15\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.12\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.09\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.03\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.0\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.001\n",
      "      support:  0.3\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.27\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.24\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.21\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.15\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.12\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.09\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.03\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.0\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n"
     ]
    }
   ],
   "source": [
    "normal_run_prob = {'init': 0.1, 'epochs': 15000}\n",
    "\n",
    "memory_fine_values = [1., 0.1, 0.01, 0.001]\n",
    "support_values = [0.3 - i  * 0.03 for i in range(11)]\n",
    "swap_probability_values = [0.2, 0.14, 0.1, 0.07, 0.05, 0.03]\n",
    "\n",
    "threshold = {'fixed': True, 'min': 0.5, 'max': 0.7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "\n",
    "for memory_fine_value in memory_fine_values:\n",
    "    print(' '*10, \"memory fine: \", memory_fine_value)\n",
    "    optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "    for support_value in support_values:\n",
    "        print(' '*5, \"support: \", support_value)\n",
    "        for swap_value in swap_probability_values:\n",
    "            print(\"swap probability: \", swap_value)\n",
    "            model = random_choice(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 1,\n",
    "                                 [128],\n",
    "                                 0.,\n",
    "                                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                                        normal_run_prob,\n",
    "                                  support_value,\n",
    "                                        swap_value,\n",
    "                                 train_text,\n",
    "                                 valid_text)\n",
    "            text_list, trigger_list = model.run_for_analitics(model.get_triggers,\n",
    "                                                        'random_choice/variables/sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value),\n",
    "                                                        [300, 75, None])\n",
    "            \"\"\"check_text = ''\n",
    "            check_text = check_text.join(text_list)\n",
    "            count_words(check_text)\"\"\"\n",
    "            triggers = list()\n",
    "            for text_number, text in enumerate(text_list):\n",
    "                trig = list()\n",
    "\n",
    "                text_triggers = trigger_list[text_number]\n",
    "                for text_trigger in text_triggers:\n",
    "                    trig.append(text_trigger[0, 0])\n",
    "                triggers.append(trig)\n",
    "            structure_vocabulary_plots(text_list,\n",
    "                                   triggers,\n",
    "                                   'triggers (swap probability: %s; support %s%%; memory fine: %s)' % (swap_value, support_value, memory_fine_value),\n",
    "                                   'mean trigger',\n",
    "                                   ['random_choice', 'vocabulary_plots'],\n",
    "                                   'ns20k_th0.5_ib0_sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value),\n",
    "                                       ylims=[0., 1.],\n",
    "                                       threshold=0.5,\n",
    "                                   show=False)\n",
    "            for i in range(50):\n",
    "                text_plot(text_list[i],\n",
    "                      triggers[i],\n",
    "                      'trigger',\n",
    "                      'triggers (swap probability: %s; support %s%%; memory fine: %s)' % (swap_value, support_value, memory_fine_value),\n",
    "                      ['random_choice', 'text_plots', 'ns20k_th0.5_ib0', 'sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value)],\n",
    "                      'ns20k_th0.5_ib0_sw%s_sp%s_mf%s#%s' % (swap_value, support_value, memory_fine_value, i),\n",
    "                          threshold=0.5,\n",
    "                      show=False)\n",
    "            model.destroy()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           memory fine:  1.0\n",
      "      support:  -0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.1\n",
      "      support:  -0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.01\n",
      "      support:  -0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.001\n",
      "      support:  -0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.02\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  0.04\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n"
     ]
    }
   ],
   "source": [
    "normal_run_prob = {'init': 0.1, 'epochs': 15000}\n",
    "\n",
    "memory_fine_values = [1., 0.1, 0.01, 0.001]\n",
    "support_values = [-0.04, -0.02, 0.02, 0.04]\n",
    "swap_probability_values = [0.2, 0.14, 0.1, 0.07, 0.05, 0.03]\n",
    "\n",
    "threshold = {'fixed': True, 'min': 0.5, 'max': 0.7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "\n",
    "for memory_fine_value in memory_fine_values:\n",
    "    print(' '*10, \"memory fine: \", memory_fine_value)\n",
    "    optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "    for support_value in support_values:\n",
    "        print(' '*5, \"support: \", support_value)\n",
    "        for swap_value in swap_probability_values:\n",
    "            print(\"swap probability: \", swap_value)\n",
    "            model = random_choice(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 1,\n",
    "                                 [128],\n",
    "                                 0.,\n",
    "                                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                                        normal_run_prob,\n",
    "                                  support_value,\n",
    "                                        swap_value,\n",
    "                                 train_text,\n",
    "                                 valid_text)\n",
    "            text_list, trigger_list = model.run_for_analitics(model.get_triggers,\n",
    "                                                        'random_choice/variables/negative_support1/sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value),\n",
    "                                                        [300, 75, None])\n",
    "            \"\"\"check_text = ''\n",
    "            check_text = check_text.join(text_list)\n",
    "            count_words(check_text)\"\"\"\n",
    "            triggers = list()\n",
    "            for text_number, text in enumerate(text_list):\n",
    "                trig = list()\n",
    "\n",
    "                text_triggers = trigger_list[text_number]\n",
    "                for text_trigger in text_triggers:\n",
    "                    trig.append(text_trigger[0, 0])\n",
    "                triggers.append(trig)\n",
    "            structure_vocabulary_plots(text_list,\n",
    "                                   triggers,\n",
    "                                   'triggers (swap probability: %s; support %s%%; memory fine: %s)' % (swap_value, support_value, memory_fine_value),\n",
    "                                   'mean trigger',\n",
    "                                   ['random_choice', 'vocabulary_plots'],\n",
    "                                   'ns20k_th0.5_ib0_sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value),\n",
    "                                       ylims=[0., 1.],\n",
    "                                       threshold=0.5,\n",
    "                                   show=False)\n",
    "            for i in range(50):\n",
    "                text_plot(text_list[i],\n",
    "                      triggers[i],\n",
    "                      'trigger',\n",
    "                      'triggers (swap probability: %s; support %s%%; memory fine: %s)' % (swap_value, support_value, memory_fine_value),\n",
    "                      ['random_choice', 'text_plots', 'ns20k_th0.5_ib0', 'sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value)],\n",
    "                      'ns20k_th0.5_ib0_sw%s_sp%s_mf%s#%s' % (swap_value, support_value, memory_fine_value, i),\n",
    "                          threshold=0.5,\n",
    "                      show=False)\n",
    "            model.destroy()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           memory fine:  1.0\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.1\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.01\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "           memory fine:  0.001\n",
      "      support:  -0.06\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.1\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.14\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.18\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.22\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n",
      "      support:  -0.26\n",
      "swap probability:  0.2\n",
      "swap probability:  0.14\n",
      "swap probability:  0.1\n",
      "swap probability:  0.07\n",
      "swap probability:  0.05\n",
      "swap probability:  0.03\n"
     ]
    }
   ],
   "source": [
    "normal_run_prob = {'init': 0.1, 'epochs': 15000}\n",
    "\n",
    "memory_fine_values = [1., 0.1, 0.01, 0.001]\n",
    "support_values = [-0.06 - i  * 0.04 for i in range(6)]\n",
    "swap_probability_values = [0.2, 0.14, 0.1, 0.07, 0.05, 0.03]\n",
    "\n",
    "threshold = {'fixed': True, 'min': 0.5, 'max': 0.7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "\n",
    "for memory_fine_value in memory_fine_values:\n",
    "    print(' '*10, \"memory fine: \", memory_fine_value)\n",
    "    optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "    for support_value in support_values:\n",
    "        print(' '*5, \"support: \", support_value)\n",
    "        for swap_value in swap_probability_values:\n",
    "            print(\"swap probability: \", swap_value)\n",
    "            model = random_choice(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 1,\n",
    "                                 [128],\n",
    "                                 0.,\n",
    "                                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                                        normal_run_prob,\n",
    "                                  support_value,\n",
    "                                        swap_value,\n",
    "                                 train_text,\n",
    "                                 valid_text)\n",
    "            text_list, trigger_list = model.run_for_analitics(model.get_triggers,\n",
    "                                                        'random_choice/variables/negative_support2/sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value),\n",
    "                                                        [300, 75, None])\n",
    "            \"\"\"check_text = ''\n",
    "            check_text = check_text.join(text_list)\n",
    "            count_words(check_text)\"\"\"\n",
    "            triggers = list()\n",
    "            for text_number, text in enumerate(text_list):\n",
    "                trig = list()\n",
    "\n",
    "                text_triggers = trigger_list[text_number]\n",
    "                for text_trigger in text_triggers:\n",
    "                    trig.append(text_trigger[0, 0])\n",
    "                triggers.append(trig)\n",
    "            structure_vocabulary_plots(text_list,\n",
    "                                   triggers,\n",
    "                                   'triggers (swap probability: %s; support %s%%; memory fine: %s)' % (swap_value, support_value, memory_fine_value),\n",
    "                                   'mean trigger',\n",
    "                                   ['random_choice', 'vocabulary_plots'],\n",
    "                                   'ns20k_th0.5_ib0_sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value),\n",
    "                                       ylims=[0., 1.],\n",
    "                                       threshold=0.5,\n",
    "                                   show=False)\n",
    "            for i in range(50):\n",
    "                text_plot(text_list[i],\n",
    "                      triggers[i],\n",
    "                      'trigger',\n",
    "                      'triggers (swap probability: %s; support %s%%; memory fine: %s)' % (swap_value, support_value, memory_fine_value),\n",
    "                      ['random_choice', 'text_plots', 'ns20k_th0.5_ib0', 'sw%s_sp%s_mf%s' % (swap_value, support_value, memory_fine_value)],\n",
    "                      'ns20k_th0.5_ib0_sw%s_sp%s_mf%s#%s' % (swap_value, support_value, memory_fine_value, i),\n",
    "                          threshold=0.5,\n",
    "                      show=False)\n",
    "            model.destroy()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
