{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not one byte characters:  0\n",
      "min order index:  9\n",
      "max order index:  255\n",
      "total number of characters:  196\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99337500 side of the leftist milieu. It often focuses on the individual r\n",
      "22500 ture in Mutual Aid: A Factor of Evolution (1897). Subsequent ana\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 22500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ\n",
      "char2id(u'a') = 67,  char2id(u'z') = 92,  char2id(u' ') = 2\n",
      "id2char(78) = l,  id2char(156) = Ø,  id2char(140) = È\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'side of the', u'though the ', u'; \\n|colspan', u'ustrian.asp', u'e most cons', u' Gaul, whic', u'h can be pr', u'pitome of t', u' first team', u'Roussimoff ', u'col. In Feb', u's]] - [[Uni', u'g oven, dis', u' exposed to', u' wrote the ', u'ird.  He co', u'a]].  The O', u'nd Lower Lo', u'In another ', u'or sensitiv', u'ents used b', u'e Chile]] i', u\"achelor's d\", u'till employ', u'flexibility', u'nects all c', u'variety of ', u' [[Cheshire', u' sleek, str', u' have led t', u'ongside man', u'eature.\\n\\n==', u'st and cent', u'oblems incl', u'aunched his', u'310)\\n\\n*&quo', u'character n', u'ward-winnin', u'ularly elec', u'expense of ', u\", '''Eyes W\", u';Hesperus i', u'sletter was', u'uction comp', u's. The conc', u'ow]] for mo', u'l football ', u' [[Hayek So', u'ed [[transl', u'amp>2006-03', u'ears, even ', u' plates for', u'[post punk]', u' equal righ', u'ng with onl', u'his English', u'lling the r', u's of Parlia', u' England.  ', u're being ma', u'endered in ', u'okomo]]\\n***', u'D&gt;&lt;FO', u'e natural r']\n",
      "[u'e leftist m', u' relationsh', u'n=&quot;7&q', u'p What is A', u'scipuous fe', u'ch lost Gau', u'rovided leg', u'the [[neocl', u'm to win wh', u' as the big', u'bruary, 200', u'ited States', u'shwasher, a', u'o sulfides.', u' Book of Mo', u'ontinued to', u'Orioles als', u'otharingia ', u' version of', u've function', u'by stand-al', u'in the Span', u'degrees. Be', u'y slow hang', u'y. To my su', u'campus buil', u' rice prepa', u'e]]; more r', u'reamlined h', u'to accusati', u'ny other re', u'=Phonology=', u'tral [[Asia', u'lude the [[', u's own label', u'ot;Those wh', u'named &quot', u'ng journali', u'ctronic sou', u' critical w', u\"Wide Shut''\", u'is Phosphor', u's abandoned', u'pany also m', u'cepts of [[', u'ore details', u' team|West ', u'ociety]], w', u'literation]', u'3-03T23:58:', u' decades, e', u'r maps of [', u']] bands li', u'hts to Cath', u'ly drums) f', u'h bride.  H', u'region. Mos', u'ament are l', u' [[Hans Hol', u'anufactured', u' revolution', u'*[[Purdue U', u'ONT SIZE=&q', u'rock was sc']\n",
      "[u'tu']\n",
      "[u'ur']\n"
     ]
    }
   ],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class adaptive_random(MODEL):\n",
    "    def layer(self, \n",
    "              inp_t,\n",
    "              state_t_minus_1,\n",
    "              memory_t_minus_1):\n",
    "        X_t = tf.concat(1, [inp_t,\n",
    "                            state_t_minus_1,\n",
    "                            memory_t_minus_1])\n",
    "        RES = tf.matmul(X_t, self.Matrix) + self.Bias\n",
    "        state_t = tf.tanh(RES)\n",
    "        return state_t\n",
    "\n",
    "    \n",
    "    def iteration(self, inp, state):\n",
    "        output = self.layer(inp,\n",
    "                            state[0],\n",
    "                            state[1])\n",
    "        trigger = tf.sigmoid(tf.matmul(tf.concat(1, [inp, output]), self.trigger_matrix) + self.trigger_bias)\n",
    "        memory_list = list()\n",
    "        current_batch_size = trigger.get_shape().as_list()[0]\n",
    "        for i in range(current_batch_size):\n",
    "            memory = tf.cond(tf.greater(trigger[i, 0], self.thresh),\n",
    "                             lambda: trigger[i, 0] * tf.slice(output, [i, 0], [1, self._num_nodes[0]]),\n",
    "                             lambda: tf.slice(state[1], [i, 0], [1, self._num_nodes[0]]))\n",
    "            memory_list.append(memory)\n",
    "        memory = tf.concat(0, memory_list) \n",
    "        return output, [output, memory], trigger\n",
    "    \n",
    "    def swap_iteration(self, inp, state):\n",
    "        output = self.layer(inp,\n",
    "                            state[0],\n",
    "                            state[1])\n",
    "        trigger = tf.sigmoid(tf.matmul(tf.concat(1, [inp, output]), self.trigger_matrix) + self.trigger_bias)\n",
    "        swap_prob = tf.constant(self._swap_prob) \n",
    "        swap = tf.less(tf.random_uniform([1])[0], swap_prob)\n",
    "        coef_output = tf.to_float(swap)\n",
    "        coef_memory = tf.constant(1.) - coef_output\n",
    "        \n",
    "        memory = tf.cond(swap,\n",
    "                         lambda: trigger*output,\n",
    "                         lambda: state[1])  \n",
    "        return output, [output, memory], trigger\n",
    "    \n",
    "    def unrollings(self, train_inputs, saved_state):\n",
    "        outputs = list()\n",
    "        state = saved_state\n",
    "        triggers = list()\n",
    "        for inp in train_inputs:\n",
    "            output, state, current_trigger = self.iteration(inp, state)\n",
    "            outputs.append(output)  \n",
    "            triggers.append(current_trigger)\n",
    "        return [tf.concat(0, outputs), tf.concat(0, state), tf.concat(0, triggers)]\n",
    "    \n",
    "    def swap_unrollings(self, train_inputs, saved_state):\n",
    "        outputs = list()\n",
    "        state = saved_state\n",
    "        triggers = list()\n",
    "        for inp in train_inputs:\n",
    "            output, state, current_trigger = self.swap_iteration(inp, state)\n",
    "            outputs.append(output) \n",
    "            triggers.append(current_trigger)\n",
    "        return [tf.concat(0, outputs), tf.concat(0, state), tf.concat(0, triggers)]\n",
    "\n",
    "    def optimizer_normal(self):\n",
    "        gradients, v = zip(*self.Optimizer.compute_gradients(self._loss,\n",
    "                                                             var_list=self.all_trainable[:-2]))\n",
    "        print('normal:\\nvar_list:', self.all_trainable[:-2], '\\nv:', v)\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        \"\"\"optimizer\"\"\"\n",
    "        optimizer_apply = self.Optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)  \n",
    "        return optimizer_apply\n",
    "    \n",
    "    def optimizer_with_trigger(self):\n",
    "        gradients, v = zip(*self.Optimizer.compute_gradients(self._loss,\n",
    "                                                             var_list=self.all_trainable))\n",
    "        print('trigger:\\nvar_list:', self.all_trainable, '\\nv:', v)\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        \"\"\"optimizer\"\"\"\n",
    "        optimizer_apply = self.Optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)  \n",
    "        return optimizer_apply\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_bias,\n",
    "                 threshhold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                 normal_run_prob,\n",
    "                 swap_prob,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 mean=0.,\n",
    "                 stddev='default',\n",
    "                 shift=0.,\n",
    "                 init_learning_rate=1.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_bias = init_bias\n",
    "        self._threshold = threshold\n",
    "        self._normal_run_prob = normal_run_prob\n",
    "        self._swap_prob = swap_prob\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        \n",
    "        \n",
    "        self._mean = mean\n",
    "        \n",
    "        self._stddev = list()\n",
    "        if stddev == 'default':\n",
    "            self._stddev = 1.0 * np.sqrt(1.3/(2*num_nodes[0] + vocabulary_size))\n",
    "        else:\n",
    "            self._stddev = stddev\n",
    "            \n",
    "        self._shift = shift\n",
    "        self._init_learning_rate = init_learning_rate\n",
    "        \n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_bias\": 8,\n",
    "                         \"threshold\": 9,\n",
    "                         \"normal_run_prob\": 10,\n",
    "                         \"swap_prob\": 11,\n",
    "                         \"memory_fine\":12,\n",
    "                         \"init_mean\": 13,\n",
    "                         \"init_stddev\": 14,\n",
    "                         \"init_shift\": 15,\n",
    "                         \"init_learning_rate\": 16,\n",
    "                         \"type\": 17}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with self._graph.device('/gpu:0'): \n",
    "                self.Matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + 2*self._num_nodes[0],\n",
    "                                                               self._num_nodes[0]],\n",
    "                                                              mean=self._mean, stddev=self._stddev))\n",
    "                self.Bias = tf.Variable([self._shift for _ in range(self._num_nodes[0])])\n",
    "\n",
    "                # classifier \n",
    "                weights = tf.Variable(tf.truncated_normal([self._num_nodes[-1], self._vocabulary_size], stddev = 0.1))\n",
    "                bias = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "                \n",
    "                self.trigger_matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + self._num_nodes[0], 1], stddev = 0.1))\n",
    "                self.trigger_bias = tf.Variable([self._init_bias])\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                self._train_data = list()\n",
    "                for _ in range(self._num_unrollings + 1):\n",
    "                    self._train_data.append(\n",
    "                        tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size]))\n",
    "                train_inputs = self._train_data[: self._num_unrollings]\n",
    "                train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                # Unrolled LSTM loop.\n",
    "\n",
    "                saved_state = [tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False),\n",
    "                               tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False)]\n",
    "                \n",
    "                \"\"\"global step\"\"\"\n",
    "                self._global_step = tf.Variable(0)\n",
    "                \n",
    "                normal_prob = tf.minimum(\n",
    "                    tf.constant(self._normal_run_prob['init']) + tf.to_float(self._global_step) / self._normal_run_prob['epochs'] * tf.constant(1. - self._normal_run_prob['init']),\n",
    "                    tf.constant(1.))\n",
    "                \"\"\"swap\"\"\"\n",
    "                swap = tf.greater(tf.random_uniform([1])[0], normal_prob) \n",
    "                \"\"\"self.thresh\"\"\"\n",
    "                if self._threshold['fixed']:\n",
    "                    self.thresh = tf.constant(self._threshold['min'])\n",
    "                else:\n",
    "                    thresh_range = self._threshold['max'] - self._threshold['min']\n",
    "                    self.thresh = tf.minimum(tf.constant(self._threshold['min']) + tf.to_float(self._global_step) / self._threshold['epochs'] * tf.constant(thresh_range), tf.constant(self._threshold['max']))\n",
    "                outputs = list()\n",
    "                state = saved_state\n",
    "                [outputs, state, current_trigger] = tf.cond(swap,\n",
    "                                                            lambda: self.swap_unrollings(train_inputs, state),\n",
    "                                                            lambda: self.unrollings(train_inputs, state))\n",
    "\n",
    "                state = tf.split(0, 2, state)\n",
    "                save_list = list()\n",
    "                \n",
    "                save_list.append(saved_state[0].assign(state[0]))\n",
    "                save_list.append(saved_state[1].assign(state[1]))\n",
    "                \n",
    "                \"\"\"skip operation\"\"\"\n",
    "                self._skip_operation = tf.group(*save_list)\n",
    "                \n",
    "                self.memory_fine = tf.placeholder(tf.float32)\n",
    "                mf = tf.minimum(tf.to_float(swap), self.memory_fine)\n",
    "                with tf.control_dependencies(save_list):\n",
    "                        # Classifier.\n",
    "                    logits = tf.nn.xw_plus_b(outputs, weights, bias)\n",
    "                    \"\"\"loss\"\"\"\n",
    "                    self._loss = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits, tf.concat(0, train_labels)))\n",
    "                    fact_loss = tf.reduce_mean(\n",
    "                                                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                                logits, tf.concat(0, train_labels))) + mf * tf.reduce_sum(current_trigger)\n",
    "\n",
    "                # Optimizer.\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                self._half_life = tf.placeholder(tf.int32)\n",
    "                self._decay = tf.placeholder(tf.float32)\n",
    "                \"\"\"learning rate\"\"\"\n",
    "                self._learning_rate = tf.train.exponential_decay(self._init_learning_rate,\n",
    "                                                                 self._global_step,\n",
    "                                                                 self._half_life,\n",
    "                                                                 self._decay,\n",
    "                                                                 staircase=True)\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                gradients, v = zip(*optimizer.compute_gradients(fact_loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                \"\"\"optimizer\"\"\"\n",
    "                self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                \"\"\"train prediction\"\"\"\n",
    "                self._train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                saved_sample_state = list()\n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size])\n",
    "\n",
    "                reset_list = list()\n",
    "                reset_list.append(saved_sample_state[0].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_state[1].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "\n",
    "                \"\"\"reset sample state\"\"\"\n",
    "                self._reset_sample_state = tf.group(*reset_list)\n",
    "                \n",
    "                \"\"\"trigger\"\"\"\n",
    "                sample_output, sample_state, self.trigger = self.iteration(self._sample_input, saved_sample_state)\n",
    "\n",
    "                sample_save_list = list()\n",
    "                sample_save_list.append(saved_sample_state[0].assign(sample_state[0]))\n",
    "                sample_save_list.append(saved_sample_state[1].assign(sample_state[1]))\n",
    "\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    \"\"\"sample prediction\"\"\"\n",
    "                    self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, weights, bias)) \n",
    "                \n",
    "                \n",
    "                \"\"\"saver\"\"\"\n",
    "                self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                            \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations, memory_fine):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append(dict(self._threshold))\n",
    "        metadata.append(self._normal_run_prob)\n",
    "        metadata.append(self._swap_prob)\n",
    "        metadata.append(memory_fine)\n",
    "        metadata.append(self._mean)\n",
    "        metadata.append(self._stddev)\n",
    "        metadata.append(self._shift)\n",
    "        metadata.append(self._init_learning_rate)\n",
    "        metadata.append('adaptive_random')\n",
    "        return metadata\n",
    "  \n",
    "    def get_triggers(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        trigger_list = list()\n",
    "        collect_triggers = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_triggers: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    t_list = list()\n",
    "                    collect_triggers = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                t_list.append(self.trigger.eval({self._sample_input: b[0]}))\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_triggers = False\n",
    "                    trigger_list.append(t_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, trigger_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = {'fixed': True, 'min': 0., 'max': 0.7, 'epochs': 10000}\n",
    "normal_run_prob = {'init': 0.1, 'epochs': 10000}\n",
    "\n",
    "model = adaptive_random(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 1,\n",
    "                 [128],\n",
    "                 0.,\n",
    "                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                 normal_run_prob,\n",
    "                        0.1,\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.283017 learning rate: 1.000000\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "a\"e-Ua+§ÍÏ£¼4S¯cs8 +¶×sP²ehVu÷ÞÑ¬øü¬°¤Û¬cê*Ø$¤ h¥6Û0!\tph^¡]~þ/ì°ðûûôXêOýÄ©?æÆU\n",
      "ß.=Æ¾yccÒúD]9Ê¸þÝfVô:5c5Ñj½ÑbèÂß¾,¤:ÔýóJ÷9Õ@¡õà#¢&Ã®}Â«âjªá^4Ð½·ï¼4=î²6MÐÎ£fR\n",
      "«´h6ßêêZÅõy¡cMýÿ,º²iE¦³¶ë£¼Ä&m)Ú 5~»£¼ã1ÛWÍç\tj$/t«aÅÒióÔ7ãúM­v3°÷eVZ-ñ2AtNz³Æ-Àµ\n",
      "Êp³64ÌÖçþ¸:lÂ§O>ê\n",
      "r[6,\n",
      "¹«kù¬UÙªÖÒ@2K­ø@(àTMJÛ­¢su«ÍÁÅæÈÉ$åyÆnÍ×¾1KÐ½!gK¹!ÞÕb\n",
      "Ud\n",
      "(ß½¢©'DÙîys¡8G´ÑÙt7Í¸h×O(­u£åyÔPt¦ Ý,OZ¨æ®\\AoãrtDßáI<»¿Iú/í®ïÀaìjØ2 Ý;½ZínôÃå\n",
      "================================================================================\n",
      "Validation percentage of correct: 12.65%\n",
      "\n",
      "Average loss at step 200: 3.353818 learning rate: 1.000000\n",
      "Percentage_of correct: 17.87%\n",
      "Validation percentage of correct: 26.36%\n",
      "\n",
      "Average loss at step 400: 2.777325 learning rate: 0.900000\n",
      "Percentage_of correct: 27.77%\n",
      "Validation percentage of correct: 30.56%\n",
      "\n",
      "Average loss at step 600: 2.593143 learning rate: 0.900000\n",
      "Percentage_of correct: 31.13%\n",
      "Validation percentage of correct: 33.52%\n",
      "\n",
      "Average loss at step 800: 2.472961 learning rate: 0.810000\n",
      "Percentage_of correct: 34.30%\n",
      "Validation percentage of correct: 33.90%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3eb48060d638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             print_intermediate_results = True)\n\u001b[0m",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_stairs, decay, train_frequency, min_num_points, stop_percent, num_train_points_per_1_validation_point, averaging_number, optional_feed_dict, print_intermediate_results, half_life_fixed, add_operation, print_steps, fuse_texts)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_frequency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_frequency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maveraging_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maverage_summing_started\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                     \u001b[0maverage_percentage_of_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpercent_of_correct_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0maverage_summing_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mpercent_of_correct_predictions\u001b[0;34m(predictions, labels)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_characters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_correct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_characters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optional_feed_dict = {'self.memory_fine': 0.0000001}\n",
    "model.run(30,\n",
    "          0.9,\n",
    "            200,\n",
    "            50,\n",
    "            3,\n",
    "            1,\n",
    "            20,\n",
    "          optional_feed_dict=optional_feed_dict,\n",
    "            print_intermediate_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       threshold:  0.5\n",
      "memory fine:  1e-05\n",
      "Number of steps = 80001     Percentage = 50.50%     Time = 34046s     Learning rate = 0.0424\n",
      "memory fine:  6.32911392405e-06\n",
      "Number of steps = 80001     Percentage = 50.46%     Time = 34459s     Learning rate = 0.0424\n",
      "memory fine:  4.00576830636e-06\n",
      "Number of steps = 80001     Percentage = 50.26%     Time = 33355s     Learning rate = 0.0424\n",
      "memory fine:  2.53529639643e-06\n",
      "Number of steps = 80001     Percentage = 50.35%     Time = 35203s     Learning rate = 0.0424\n",
      "memory fine:  1.60461797242e-06\n",
      "Number of steps = 80001     Percentage = 50.37%     Time = 33721s     Learning rate = 0.0424\n",
      "memory fine:  1.01558099521e-06\n",
      "Number of steps = 80001     Percentage = 50.17%     Time = 33607s     Learning rate = 0.0424\n",
      "memory fine:  6.42772781776e-07\n",
      "Number of steps = 80001     Percentage = 50.37%     Time = 33598s     Learning rate = 0.0424\n",
      "memory fine:  4.06818216314e-07\n",
      "Number of steps = 80001     Percentage = 50.46%     Time = 34925s     Learning rate = 0.0424\n",
      "memory fine:  2.57479883743e-07\n",
      "Number of steps = 80001     Percentage = 50.46%     Time = 33677s     Learning rate = 0.0424\n",
      "memory fine:  1.62961951736e-07\n",
      "Number of steps = 80001     Percentage = 50.39%     Time = 34064s     Learning rate = 0.0424\n",
      "memory fine:  1.03140475782e-07\n",
      "Number of steps = 80001     Percentage = 50.45%     Time = 35147s     Learning rate = 0.0424\n",
      "memory fine:  6.52787821407e-08\n",
      "Number of steps = 80001     Percentage = 50.50%     Time = 35372s     Learning rate = 0.0424\n",
      "memory fine:  4.13156848992e-08\n",
      "Number of steps = 80001     Percentage = 50.47%     Time = 34881s     Learning rate = 0.0424\n",
      "memory fine:  2.61491676577e-08\n",
      "Number of steps = 80001     Percentage = 50.55%     Time = 35152s     Learning rate = 0.0424\n",
      "memory fine:  1.65501061125e-08\n",
      "Number of steps = 80001     Percentage = 50.60%     Time = 33596s     Learning rate = 0.0424\n",
      "memory fine:  1.04747507041e-08\n",
      "Number of steps = 80001     Percentage = 50.46%     Time = 35693s     Learning rate = 0.0424\n",
      "memory fine:  6.62958905322e-09\n",
      "Number of steps = 80001     Percentage = 50.35%     Time = 33872s     Learning rate = 0.0424\n",
      "memory fine:  4.19594243875e-09\n",
      "Number of steps = 80001     Percentage = 50.70%     Time = 35492s     Learning rate = 0.0424\n",
      "memory fine:  2.65565977136e-09\n",
      "Number of steps = 80001     Percentage = 50.38%     Time = 33738s     Learning rate = 0.0424\n",
      "memory fine:  1.68079732365e-09\n",
      "Number of steps = 80001     Percentage = 50.30%     Time = 34426s     Learning rate = 0.0424\n"
     ]
    }
   ],
   "source": [
    "normal_run_prob = {'init': 0.1, 'epochs': 79000}\n",
    "\n",
    "threshold_values = [0.5]\n",
    "reduce_coef = 1.58\n",
    "num_iter = 20\n",
    "memory_fine_value = 0.00001\n",
    "threshold = {'fixed': True, 'min': 0.2, 'max': 0.7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "for threshold_value in threshold_values:\n",
    "    print(\"\\n\", ' '*5, \"threshold: \", threshold_value)\n",
    "    threshold['min'] = threshold_value\n",
    "    for _ in range(num_iter):\n",
    "        print(\"memory fine: \", memory_fine_value)\n",
    "        optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "        model = adaptive_random(64,\n",
    "                         vocabulary,\n",
    "                         characters_positions_in_vocabulary,\n",
    "                         30,\n",
    "                         1,\n",
    "                         [128],\n",
    "                         0.,\n",
    "                         threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                                normal_run_prob,\n",
    "                                0.1,\n",
    "                         train_text,\n",
    "                         valid_text)\n",
    "        model.simple_run(100,\n",
    "                       'adaptive_random/init_bias_0/variables/swpr0.1_th%s_mf%s' % (threshold_value, memory_fine_value),\n",
    "                       80000,\n",
    "                       4000,\n",
    "                       5000,        #learning has a chance to be stopped after every block of steps\n",
    "                       30,\n",
    "                       0.9,\n",
    "                       3,\n",
    "                       optional_feed_dict=optional_feed_dict)\n",
    "        results_GL.extend(model._results)\n",
    "        model.destroy()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        memory_fine_value /= reduce_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'adaptive_random'\n",
    "file_name = 'adaptive_random_ns_80000_hl_2667_dc_0.9_ib0_ilr1._th0.5_mf1e-5_1.6e-9.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'adaptive_random'\n",
    "pickle_file = 'adaptive_random_ns_80000_hl_2667_dc_0.9_ib0_ilr1._th0.5_mf1e-5_1.6e-9.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "\n",
    "adaptive_random_plots = ComparePlots('adaptive_random')\n",
    "adaptive_random_plots.add_network(results_GL, model._indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEkCAYAAACPCFMiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxYeZmZmVlYsPMzMzK6v/ByWbNJUoeG7oAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7273899c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_options = {'x': 'log'}\n",
    "plot_data, _ = adaptive_random_plots.one_key_layout_data('adaptive_random_1',\n",
    "                                         'memory_fine',\n",
    "                                         \"threshold['min']\")\n",
    "adaptive_random_plots.save_layout(plot_data[0],\n",
    "                    'memory fine effect (half life: 2667, decay: 0.9)',\n",
    "                    ['memory_fine_effect', 'plots'],\n",
    "                    'nn128;ns80000;hl2667;dc0.9',\n",
    "                              plot_options=plot_options)\n",
    "adaptive_random_plots.draw(plot_data[0], 'memory fine effect (half life: 2667, decay: 0.9)', plot_options=plot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       threshold:  0.5\n",
      "memory fine:  1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory fine:  6.32911392405e-06\n",
      "memory fine:  4.00576830636e-06\n",
      "memory fine:  2.53529639643e-06\n",
      "memory fine:  1.60461797242e-06\n",
      "memory fine:  1.01558099521e-06\n",
      "memory fine:  6.42772781776e-07\n",
      "memory fine:  4.06818216314e-07\n",
      "memory fine:  2.57479883743e-07\n",
      "memory fine:  1.62961951736e-07\n",
      "memory fine:  1.03140475782e-07\n",
      "memory fine:  6.52787821407e-08\n",
      "memory fine:  4.13156848992e-08\n",
      "memory fine:  2.61491676577e-08\n",
      "memory fine:  1.65501061125e-08\n",
      "memory fine:  1.04747507041e-08\n",
      "memory fine:  6.62958905322e-09\n",
      "memory fine:  4.19594243875e-09\n",
      "memory fine:  2.65565977136e-09\n",
      "memory fine:  1.68079732365e-09\n"
     ]
    }
   ],
   "source": [
    "normal_run_prob = {'init': 0.1, 'epochs': 79000}\n",
    "threshold_values = [0.5]\n",
    "memory_fine_values = [(0.00001 / 1.58**i) for i in range(20)]\n",
    "threshold = {'fixed': True, 'min': 0.2, 'max': 0.7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "for threshold_value in threshold_values:\n",
    "    print(\"\\n\", ' '*5, \"threshold: \", threshold_value)\n",
    "    threshold['min'] = threshold_value\n",
    "    for memory_fine_value in memory_fine_values:\n",
    "        print(\"memory fine: \", memory_fine_value)\n",
    "        model = adaptive_random(64,\n",
    "                         vocabulary,\n",
    "                         characters_positions_in_vocabulary,\n",
    "                         30,\n",
    "                         1,\n",
    "                         [128],\n",
    "                         0.,\n",
    "                         threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                                normal_run_prob,\n",
    "                                0.07,\n",
    "                         train_text,\n",
    "                         valid_text)\n",
    "        text_list, trigger_list = model.run_for_analitics(model.get_triggers,\n",
    "                                                        'adaptive_random/init_bias_0/variables/ep40k_th%s_mf%s' % (threshold_value, memory_fine_value),\n",
    "                                                        [300, 75, None])\n",
    "        triggers = list()\n",
    "        for text_number, text in enumerate(text_list):\n",
    "            trig = list()\n",
    "\n",
    "            text_triggers = trigger_list[text_number]\n",
    "            for text_trigger in text_triggers:\n",
    "                trig.append(text_trigger[0, 0])\n",
    "            triggers.append(trig)\n",
    "        structure_vocabulary_plots(text_list,\n",
    "                                   triggers,\n",
    "                                   'triggers for letter position (threshold %s, memory fine %.2e)' % (threshold_value, memory_fine_value),\n",
    "                                   'mean trigger',\n",
    "                                   ['adaptive_random', 'triggers(new_graphs)', 'init_bias0.', 'vocabulary_plots'],\n",
    "                                   'mean_triggers128_ib0_th%s_mf%.2e' % (threshold_value, memory_fine_value),\n",
    "                                   ylims=[0., 1.],\n",
    "                                   ylims_fixed=True,\n",
    "                                   threshold=threshold_value,\n",
    "                                   show=False)\n",
    "        for i in range(99):\n",
    "            text_plot(text_list[i],\n",
    "                      triggers[i],\n",
    "                      'trigger',\n",
    "                      'triggers (threshold %s, memory fine %s)' % (threshold_value, memory_fine_value),\n",
    "                      ['adaptive_random', 'text_plots(new_graphs)', 'init_bias0.', 'th%s_mf%s' % (threshold_value, memory_fine_value)],\n",
    "                      'triggers128_ib0_th%s_mf%s#%s' % (threshold_value, memory_fine_value, i),\n",
    "                      threshold=threshold_value,\n",
    "                      show=False)\n",
    "        model.destroy()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
