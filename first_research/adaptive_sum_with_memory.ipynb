{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not one byte characters:  0\n",
      "min order index:  9\n",
      "max order index:  255\n",
      "total number of characters:  196\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99337500 side of the leftist milieu. It often focuses on the individual r\n",
      "22500 ture in Mutual Aid: A Factor of Evolution (1897). Subsequent ana\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 22500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ\n",
      "char2id(u'a') = 67,  char2id(u'z') = 92,  char2id(u' ') = 2\n",
      "id2char(78) = l,  id2char(156) = Ø,  id2char(140) = È\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'side of the', u'though the ', u'; \\n|colspan', u'ustrian.asp', u'e most cons', u' Gaul, whic', u'h can be pr', u'pitome of t', u' first team', u'Roussimoff ', u'col. In Feb', u's]] - [[Uni', u'g oven, dis', u' exposed to', u' wrote the ', u'ird.  He co', u'a]].  The O', u'nd Lower Lo', u'In another ', u'or sensitiv', u'ents used b', u'e Chile]] i', u\"achelor's d\", u'till employ', u'flexibility', u'nects all c', u'variety of ', u' [[Cheshire', u' sleek, str', u' have led t', u'ongside man', u'eature.\\n\\n==', u'st and cent', u'oblems incl', u'aunched his', u'310)\\n\\n*&quo', u'character n', u'ward-winnin', u'ularly elec', u'expense of ', u\", '''Eyes W\", u';Hesperus i', u'sletter was', u'uction comp', u's. The conc', u'ow]] for mo', u'l football ', u' [[Hayek So', u'ed [[transl', u'amp>2006-03', u'ears, even ', u' plates for', u'[post punk]', u' equal righ', u'ng with onl', u'his English', u'lling the r', u's of Parlia', u' England.  ', u're being ma', u'endered in ', u'okomo]]\\n***', u'D&gt;&lt;FO', u'e natural r']\n",
      "[u'e leftist m', u' relationsh', u'n=&quot;7&q', u'p What is A', u'scipuous fe', u'ch lost Gau', u'rovided leg', u'the [[neocl', u'm to win wh', u' as the big', u'bruary, 200', u'ited States', u'shwasher, a', u'o sulfides.', u' Book of Mo', u'ontinued to', u'Orioles als', u'otharingia ', u' version of', u've function', u'by stand-al', u'in the Span', u'degrees. Be', u'y slow hang', u'y. To my su', u'campus buil', u' rice prepa', u'e]]; more r', u'reamlined h', u'to accusati', u'ny other re', u'=Phonology=', u'tral [[Asia', u'lude the [[', u's own label', u'ot;Those wh', u'named &quot', u'ng journali', u'ctronic sou', u' critical w', u\"Wide Shut''\", u'is Phosphor', u's abandoned', u'pany also m', u'cepts of [[', u'ore details', u' team|West ', u'ociety]], w', u'literation]', u'3-03T23:58:', u' decades, e', u'r maps of [', u']] bands li', u'hts to Cath', u'ly drums) f', u'h bride.  H', u'region. Mos', u'ament are l', u' [[Hans Hol', u'anufactured', u' revolution', u'*[[Purdue U', u'ONT SIZE=&q', u'rock was sc']\n",
      "[u'tu']\n",
      "[u'ur']\n"
     ]
    }
   ],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class adaptive_sum_with_memory(MODEL):\n",
    "    def layer(self, \n",
    "              inp_t,\n",
    "              state_t_minus_1,\n",
    "              memory_t_minus_1):\n",
    "        X_t = tf.concat(1, [inp_t,\n",
    "                            state_t_minus_1,\n",
    "                            memory_t_minus_1])\n",
    "        RES = tf.matmul(X_t, self.Matrix) + self.Bias\n",
    "        state_t = tf.tanh(RES)\n",
    "        return state_t\n",
    "    \n",
    "    def swap(self, output, i):\n",
    "        return [tf.slice(output, [i, 0], [1, self._num_nodes[0]]), tf.constant(0., shape=[1, 1])]\n",
    "    \n",
    "    def skip_swap(self, memory, i, current_trigger):\n",
    "        return [tf.slice(memory, [i, 0], [1, self._num_nodes[0]]), tf.slice(current_trigger, [i, 0], [1, 1])]\n",
    "\n",
    "    \n",
    "    def iteration(self, inp, state, current_trigger):\n",
    "        output = self.layer(inp,\n",
    "                            state[0],\n",
    "                            state[1])\n",
    "        trigger = tf.sigmoid(tf.matmul(tf.concat(1, [inp, output, state[1]]), self.trigger_matrix) + self.trigger_bias)\n",
    "        current_trigger = current_trigger + trigger\n",
    "        trigger_for_plot = current_trigger\n",
    "        memory_list = list()\n",
    "        trigger_list = list()\n",
    "        current_batch_size = trigger.get_shape().as_list()[0]\n",
    "        ones = tf.ones([current_batch_size])\n",
    "        to_swap = tf.to_float(tf.greater(tf.reshape(current_trigger, [-1]), self.thresh)) \n",
    "        memory = tf.transpose( tf.transpose(trigger)*tf.transpose(output)*to_swap + tf.transpose(state[1]) * (ones - to_swap))\n",
    "        current_trigger = tf.transpose( tf.transpose(current_trigger) * (ones - to_swap))\n",
    "        return output, [output, memory], trigger, current_trigger, trigger_for_plot\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_bias,\n",
    "                 threshhold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 mean=0.,\n",
    "                 stddev='default',\n",
    "                 shift=0.,\n",
    "                 init_learning_rate=1.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = list(vocabulary)\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = list(num_nodes)\n",
    "        self._init_bias = init_bias\n",
    "        self._threshold = dict(threshold)\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        \n",
    "        \n",
    "        self._mean = mean\n",
    "        \n",
    "        self._stddev = list()\n",
    "        if stddev == 'default':\n",
    "            self._stddev = 1.0 * np.sqrt(1./(num_nodes[0] + vocabulary_size))\n",
    "        else:\n",
    "            self._stddev = stddev\n",
    "            \n",
    "        self._shift = shift\n",
    "        self._init_learning_rate = init_learning_rate\n",
    "        \n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_bias\": 8,\n",
    "                         \"threshold\": 9,\n",
    "                         \"memory_fine\":10,\n",
    "                         \"init_mean\": 11,\n",
    "                         \"init_stddev\": 12,\n",
    "                         \"init_shift\": 13,\n",
    "                         \"init_learning_rate\": 14,\n",
    "                         \"type\": 15}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with self._graph.device('/gpu:0'): \n",
    "                self.Matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + 2*self._num_nodes[0],\n",
    "                                                               self._num_nodes[0]],\n",
    "                                                              mean=self._mean, stddev=self._stddev))\n",
    "                self.Bias = tf.Variable([self._shift for _ in range(self._num_nodes[0])])\n",
    "\n",
    "                # classifier \n",
    "                weights = tf.Variable(tf.truncated_normal([self._num_nodes[-1], self._vocabulary_size], stddev = 0.1))\n",
    "                bias = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "                \n",
    "                self.trigger_matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + 2*self._num_nodes[0], 1], stddev = 0.1))\n",
    "                self.trigger_bias = tf.Variable([self._init_bias])\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                self._train_data = list()\n",
    "                for _ in range(self._num_unrollings + 1):\n",
    "                    self._train_data.append(\n",
    "                        tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size]))\n",
    "                train_inputs = self._train_data[: self._num_unrollings]\n",
    "                train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                # Unrolled LSTM loop.\n",
    "\n",
    "                saved_state = [tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False),\n",
    "                               tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False)]\n",
    "                \n",
    "                \"\"\"TRIGGER\"\"\"\n",
    "                saved_trigger = tf.Variable(tf.zeros([self._batch_size, 1]))\n",
    "                \"\"\"global step\"\"\"\n",
    "                self._global_step = tf.Variable(0)\n",
    "                \"\"\"self.thresh\"\"\"\n",
    "                if self._threshold['fixed']:\n",
    "                    self.thresh = tf.constant(self._threshold['min'])\n",
    "                else:\n",
    "                    thresh_range = self._threshold['max'] - self._threshold['min']\n",
    "                    self.thresh = tf.minimum(tf.constant(self._threshold['min']) + tf.to_float(self._global_step) / self._threshold['epochs'] * tf.constant(thresh_range), tf.constant(self._threshold['max']))\n",
    "                outputs = list()\n",
    "                triggers_for_letters =  list()\n",
    "                state = saved_state\n",
    "                current_trigger = saved_trigger\n",
    "                for inp in train_inputs:\n",
    "                    output, state, trigger_for_letter, current_trigger, _ = self.iteration(inp, state, current_trigger)\n",
    "                    outputs.append(output)\n",
    "                    triggers_for_letters.append(trigger_for_letter)\n",
    "\n",
    "                save_list = list()\n",
    "                save_list.append(saved_state[0].assign(state[0]))\n",
    "                save_list.append(saved_state[1].assign(state[1]))\n",
    "                save_list.append(saved_trigger.assign(current_trigger))\n",
    "                \n",
    "                \"\"\"skip operation\"\"\"\n",
    "                self._skip_operation = tf.group(*save_list)\n",
    "                \n",
    "                self.memory_fine = tf.placeholder(tf.float32)\n",
    "                with tf.control_dependencies(save_list):\n",
    "                        # Classifier.\n",
    "                    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), weights, bias)\n",
    "                    \"\"\"loss\"\"\"\n",
    "                    self._loss = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits, tf.concat(0, train_labels)))\n",
    "                    fact_loss = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits, tf.concat(0, train_labels))) + self.memory_fine * tf.reduce_sum(current_trigger)\n",
    "                # Optimizer.\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                self._half_life = tf.placeholder(tf.int32)\n",
    "                self._decay = tf.placeholder(tf.float32)\n",
    "                \"\"\"learning rate\"\"\"\n",
    "                self._learning_rate = tf.train.exponential_decay(0.5,\n",
    "                                                                 self._global_step,\n",
    "                                                                 self._half_life,\n",
    "                                                                 self._decay,\n",
    "                                                                 staircase=True)\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                gradients, v = zip(*optimizer.compute_gradients(fact_loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                \"\"\"optimizer\"\"\"\n",
    "                self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                \"\"\"train prediction\"\"\"\n",
    "                self._train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                saved_sample_state = list()\n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                saved_sample_trigger = tf.Variable(tf.zeros([1, 1]))\n",
    "                \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size])\n",
    "\n",
    "                reset_list = list()\n",
    "                reset_list.append(saved_sample_state[0].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_state[1].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_trigger.assign(tf.zeros([1, 1])))\n",
    "\n",
    "                \"\"\"reset sample state\"\"\"\n",
    "                self._reset_sample_state = tf.group(*reset_list)\n",
    "                \n",
    "                \"\"\"trigger\"\"\"\n",
    "                sample_output, sample_state, self.local_trigger, trigger, self.trigger_for_plot = self.iteration(self._sample_input,\n",
    "                                                                                                                 saved_sample_state,\n",
    "                                                                                                                 saved_sample_trigger)\n",
    "\n",
    "                sample_save_list = list()\n",
    "                sample_save_list.append(saved_sample_state[0].assign(sample_state[0]))\n",
    "                sample_save_list.append(saved_sample_state[1].assign(sample_state[1]))\n",
    "                sample_save_list.append(saved_sample_trigger.assign(trigger))\n",
    "\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    \"\"\"sample prediction\"\"\"\n",
    "                    self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, weights, bias)) \n",
    "                \n",
    "                \n",
    "                \"\"\"saver\"\"\"\n",
    "                self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                            \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations, optional_feed_dict):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(list(self._num_nodes))\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append(dict(self._threshold))\n",
    "        metadata.append(optional_feed_dict['self.memory_fine'])\n",
    "        metadata.append(self._mean)\n",
    "        metadata.append(self._stddev)\n",
    "        metadata.append(self._shift)\n",
    "        metadata.append(self._init_learning_rate)\n",
    "        metadata.append('adaptive_sum')\n",
    "        return metadata\n",
    "  \n",
    "    def get_triggers(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        trigger_list = list()\n",
    "        local_trigger_list = list()\n",
    "        collect_triggers = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_triggers: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    t_list = list()\n",
    "                    l_t_list = list()\n",
    "                    collect_triggers = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                t_list.append(self.trigger_for_plot.eval({self._sample_input: b[0]}))\n",
    "                l_t_list.append(self.local_trigger.eval({self._sample_input: b[0]}))\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_triggers = False\n",
    "                    trigger_list.append(t_list)\n",
    "                    local_trigger_list.append(l_t_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, trigger_list, local_trigger_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = {'fixed': True, 'min': 0.5, 'max': 0.7, 'epochs': 10000}\n",
    "\n",
    "model = adaptive_sum_with_memory(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 1,\n",
    "                 [128],\n",
    "                 0.,\n",
    "                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.279430 learning rate: 0.500000\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "\n",
      "Ú¯¤À+zí\n",
      "'·Û©»¬tpXµ.SX6o®¬e§wÃô@ûªúõ©W×\"VÎ¨F«:p£B£ÿu©.Ü«ÞÚ#Uþ øèÔI¿bùób%»à'h!vÂÎ\n",
      "H\tÒÐÂ`tþ}¼ºï!t².ÚhÑHÊIqydB`ËËÃÈ<@mwK®dP_¹O(kôU(*ÀÚzYý³K=Úë!Ú­ZÚ)Ò×[NN¸oÃ:þ²ãAw\n",
      "8i]bé%4ïö4 [O¼´l3H¾'¼ä%ª©B=ÞHÉC°CÔ½WÆ,Çí)å÷Êd(j§Å¸ÛV÷Cò*´eRsô4S¨,7)£Õ6ÏûfíÁMMn~\n",
      "3ðøt¤I2V~Æð·úuµ(\n",
      "9<£c\\ôöT:ÜkäöRû Éòô_T$+¬{Æ½|Û¨Õz>}©yéHp:kgÀ¼é qÂûÖÆV#ôN#Õ®}5!»\n",
      "¯bÓ§ÐÒ:vkLll-øxí'Û\\7BëPýnèúûi¿ç3¯ùO+Cbck!Í©!´+ý¥<,RÄÌêdµµ$Lï´ÊSâËæUl.\n",
      "üÖN¯yôf®\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9ce07fa0fd06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             print_intermediate_results = True)\n\u001b[0m",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_stairs, decay, train_frequency, min_num_points, stop_percent, num_train_points_per_1_validation_point, averaging_number, optional_feed_dict, print_intermediate_results, half_life_fixed, add_operations, add_text_operations, print_steps, validation_add_operations, num_validation_prints, validation_example_length, fuse_texts)\u001b[0m\n\u001b[1;32m    662\u001b[0m                                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valid_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                                 validation_result = session.run(validation_operations,\n\u001b[0;32m--> 664\u001b[0;31m                                                                 {self._sample_input: b[0]})\n\u001b[0m\u001b[1;32m    665\u001b[0m                                 \u001b[0mvalidation_percentage_of_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpercent_of_correct_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mprint_intermediate_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optional_feed_dict = {'self.memory_fine': 0.000001}\n",
    "model.run(30,\n",
    "          0.9,\n",
    "            200,\n",
    "            50,\n",
    "            3,\n",
    "            1,\n",
    "            20,\n",
    "          optional_feed_dict=optional_feed_dict,\n",
    "            print_intermediate_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps = 10000     Percentage = 41.85%     Time = 618s     Learning rate = 0.0212\n"
     ]
    }
   ],
   "source": [
    "optional_feed_dict = {'self.memory_fine': 0.0000001}\n",
    "model.simple_run(100,\n",
    "                       'adaptive_sum_with_memory/test',\n",
    "                       10000,\n",
    "                       400,\n",
    "                       5000,        #learning has a chance to be stopped after every block of steps\n",
    "                       30,\n",
    "                       0.9,\n",
    "                       3,\n",
    "                       optional_feed_dict=optional_feed_dict,\n",
    "                        fixed_num_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    }
   ],
   "source": [
    "text_list, trigger_list, local_trigger_list = model.run_for_analitics(model.get_triggers,\n",
    "                                                        'adaptive_sum_with_memory/test',\n",
    "                                                        [300, 75, None])\n",
    "triggers = list()\n",
    "local_triggers = list()\n",
    "for text_number, text in enumerate(text_list):\n",
    "    trig = list()\n",
    "    l_trig = list()\n",
    "    text_triggers = trigger_list[text_number]\n",
    "    text_local_triggers = local_trigger_list[text_number]\n",
    "    for text_trigger, text_local_trigger in zip(text_triggers, text_local_triggers):\n",
    "        trig.append(text_trigger[0, 0])\n",
    "        l_trig.append(text_local_trigger[0, 0])\n",
    "    triggers.append(trig)\n",
    "    local_triggers.append(l_trig)\n",
    "structure_vocabulary_plots(text_list,\n",
    "                            triggers,\n",
    "                            'triggers',\n",
    "                            'mean trigger',\n",
    "                            ['adaptive_sum_with_memory', 'test_folder'],\n",
    "                            'triggers',\n",
    "                            ylims=[0., 1.],\n",
    "                            ylims_fixed=True,\n",
    "                            threshold=.5,\n",
    "                            show=False)\n",
    "structure_vocabulary_plots(text_list,\n",
    "                            local_triggers,\n",
    "                            'local_triggers',\n",
    "                            'mean trigger',\n",
    "                            ['adaptive_sum_with_memory', 'test_folder'],\n",
    "                            'local_triggers',\n",
    "                            ylims=[0., 1.],\n",
    "                            ylims_fixed=True,\n",
    "                            threshold=.5,\n",
    "                            show=False)\n",
    "for i in range(99):\n",
    "    text_plot(text_list[i],\n",
    "                      triggers[i],\n",
    "                      'trigger',\n",
    "                      'triggers',\n",
    "                      ['adaptive_sum_with_memory', 'test_folder', 'text_triggers'],\n",
    "                      'triggers#%s' % i,\n",
    "                      threshold=.5,\n",
    "                      show=False, \n",
    "              sum_flag=True)\n",
    "\n",
    "    text_plot(text_list[i],\n",
    "                      local_triggers[i],\n",
    "                      'local_trigger',\n",
    "                      'local_triggers',\n",
    "                      ['adaptive_sum_with_memory', 'test_folder', 'text_local_triggers'],\n",
    "                      'local_triggers#%s' % i,\n",
    "                      threshold=.5,\n",
    "                      show=False, \n",
    "              sum_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     threshold:  0.9\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "Number of steps = 40000     Percentage = 48.70%     Time = 2455s     Learning rate = 0.0212\n",
      "init bias:  0.5\n",
      "Number of steps = 40000     Percentage = 48.49%     Time = 2446s     Learning rate = 0.0212\n",
      "init bias:  1.0\n",
      "Number of steps = 40000     Percentage = 48.44%     Time = 2432s     Learning rate = 0.0212\n",
      "init bias:  2.0\n",
      "Number of steps = 40000     Percentage = 48.73%     Time = 2417s     Learning rate = 0.0212\n",
      "init bias:  5.0\n",
      "Number of steps = 40000     Percentage = 48.55%     Time = 2417s     Learning rate = 0.0212\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "Number of steps = 40000     Percentage = 48.67%     Time = 2431s     Learning rate = 0.0212\n",
      "init bias:  0.5\n",
      "Number of steps = 40000     Percentage = 48.81%     Time = 2419s     Learning rate = 0.0212\n",
      "init bias:  1.0\n",
      "Number of steps = 40000     Percentage = 48.82%     Time = 2433s     Learning rate = 0.0212\n",
      "init bias:  2.0\n",
      "Number of steps = 40000     Percentage = 48.66%     Time = 2418s     Learning rate = 0.0212\n",
      "init bias:  5.0\n",
      "Number of steps = 40000     Percentage = 48.62%     Time = 2416s     Learning rate = 0.0212\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "Number of steps = 40000     Percentage = 48.66%     Time = 2421s     Learning rate = 0.0212\n",
      "init bias:  0.5\n",
      "Number of steps = 40000     Percentage = 48.31%     Time = 2420s     Learning rate = 0.0212\n",
      "init bias:  1.0\n",
      "Number of steps = 40000     Percentage = 48.75%     Time = 2422s     Learning rate = 0.0212\n",
      "init bias:  2.0\n",
      "Number of steps = 40000     Percentage = 48.49%     Time = 2422s     Learning rate = 0.0212\n",
      "init bias:  5.0\n",
      "Number of steps = 40000     Percentage = 48.56%     Time = 2434s     Learning rate = 0.0212\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "Number of steps = 40000     Percentage = 48.50%     Time = 2440s     Learning rate = 0.0212\n",
      "init bias:  0.5\n",
      "Number of steps = 40000     Percentage = 48.69%     Time = 2428s     Learning rate = 0.0212\n",
      "init bias:  1.0\n",
      "Number of steps = 40000     Percentage = 48.68%     Time = 2427s     Learning rate = 0.0212\n",
      "init bias:  2.0\n",
      "Number of steps = 40000     Percentage = 48.44%     Time = 2427s     Learning rate = 0.0212\n",
      "init bias:  5.0\n",
      "Number of steps = 40000     Percentage = 48.52%     Time = 2435s     Learning rate = 0.0212\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "Number of steps = 40000     Percentage = 48.50%     Time = 2481s     Learning rate = 0.0212\n",
      "init bias:  0.5\n",
      "Number of steps = 40000     Percentage = 48.44%     Time = 2489s     Learning rate = 0.0212\n",
      "init bias:  1.0\n",
      "Number of steps = 40000     Percentage = 48.55%     Time = 2547s     Learning rate = 0.0212\n",
      "init bias:  2.0\n",
      "Number of steps = 40000     Percentage = 48.52%     Time = 2432s     Learning rate = 0.0212\n",
      "init bias:  5.0\n",
      "Number of steps = 40000     Percentage = 48.68%     Time = 2427s     Learning rate = 0.0212\n"
     ]
    }
   ],
   "source": [
    "#.001, .01, .1, .2, .3, .5, .7,\n",
    "threshold_values = [.9]\n",
    "memory_fine_values = [.00001, .000001, .0000001, .00000001, .0]\n",
    "init_bias_values = [0., .5, 1., 2., 5.]\n",
    "threshold = {'fixed': True, 'min': 0.2, 'max': 0.7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "num_iter = 20\n",
    "results_GL = list()\n",
    "for threshold_value in threshold_values:\n",
    "    print(' '*4, \"threshold: \", threshold_value)\n",
    "    threshold['min'] = threshold_value\n",
    "    for memory_fine_value in memory_fine_values:\n",
    "        print(' '*2, \"memory fine: \", memory_fine_value)\n",
    "        optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "        for init_bias_value in init_bias_values:\n",
    "            print(\"init bias: \", init_bias_value)\n",
    "            model = adaptive_sum_with_memory(64,\n",
    "                             vocabulary,\n",
    "                             characters_positions_in_vocabulary,\n",
    "                             30,\n",
    "                             1,\n",
    "                             [128],\n",
    "                             init_bias_value,\n",
    "                             threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                             train_text,\n",
    "                             valid_text)\n",
    "            model.simple_run(100,\n",
    "                           'adaptive_sum_with_memory/variables/1_128_nu30_ns40k_dcs30/th%s_mf%s_ib%s' % (threshold_value, memory_fine_value, init_bias_value),\n",
    "                               40000,\n",
    "                               4000,\n",
    "                               5000,        #learning has a chance to be stopped after every block of steps\n",
    "                               30,\n",
    "                               0.9,\n",
    "                               3,\n",
    "                               optional_feed_dict=optional_feed_dict,\n",
    "                                fixed_num_steps=True)\n",
    "            results_GL.extend(model._results)\n",
    "            model.destroy()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     threshold:  0.001\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "Number of steps = 0     Percentage = 48.63%     Time = 0s     Learning rate = 0.0212\n",
      "{'self.memory_fine': 1e-05}\n",
      "init bias:  0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-921ac656a2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                    \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                            \u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                         fixed_num_steps=True)\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptional_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mresults_GL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, restore_path, num_averaging_iterations, min_num_steps, num_stairs, decay, optional_feed_dict, fixed_num_steps)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m             \u001b[0mdata_for_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_percentages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_averaging_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m             \u001b[0mGLOBAL_STEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_num_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             lr = self._learning_rate.eval(feed_dict={self._global_step: GLOBAL_STEP,\n",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mcalculate_percentages\u001b[0;34m(self, session, num_averaging_iterations)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0maverage_percentage_of_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpercent_of_correct_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0maverage_percentage_of_correct\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnum_averaging_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mpercent_of_correct_predictions\u001b[0;34m(predictions, labels)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_characters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_correct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_characters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#.001, .01, .1, .2, .3, .5, .7,\n",
    "threshold_values = [.001, .01, .1, .2, .3, .5, .7, .9]\n",
    "memory_fine_values = [.00001, .000001, .0000001, .00000001, .0]\n",
    "init_bias_values = [0., .5, 1., 2., 5.]\n",
    "threshold = {'fixed': True, 'min': .2, 'max': .7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "for threshold_value in threshold_values:\n",
    "    print(' '*4, \"threshold: \", threshold_value)\n",
    "    threshold['min'] = threshold_value\n",
    "    for memory_fine_value in memory_fine_values:\n",
    "        print(' '*2, \"memory fine: \", memory_fine_value)\n",
    "        optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "        for init_bias_value in init_bias_values:\n",
    "            print(\"init bias: \", init_bias_value)\n",
    "            model = adaptive_sum_with_memory(64,\n",
    "                             vocabulary,\n",
    "                             characters_positions_in_vocabulary,\n",
    "                             30,\n",
    "                             1,\n",
    "                             [128],\n",
    "                             init_bias_value,\n",
    "                             threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                             train_text,\n",
    "                             valid_text)\n",
    "            save_path = 'adaptive_sum_with_memory/variables/1_128_nu30_ns40k_dcs30/th%s_mf%s_ib%s' % (threshold_value, memory_fine_value, init_bias_value)\n",
    "            model.get_result(save_path,\n",
    "                             100,\n",
    "                   40000,\n",
    "                   30,\n",
    "                   0.9,\n",
    "                           optional_feed_dict=optional_feed_dict,  \n",
    "                        fixed_num_steps=True)\n",
    "            print('cycle:', optional_feed_dict)\n",
    "            results_GL.extend(model._results)\n",
    "            model.destroy()\n",
    "            del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for result in results_GL:\n",
    "    result['metadata'][model._indices['memory_fine']] = result['metadata'][model._indices['memory_fine']]['self.memory_fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_name = 'adaptive_sum_with_memory'\n",
    "file_name = 'adaptive_sum_with_memory_ns_40000_hl_1333_dc_0.9_ilr1._th0.001-0.9_mf0-1e-5_ib0-5_fixed.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'adaptive_sum_with_memory'\n",
    "pickle_file = 'adaptive_sum_with_memory_ns_40000_hl_1333_dc_0.9_ilr1._th0.001-0.9_mf0-1e-5_ib0-5_fixed.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'adaptive_sum_with_memory'\n",
    "pickle_file = 'adaptive_sum_with_memory_ns_40000_hl_1333_dc_0.9_ilr1._th0.001-0.9_mf0-1e-5_ib0-5.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_old = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The error occured because of no using constructor for optional_feed_dict\n",
    "memory_fine_values = [.00001, .000001, .0000001, .00000001, .0]\n",
    "idx = 0\n",
    "new_results = list()\n",
    "for thr_idx in range(8):\n",
    "    for memory_fine_value in memory_fine_values:\n",
    "        for other_idx in range(5):\n",
    "            new_results.append(dict(results_old[idx]))\n",
    "            new_results[idx]['metadata'][model._indices['memory_fine']] = memory_fine_value\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling adaptive_sum_with_memory/adaptive_sum_with_memory_ns_40000_hl_1333_dc_0.9_ilr1._th0.001-0.9_mf0-1e-5_ib0-5_fixed.pickle.\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'adaptive_sum_with_memory'\n",
    "file_name = 'adaptive_sum_with_memory_ns_40000_hl_1333_dc_0.9_ilr1._th0.001-0.9_mf0-1e-5_ib0-5_fixed.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': new_results}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'adaptive_sum_with_memory'\n",
    "pickle_file = 'adaptive_sum_with_memory_ns_40000_hl_1333_dc_0.9_ilr1._th0.001-0.9_mf0-1e-5_ib0-5_fixed.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#.001, .01, .1, .2, .3, .5, .7,\n",
    "threshold_values = [.001, .01, .1, .2, .3, .5, .7, .9]\n",
    "memory_fine_values = [.00001, .000001, .0000001, .00000001, .0]\n",
    "init_bias_values = [0., .5, 1., 2., 5.]\n",
    "short_results = dict()\n",
    "for result in results_GL:\n",
    "    key1 = result['metadata'][model._indices['threshold']]['min']\n",
    "    key2 = result['metadata'][model._indices['memory_fine']]\n",
    "    key3 = result['metadata'][model._indices['init_bias']]\n",
    "    if key1 not in short_results.keys():\n",
    "        short_results[key1] = dict()\n",
    "        short_results[key1][key2] = dict()\n",
    "        short_results[key1][key2][key3] = result['data']['train']['percentage'][-1]\n",
    "    elif key2 not in short_results[key1].keys():\n",
    "        short_results[key1][key2] = dict()\n",
    "        short_results[key1][key2][key3] = result['data']['train']['percentage'][-1]\n",
    "    elif key3 not in short_results[key1][key2].keys():\n",
    "        short_results[key1][key2][key3] = result['data']['train']['percentage'][-1]\n",
    "    else:\n",
    "        print('Error! Skipping')\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short_results.keys(): [0.5, 0.1, 0.2, 0.001, 0.3, 0.9, 0.7, 0.01]\n",
      "short_results[first].keys(): [1e-08, 0.0, 1e-06, 1e-05, 1e-07]\n",
      "short_results[first][first].keys(): [0.0, 0.5, 2.0, 5.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('short_results.keys():', short_results.keys())\n",
    "print('short_results[first].keys():', short_results[short_results.keys()[0]].keys())\n",
    "print('short_results[first][first].keys():', short_results[short_results.keys()[0]][short_results[short_results.keys()[0]].keys()[0]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     threshold:  0.001\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "     threshold:  0.01\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "     threshold:  0.1\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "     threshold:  0.2\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "     threshold:  0.3\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "     threshold:  0.5\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "     threshold:  0.7\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "     threshold:  0.9\n",
      "   memory fine:  1e-05\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-06\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-07\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  1e-08\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n",
      "   memory fine:  0.0\n",
      "init bias:  0.0\n",
      "init bias:  0.5\n",
      "init bias:  1.0\n",
      "init bias:  2.0\n",
      "init bias:  5.0\n"
     ]
    }
   ],
   "source": [
    "#.001, .01, .1, .2, .3, .5, .7,\n",
    "threshold_values = [.001, .01, .1, .2, .3, .5, .7, .9]\n",
    "memory_fine_values = [.00001, .000001, .0000001, .00000001, .0]\n",
    "init_bias_values = [0., .5, 1., 2., 5.]\n",
    "threshold = {'fixed': True, 'min': .2, 'max': .7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "for threshold_value in threshold_values:\n",
    "    print(' '*4, \"threshold: \", threshold_value)\n",
    "    threshold['min'] = threshold_value\n",
    "    for memory_fine_value in memory_fine_values:\n",
    "        print(' '*2, \"memory fine: \", memory_fine_value)\n",
    "        optional_feed_dict['self.memory_fine'] = memory_fine_value\n",
    "        for init_bias_value in init_bias_values:\n",
    "            print(\"init bias: \", init_bias_value)\n",
    "            model = adaptive_sum_with_memory(64,\n",
    "                             vocabulary,\n",
    "                             characters_positions_in_vocabulary,\n",
    "                             30,\n",
    "                             1,\n",
    "                             [128],\n",
    "                             init_bias_value,\n",
    "                             threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                             train_text,\n",
    "                             valid_text)\n",
    "            text_list, trigger_list, local_trigger_list = model.run_for_analitics(model.get_triggers,\n",
    "                                                            'adaptive_sum_with_memory/variables/1_128_nu30_ns40k_dcs30/th%s_mf%s_ib%s' % (threshold_value, memory_fine_value, init_bias_value),\n",
    "                                                            [300, 75, None])\n",
    "            \n",
    "            triggers = list()\n",
    "            local_triggers = list()\n",
    "            for text_number, text in enumerate(text_list):\n",
    "                trig = list()\n",
    "                l_trig = list()\n",
    "                text_triggers = trigger_list[text_number]\n",
    "                text_local_triggers = local_trigger_list[text_number]\n",
    "                for text_trigger, text_local_trigger in zip(text_triggers, text_local_triggers):\n",
    "                    trig.append(text_trigger[0, 0])\n",
    "                    l_trig.append(text_local_trigger[0, 0])\n",
    "                triggers.append(trig)\n",
    "                local_triggers.append(l_trig)\n",
    "                \n",
    "            structure_vocabulary_plots(text_list,\n",
    "                                       triggers,\n",
    "                                       'triggers for letter position (threshold %s, memory fine %s, init_bias %s, result %.2f%%)' % (threshold_value, \n",
    "                                                                                                                                     memory_fine_value,\n",
    "                                                                                                                                     init_bias_value,\n",
    "                                                                                                                                     short_results[threshold_value][memory_fine_value][init_bias_value]),\n",
    "                                       'mean trigger',\n",
    "                                       ['adaptive_sum_with_memory',\n",
    "                                        'triggers',\n",
    "                                        'init_bias_%s' % init_bias_value,\n",
    "                                        'memory_fine_%s' % memory_fine_value,\n",
    "                                        'threshold_%s_pcnt %.2f' % (threshold_value, short_results[threshold_value][memory_fine_value][init_bias_value])],\n",
    "                                       'mean_triggers128_ib%s_th%s_mf%s' % (init_bias_value, threshold_value, memory_fine_value),\n",
    "                                       ylims=[0., 1.],\n",
    "                                       ylims_fixed=True,\n",
    "                                       threshold=threshold_value,\n",
    "                                       show=False)\n",
    "            structure_vocabulary_plots(text_list,\n",
    "                                       local_triggers,\n",
    "                                       'triggers for letter position (threshold %s, memory fine %s, init_bias %s, result %.2f%%)' % (threshold_value, \n",
    "                                                                                                                                     memory_fine_value,\n",
    "                                                                                                                                     init_bias_value,\n",
    "                                                                                                                                     short_results[threshold_value][memory_fine_value][init_bias_value]),\n",
    "                                       'mean trigger',\n",
    "                                       ['adaptive_sum_with_memory',\n",
    "                                        'local_triggers',\n",
    "                                        'init_bias_%s' % init_bias_value,\n",
    "                                        'memory_fine_%s' % memory_fine_value,\n",
    "                                        'threshold_%s_pcnt %.2f' % (threshold_value, short_results[threshold_value][memory_fine_value][init_bias_value])],\n",
    "                                       'mean_triggers128_ib%s_th%s_mf%s' % (init_bias_value, threshold_value, memory_fine_value),\n",
    "                                       ylims=[0., 1.],\n",
    "                                       ylims_fixed=True,\n",
    "                                       threshold=threshold_value,\n",
    "                                       show=False)\n",
    "            for i in range(50):\n",
    "                text_plot(text_list[i],\n",
    "                          triggers[i],\n",
    "                          'trigger',\n",
    "                          'mean trigger values (threshold %s, memory fine %s, init_bias %s, result %.2f%%)' % (threshold_value, \n",
    "                                                                                                               memory_fine_value,\n",
    "                                                                                                               init_bias_value,\n",
    "                                                                                                               short_results[threshold_value][memory_fine_value][init_bias_value]),\n",
    "                          ['adaptive_sum_with_memory',\n",
    "                           'triggers',\n",
    "                           'init_bias_%s' % init_bias_value,\n",
    "                           'memory_fine_%s' % memory_fine_value,\n",
    "                           'threshold_%s_pcnt %.2f' % (threshold_value, short_results[threshold_value][memory_fine_value][init_bias_value]),\n",
    "                           'text_plots'],\n",
    "                          'triggers128_ib%s_th%s_mf%s#%s' % (init_bias_value, threshold_value, memory_fine_value, i),\n",
    "                          threshold=threshold_value,\n",
    "                          show=False,\n",
    "                          sum_flag=True)\n",
    "                text_plot(text_list[i],\n",
    "                          local_triggers[i],\n",
    "                          'trigger',\n",
    "                          'mean trigger values (threshold %s, memory fine %s, init_bias %s, result %.2f%%)' % (threshold_value, \n",
    "                                                                                                               memory_fine_value,\n",
    "                                                                                                               init_bias_value,\n",
    "                                                                                                               short_results[threshold_value][memory_fine_value][init_bias_value]),\n",
    "                          ['adaptive_sum_with_memory',\n",
    "                           'local_triggers',\n",
    "                           'init_bias_%s' % init_bias_value,\n",
    "                           'memory_fine_%s' % memory_fine_value,\n",
    "                           'threshold_%s_pcnt %.2f' % (threshold_value, short_results[threshold_value][memory_fine_value][init_bias_value]),\n",
    "                           'text_plots'],\n",
    "                          'triggers128_ib%s_th%s_mf%s#%s' % (init_bias_value, threshold_value, memory_fine_value, i),\n",
    "                          threshold=threshold_value,\n",
    "                          show=False,\n",
    "                          sum_flag=False)\n",
    "            model.destroy()\n",
    "            del model\n",
    "            gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "adaptive_sum_plots = ComparePlots('adaptive_sum')\n",
    "adaptive_sum_plots.add_network(results_GL, model._indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_options={'x': 'log'}\n",
    "threshold_values = [.001, .01, .1, .2, .3, .5, .7, .9]\n",
    "memory_fine_values = [.00001, .000001, .0000001, .00000001, .0]\n",
    "init_bias_values = [0., .5, 1., 2., 5.]\n",
    "threshold = {'fixed': True, 'min': .2, 'max': .7, 'epochs': 10000}\n",
    "optional_feed_dict = {'self.memory_fine': 0.001}\n",
    "results_GL = list()\n",
    "plot_data, _ = adaptive_sum_plots.one_key_layout_data('adaptive_sum_1',\n",
    "                                         'memory_fine',\n",
    "                                         \"threshold['min']\",\n",
    "                                                        fixed_variables_list=['init_bias'])\n",
    "for one_plot_data in plot_data:\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'memory fine effect',\n",
    "                    ['to_move_plots', 'init_bias', 'mf_dep'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;ib%s' % one_plot_data['fixed'][('init_bias', None)],\n",
    "                     plot_options=plot_options)\n",
    "    \n",
    "plot_data, _ = adaptive_sum_plots.one_key_layout_data('adaptive_sum_1',\n",
    "                                         \"threshold['min']\",\n",
    "                                                           'memory_fine',\n",
    "                                                        fixed_variables_list=['init_bias'])\n",
    "for idx, one_plot_data in enumerate(plot_data):\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'threshold effect',\n",
    "                    ['to_move_plots', 'init_bias', 'thr_dep'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;ib%s_log' % one_plot_data['fixed'][('init_bias', None)],\n",
    "                     plot_options=plot_options)\n",
    "for idx, one_plot_data in enumerate(plot_data):\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'threshold effect',\n",
    "                    ['to_move_plots', 'init_bias', 'thr_dep'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;ib%s' % one_plot_data['fixed'][('init_bias', None)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data, _ = adaptive_sum_plots.one_key_layout_data('adaptive_sum_1',\n",
    "                                         'memory_fine',\n",
    "                                         \"init_bias\",\n",
    "                                                        fixed_variables_list=[\"threshold['min']\"])\n",
    "for one_plot_data in plot_data:\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'memory fine effect',\n",
    "                    ['to_move_plots', \"threshold['min']\", 'mf_dep'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;thr%s' % one_plot_data['fixed'][('threshold', 'min')],\n",
    "                     plot_options=plot_options)\n",
    "    \n",
    "plot_data, _ = adaptive_sum_plots.one_key_layout_data('adaptive_sum_1',\n",
    "                                         \"init_bias\",\n",
    "                                         'memory_fine',\n",
    "                                                        fixed_variables_list=[\"threshold['min']\"])\n",
    "for one_plot_data in plot_data:\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'init_bias effect',\n",
    "                    ['to_move_plots', \"threshold['min']\", 'ib_dep'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;thr%s' % one_plot_data['fixed'][('threshold', 'min')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data, _ = adaptive_sum_plots.one_key_layout_data('adaptive_sum_1',\n",
    "                                         \"threshold['min']\",\n",
    "                                         \"init_bias\",\n",
    "                                                        fixed_variables_list=['memory_fine'])\n",
    "for one_plot_data in plot_data:\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'threshold effect',\n",
    "                    ['to_move_plots', \"memory_fine\", 'thr_dep_log'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;mf%s' % one_plot_data['fixed'][('memory_fine', None)],\n",
    "                     plot_options=plot_options)\n",
    "for one_plot_data in plot_data:\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'threshold effect',\n",
    "                    ['to_move_plots', \"memory_fine\", 'thr_dep'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;mf%s' % one_plot_data['fixed'][('memory_fine', None)])\n",
    "    \n",
    "plot_data, _ = adaptive_sum_plots.one_key_layout_data('adaptive_sum_1',\n",
    "                                         \"init_bias\",\n",
    "                                         \"threshold['min']\",\n",
    "                                                        fixed_variables_list=['memory_fine'])\n",
    "for one_plot_data in plot_data:\n",
    "    adaptive_sum_plots.save_layout(one_plot_data,\n",
    "                    'init_bias effect',\n",
    "                    ['to_move_plots', \"memory_fine\", 'ib_dep'],\n",
    "                    'nn128;ns40000;dc0.9;dcs30;mf%s' % one_plot_data['fixed'][('memory_fine', None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
