{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not one byte characters:  0\n",
      "min order index:  9\n",
      "max order index:  255\n",
      "total number of characters:  196\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99325000 c facade.&lt;ref&gt;[http://library.nothingness.org/articles/SI/\n",
      "25000 n in the February 1934 riots, anarchists divided over a 'united \n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset = 30000\n",
    "valid_size = 25000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99371900 , Zeno &quot;repudiated the omnipotence of the state, its interv\n",
      "4000 <mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmln\n",
      "4000 reserve\">{{Anarchism}}\n",
      "'''Anarchism''' originated as a term of a\n",
      "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\n",
      "  <siteinfo>\n",
      "    <sitename>Wikipedia</sitename>\n",
      "    <base>http://en.wikipedia.org/wiki/Main_Page</base>\n",
      "    <generator>MediaWiki 1.6alpha</generator>\n",
      "    <case>first-letter</case>\n",
      "      <namespaces>\n",
      "      <namespace key=\"-2\">Media</namespace>\n",
      "      <namespace key=\"-1\">Special</namespace>\n",
      "      <namespace key=\"0\" />\n",
      "      <namespace key=\"1\">Talk</namespace>\n",
      "      <namespace key=\"2\">User</namespace>\n",
      "      <namespace key=\"3\">User talk</namespace>\n",
      "      <namespace key=\"4\">Wikipedia</namespace>\n",
      "      <namespace key=\"5\">Wikipedia talk</namespace>\n",
      "      <namespace key=\"6\">Image</namespace>\n",
      "      <namespace key=\"7\">Image talk</namespace>\n",
      "      <namespace key=\"8\">MediaWiki</namespace>\n",
      "      <namespace key=\"9\">MediaWiki talk</namespace>\n",
      "      <namespace key=\"10\">Template</namespace>\n",
      "      <namespace key=\"11\">Template talk</namespace>\n",
      "      <namespace key=\"12\">Help</namespace>\n",
      "      <namespace key=\"13\">Help talk</namespace>\n",
      "      <namespace key=\"14\">Category</namespace>\n",
      "      <namespace key=\"15\">Category talk</namespace>\n",
      "      <namespace key=\"100\">Portal</namespace>\n",
      "      <namespace key=\"101\">Portal talk</namespace>\n",
      "    </namespaces>\n",
      "  </siteinfo>\n",
      "  <page>\n",
      "    <title>AaA</title>\n",
      "    <id>1</id>\n",
      "    <revision>\n",
      "      <id>32899315</id>\n",
      "      <timestamp>2005-12-27T18:46:47Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Jsmethers</username>\n",
      "        <id>614213</id>\n",
      "      </contributor>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[AAA]]</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AlgeriA</title>\n",
      "    <id>5</id>\n",
      "    <revision>\n",
      "      <id>18063769</id>\n",
      "      <timestamp>2005-07-03T11:13:13Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Docu</username>\n",
      "        <id>8029</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>adding cur_id=5: {{R from CamelCase}}</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Algeria]]{{R from CamelCase}}</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AmericanSamoa</title>\n",
      "    <id>6</id>\n",
      "    <revision>\n",
      "      <id>18063795</id>\n",
      "      <timestamp>2005-07-03T11:14:17Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Docu</username>\n",
      "        <id>8029</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>adding to cur_id=6  {{R from CamelCase}}</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[American Samoa]]{{R from CamelCase}}</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AppliedEthics</title>\n",
      "    <id>8</id>\n",
      "    <revision>\n",
      "      <id>15898943</id>\n",
      "      <timestamp>2002-02-25T15:43:11Z</timestamp>\n",
      "      <contributor>\n",
      "        <ip>Conversion script</ip>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>Automated conversion</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Applied ethics]]\n",
      "</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AccessibleComputing</title>\n",
      "    <id>10</id>\n",
      "    <revision>\n",
      "      <id>15898945</id>\n",
      "      <timestamp>2003-04-25T22:18:38Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Ams80</username>\n",
      "        <id>7543</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>Fixing redirect</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Accessible_computing]]</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AdA</title>\n",
      "    <id>11</id>\n",
      "    <revision>\n",
      "      <id>15898946</id>\n",
      "      <timestamp>2002-09-22T16:02:58Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Andre Engels</username>\n",
      "        <id>300</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Ada programming language]]</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>Anarchism</title>\n",
      "    <id>12</id>\n",
      "    <revision>\n",
      "      <id>42136831</id>\n",
      "      <timestamp>2006-03-04T01:41:25Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>CJames745</username>\n",
      "        <id>832382</id>\n",
      "      </contributor>\n",
      "      <min\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reserve\">{{Anarchism}}\n",
      "'''Anarchism''' originated as a term of abuse first used against early [[working class]] [[radical]]s including the [[Diggers]] of the [[English Revolution]] and the [[sans-culotte|''sans-culottes'']] of the [[French Revolution]].[http://uk.encarta.msn.com/encyclopedia_761568770/Anarchism.html] Whilst the term is still used in a pejorative way to describe ''&quot;any act that used violent means to destroy the organization of society&quot;''&lt;ref&gt;[http://www.cas.sc.edu/socy/faculty/deflem/zhistorintpolency.html History of International Police Cooperation], from the final protocols of the &quot;International Conference of Rome for the Social Defense Against Anarchists&quot;, 1898&lt;/ref&gt;, it has also been taken up as a positive label by self-defined anarchists.\n",
      "\n",
      "The word '''anarchism''' is [[etymology|derived from]] the [[Greek language|Greek]] ''[[Wiktionary:&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&amp;#945;|&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&amp;#945;]]'' (&quot;without [[archon]]s (ruler, chief, king)&quot;). Anarchism as a [[political philosophy]], is the belief that ''rulers'' are unnecessary and should be abolished, although there are differing interpretations of what this means. Anarchism also refers to related [[social movement]]s) that advocate the elimination of authoritarian institutions, particularly the [[state]].&lt;ref&gt;[http://en.wikiquote.org/wiki/Definitions_of_anarchism Definitions of anarchism] on Wikiquote, accessed 2006&lt;/ref&gt; The word &quot;[[anarchy]],&quot; as most anarchists use it, does not imply [[chaos]], [[nihilism]], or [[anomie]], but rather a harmonious [[anti-authoritarian]] society. In place of what are regarded as authoritarian political structures and coercive economic institutions, anarchists advocate social relations based upon [[voluntary association]] of autonomous individuals, [[mutual aid]], and [[self-governance]]. \n",
      "    \n",
      "While anarchism is most easily defined by what it is against, anarchists also offer positive visions of what they believe to be a truly free society. However, ideas about how an anarchist society might work vary considerably, especially with respect to economics; there is also disagreement about how a free society might be brought about. \n",
      "\n",
      "== Origins and predecessors ==\n",
      "\n",
      "[[Peter Kropotkin|Kropotkin]], and others, argue that before recorded [[history]], human society was organized on anarchist principles.&lt;ref&gt;[[Peter Kropotkin|Kropotkin]], Peter. ''&quot;[[Mutual Aid: A Factor of Evolution]]&quot;'', 1902.&lt;/ref&gt; Most anthropologists follow Kropotkin and Engels in believing that hunter-gatherer bands were egalitarian and lacked division of labour, accumulated wealth, or decreed law, and had equal access to resources.&lt;ref&gt;[[Friedrich Engels|Engels]], Freidrich. ''&quot;[http://www.marxists.org/archive/marx/works/1884/origin-family/index.htm Origins of the Family, Private Property, and the State]&quot;'', 1884.&lt;/ref&gt;\n",
      "[[Image:WilliamGodwin.jpg|thumb|right|150px|William Godwin]]\n",
      "\n",
      "Anarchists including the [[The Anarchy Organisation]] and [[Murray Rothbard|Rothbard]] find anarchist attitudes in [[Taoism]] from [[History of China|Ancient China]].&lt;ref&gt;The Anarchy Organization (Toronto). ''Taoism and Anarchy.'' [[April 14]] [[2002]] [http://www.toxicpop.co.uk/library/taoism.htm Toxicpop mirror] [http://www.geocities.com/SoHo/5705/taoan.html Vanity site mirror]&lt;/ref&gt;&lt;ref&gt;[[Murray Rothbard|Rothbard]], Murray. ''&quot;[http://www.lewrockwell.com/rothbard/ancient-chinese.html The Ancient Chinese Libertarian Tradition]&quot;'', an extract from ''&quot;[http://www.mises.org/journals/jls/9_2/9_2_3.pdf Concepts of the Role of Intellectuals in Social Change Toward Laissez Faire]&quot;'', The Journal of Libertarian Studies, 9 (2) Fall 1990.&lt;/ref&gt; [[Peter Kropotkin|Kropotkin]] found similar ideas in [[stoicism|stoic]] [[Zeno of Citium]]. According to Kropotkin\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size_1, valid_text_1[:64])\n",
    "print(valid_size_2, valid_text_2[:64])\n",
    "print(valid_text_1)\n",
    "print('\\n\\n\\n')\n",
    "print(valid_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ\n",
      "char2id(u'a') = 67,  char2id(u'z') = 92,  char2id(u' ') = 2\n",
      "id2char(78) = l,  id2char(156) = Ø,  id2char(140) = È\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u', Zeno &quo', u'act science', u'] systems. ', u\"* ''Women E\", u'nis. It ori', u'ilanthus al', u'ave diminis', u't;Advance A', u'orded in ce', u' late [[179', u'. A strong ', u'bodia]] led', u'f the Hill]', u']] | above=', u\"t rather ''\", u\"d 'Babe Rut\", u'[Chicago Wh', u'overed to b', u'when he spr', u' search].\\n*', u'erminology=', u'ile.\\n\\n====1', u'ife and car', u'509</id>\\n  ', u' featured i', u'other marin', u'ere has bee', u' be copied ', u' subdivisio', u'eased drawi', u'include:\\n\\n[', u'for the thi', u'age do not ', u'ion>\\n      ', u'(book)|Drag', u': ([[1921]]', u'ndia in ord', u'how can we ', u'</timestamp', u's and also ', u'n ==\\nThe Es', u'nredlist.or', u' fully or p', u'ma was revi', u'ometimes &l', u'when there ', u' and [[pena', u'arkets (see', u'uage with t', u'the [[Ameri', u'lable power', u'ername>\\n   ', u'phen]]s in ', u'ws and the ', u' culture ha', u'uardians of', u'llaboration', u'ot;Lords of', u' Howard, 3r', u'ersonnel ca', u' drafts [[l', u'edarville, ', u'NA)&lt;/TD&', u'rraced agri']\n",
      "[u'ot;repudiat', u'e, deductiv', u' He also wo', u\"Encamping''\", u'iginates at', u'ltissima1.j', u'shed in imp', u'Australia f', u'ertain sect', u'92]] Philli', u' ballad wit', u'd by [[Pol ', u']] was crea', u'=- | below=', u\"'apostolic \", u\"th', see [[\", u'hite Sox]]&', u'be 99% simi', u'reads money', u'* [http://w', u'==\\n[[Image:', u'19781990===', u'reer]\\n*[htt', u'     <times', u'in many fil', u'nas and boa', u'en an exten', u' to the pri', u\"on = \\n''Vac\", u'ing and dec', u'[[Prophecy]', u'ird derivat', u' have a [[s', u' <id>159063', u\"gonology]]'\", u\"]): &quot;'\", u'der to argu', u' distinguis', u'p>\\n      <c', u' local hous', u'ssenes are ', u'rg&gt;. Dow', u'partly porn', u'italized in', u'lt;math&gt;', u' is an equi', u'alty shooto', u'e [[economi', u'the earlies', u'ican Marcon', u'r ups. Whil', u'      <id>6', u' place of a', u' survival o', u'ad spread a', u'f Scotland ', u'n with UMNO', u'f Parliamen', u'rd Duke of ', u'arrier]] (A', u'legislation', u' Indiana|Le', u'&gt;\\n    &l', u'iculture th']\n",
      "[u'<m']\n",
      "[u'me']\n"
     ]
    }
   ],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text_1,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ACT_LSTM_individual_2(MODEL):\n",
    "    \n",
    "    def LSTM_cell(self, inp, state):\n",
    "        #print('inp.shape: ', inp.get_shape().as_list())\n",
    "        #print('state[0].shape: ', state[0].get_shape().as_list())\n",
    "        X = tf.concat(1, [inp,\n",
    "                          state[0]])\n",
    "        concat = tf.matmul(X, self.Matrix) + self.Bias\n",
    "        [i, f, o, g] = tf.split(1, 4, concat)\n",
    "        i = tf.sigmoid(i)\n",
    "        f = tf.sigmoid(f)\n",
    "        o = tf.sigmoid(o)\n",
    "        g = tf.tanh(g)\n",
    "        new_state = tf.tanh(i*g+f*state[1])\n",
    "        output = o * new_state \n",
    "        return [output, new_state]\n",
    "    \n",
    "    def add_flag_to_input(self, X, idx):\n",
    "        flags = list()\n",
    "        one_flag = [0.]*self._depth\n",
    "        one_flag[idx] = 1.\n",
    "        flag = tf.pack([tf.constant(one_flag)] * X.get_shape().as_list()[0])\n",
    "        return tf.concat(1, [X, flag])\n",
    "    \n",
    "    def compute_halting_units(self, intermediate_states):\n",
    "        states_concat = tf.concat(0, [tf.concat(1, [state[0], state[1]]) for state in intermediate_states])\n",
    "        halting_units = tf.sigmoid(tf.matmul(states_concat, self.Halting_Matrix) + self.Halting_Bias)\n",
    "        return tf.split(0, len(intermediate_states), halting_units)\n",
    "        \n",
    "    \n",
    "    def compute_probabilities(self, halting_units):\n",
    "        b_size = halting_units[0].get_shape().as_list()[0]\n",
    "        agregate = tf.zeros([b_size, self._num_nodes[0]])\n",
    "        completed = tf.zeros([b_size, self._num_nodes[0]])\n",
    "        ones = tf.ones([b_size, self._num_nodes[0]])\n",
    "        minus_ones = - ones\n",
    "        probabilities = list()\n",
    "        prob_loss = 0\n",
    "        for idx, halting_unit in enumerate(halting_units):\n",
    "            if idx == self._depth - 1:\n",
    "                completed_flags = tf.to_float(tf.greater(agregate, -0.5))\n",
    "            else:\n",
    "                completed_flags = tf.to_float(tf.greater(agregate + halting_unit, self._threshold))\n",
    "            completed += completed_flags\n",
    "            probabilities.append((ones - completed) * halting_unit + completed_flags * (ones - agregate))\n",
    "            prob_loss += tf.reduce_sum(completed_flags * (ones - agregate))\n",
    "            agregate = minus_ones * completed + (ones - completed) * (agregate + halting_unit)\n",
    "        return probabilities, prob_loss\n",
    "            \n",
    "    def compute_number_of_iterations(self, probabilities):\n",
    "        probs = tf.pack(probabilities, axis=2)\n",
    "        looped = tf.greater(probs, 0)\n",
    "        looped = tf.to_int32(looped)\n",
    "        return tf.reduce_sum(looped, reduction_indices=[2])\n",
    "               \n",
    "    \n",
    "    def iteration(self, inp, old_state):\n",
    "        \"\"\"print('is_list: ', isinstance(old_state, list))\n",
    "        if isinstance(old_state, list):\n",
    "            print('old_state[0].shape: ', old_state[0].get_shape().as_list())\n",
    "        else:\n",
    "            print('old_state.shape: ', old_state.get_shape().as_list())\"\"\"\n",
    "        b_size = inp.get_shape().as_list()[0]\n",
    "        intermediate_states = list()\n",
    "        halting_units = list()\n",
    "        agregate = tf.zeros([b_size, self._num_nodes[0]])\n",
    "        ones = tf.ones([b_size, self._num_nodes[0]])\n",
    "        for idx in range(self._depth):\n",
    "            new_state = self.LSTM_cell(self.add_flag_to_input(inp, idx), old_state)\n",
    "            mask = tf.to_float(tf.less(agregate, self._threshold))\n",
    "            very_old_state = old_state\n",
    "            old_state = list()\n",
    "            old_state.append(mask * new_state[0] + (ones - mask) * very_old_state[0])\n",
    "            old_state.append(mask * new_state[1] + (ones - mask) * very_old_state[1])\n",
    "            halting_unit = tf.sigmoid(tf.matmul(tf.concat(1, old_state), self.Halting_Matrix) + self.Halting_Bias)\n",
    "            agregate += halting_unit\n",
    "            halting_units.append(halting_unit)\n",
    "            intermediate_states.append(old_state)\n",
    "        probabilities, prob_loss = self.compute_probabilities(halting_units)\n",
    "        number_of_iterations = self.compute_number_of_iterations(probabilities)\n",
    "        new_state = [0, 0]\n",
    "        for inter_state, probability in zip(intermediate_states, probabilities):\n",
    "            new_state[0] +=  inter_state[0] * probability\n",
    "            new_state[1] +=  inter_state[1] * probability\n",
    "        return new_state[0], new_state, prob_loss, number_of_iterations, probabilities, halting_units\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_nodes,\n",
    "                 depth,\n",
    "                 indent,\n",
    "                 time_penalty,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 mean=0.,\n",
    "                 stddev='default',\n",
    "                 shift=0.,\n",
    "                 init_learning_rate=5.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        \n",
    "        self._num_unrollings = num_unrollings\n",
    "\n",
    "        self._num_layers = 1\n",
    "        self._num_nodes = num_nodes\n",
    "        self._depth = depth\n",
    "        self._indent = indent\n",
    "        self._time_penalty = time_penalty\n",
    "        self._threshold = 1. - indent\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        if isinstance(self._valid_text, dict):\n",
    "            keys = self._valid_text.keys()\n",
    "            self._valid_size = dict([zipped for zipped in zip(keys, [len(self._valid_text[key]) for key in keys])])\n",
    "        else:\n",
    "            self._valid_size = len(valid_text)\n",
    "        \n",
    "        \n",
    "        self._mean = mean\n",
    "        \n",
    "        self._shift = shift\n",
    "        \n",
    "        self._stddev = list()\n",
    "        if stddev == 'default':\n",
    "            self._stddev = 1.\n",
    "        else:\n",
    "            self._stddev = stddev\n",
    "\n",
    "        self._init_learning_rate = init_learning_rate\n",
    "  \n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"depth\": 4,\n",
    "                         \"indent\": 5,\n",
    "                         \"time_penalty\": 6,\n",
    "                         \"half_life\": 7,\n",
    "                         \"decay\": 8,\n",
    "                         \"num_steps\": 9,\n",
    "                         \"averaging_number\": 10,\n",
    "                         \"init_mean\": 11,\n",
    "                         \"init_stddev\": 12,\n",
    "                         \"init_shift\": 13,\n",
    "                         \"init_learning_rate\": 14,                         \n",
    "                         \"type\": 15}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with self._graph.device('/gpu:0'): \n",
    "                self.Matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + self._num_nodes[0] + self._depth,\n",
    "                                                                      4 * self._num_nodes[0]],\n",
    "                                                                     mean=self._mean, stddev=self._stddev / (self._vocabulary_size + self._num_nodes[0] + self._depth)**0.5))\n",
    "                self.Bias = tf.Variable(tf.zeros([4 * self._num_nodes[0]]))\n",
    "                \n",
    "                self.Halting_Matrix = tf.Variable(tf.truncated_normal([2 * self._num_nodes[0],\n",
    "                                                                      self._num_nodes[0]],\n",
    "                                                                     mean=self._mean, stddev=self._stddev / self._num_nodes[0]**0.5))\n",
    "                \n",
    "                self.Halting_Bias = tf.Variable(tf.zeros([self._num_nodes[0]]))\n",
    "                # classifier \n",
    "                weights = tf.Variable(tf.truncated_normal([self._num_nodes[-1], self._vocabulary_size], stddev = self._stddev / self._num_nodes[-1]**0.5))\n",
    "                bias = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                self._train_data = list()\n",
    "                for _ in range(self._num_unrollings + 1):\n",
    "                    self._train_data.append(\n",
    "                        tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size]))\n",
    "                    \n",
    "                    \n",
    "                train_inputs = self._train_data[: self._num_unrollings]\n",
    "                train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                # Unrolled LSTM loop.\n",
    "                \n",
    "                \"\"\"global step\"\"\"\n",
    "                self._global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "                saved_state = list()\n",
    "                saved_state.append(tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False))\n",
    "                saved_state.append(tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False))\n",
    "\n",
    "                outputs = list()\n",
    "                state = saved_state\n",
    "                self.number_of_iterations = list()\n",
    "                first_inputs = list()\n",
    "                first_outputs = list()\n",
    "                for i in train_inputs:\n",
    "                    output, state, prob_loss, number_of_iterations, _, _ = self.iteration(i, state)\n",
    "                    self.number_of_iterations.append(tf.unpack(number_of_iterations)[0])\n",
    "                    outputs.append(output)\n",
    "                    first_inputs.append(tf.split(0, self._batch_size, i)[0])\n",
    "                    first_outputs.append(tf.split(0, self._batch_size, output)[0])\n",
    "                self.first_inputs = tf.concat(0, first_inputs)\n",
    "                self.first_outputs = tf.nn.softmax(tf.nn.xw_plus_b(tf.concat(0, first_outputs), weights, bias))\n",
    "                self.number_of_iterations = tf.pack(self.number_of_iterations)    \n",
    "\n",
    "                save_list = list()\n",
    "                save_list.append(saved_state[0].assign(state[0]))\n",
    "                save_list.append(saved_state[1].assign(state[1]))\n",
    "                \n",
    "                \"\"\"skip operation\"\"\"\n",
    "                self._skip_operation = tf.group(*save_list)\n",
    "\n",
    "                with tf.control_dependencies(save_list):\n",
    "                        # Classifier.\n",
    "                    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), weights, bias)\n",
    "                    \"\"\"loss\"\"\"\n",
    "                    self._loss = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits, tf.concat(0, train_labels))) + self._time_penalty * prob_loss / self._num_nodes\n",
    "\n",
    "                # Optimizer.\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                self._half_life = tf.placeholder(tf.int32)\n",
    "                self._decay = tf.placeholder(tf.float32)\n",
    "                \"\"\"learning rate\"\"\"\n",
    "                self._learning_rate = tf.train.exponential_decay(self._init_learning_rate,\n",
    "                                                                 self._global_step,\n",
    "                                                                 self._half_life,\n",
    "                                                                 self._decay,\n",
    "                                                                 staircase=True)\n",
    "                \n",
    "                optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                \n",
    "                halting_vars = [self.Halting_Matrix, self.Halting_Bias]\n",
    "                all_vars = [self.Matrix, self.Bias, weights, bias, self.Halting_Matrix, self.Halting_Bias]\n",
    "                \n",
    "                halting_grads, halting_vars = zip(*optimizer.compute_gradients(prob_loss, var_list=halting_vars))\n",
    "                all_grads, all_vars = zip(*optimizer.compute_gradients(self._loss, var_list=all_vars))\n",
    "                \n",
    "                halting_dict = dict(zip(halting_vars, halting_grads))\n",
    "                all_dict = dict(zip(all_vars, all_grads))\n",
    "                \n",
    "                for halting_key in halting_dict.keys():\n",
    "                    all_dict[halting_key] += halting_dict[halting_key]\n",
    "                \n",
    "                all_keys = all_dict.keys()\n",
    "                for key in all_keys:\n",
    "                    help_var, _ = tf.clip_by_global_norm([all_dict[key]], 1.25)\n",
    "                    all_dict[key] = help_var[0]\n",
    "\n",
    "                \"\"\"optimizer\"\"\"\n",
    "                self._optimizer = optimizer.apply_gradients(zip([all_dict[key] for key in all_keys], [key for key in all_keys]), global_step=self._global_step)\n",
    "                \"\"\"train prediction\"\"\"\n",
    "                self._train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                saved_sample_state = list()\n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False))\n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size])\n",
    "\n",
    "                reset_list = list()\n",
    "                reset_list.append(saved_sample_state[0].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_state[1].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "\n",
    "                \"\"\"reset sample state\"\"\"\n",
    "                self._reset_sample_state = tf.group(*reset_list)\n",
    "\n",
    "                sample_output, sample_state, _, self.num_iter, self.probs, hu = self.iteration(self._sample_input, saved_sample_state)\n",
    "\n",
    "                self.halting_units = tf.concat(0, hu)\n",
    "                sample_save_list = list()\n",
    "                sample_save_list.append(saved_sample_state[0].assign(sample_state[0]))\n",
    "                sample_save_list.append(saved_sample_state[1].assign(sample_state[1]))\n",
    "\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    \"\"\"sample prediction\"\"\"\n",
    "                    self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, weights, bias)) \n",
    "                \n",
    "                \n",
    "                \"\"\"saver\"\"\"\n",
    "                self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                            \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)   \n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(self._depth)\n",
    "        metadata.append(self._indent)\n",
    "        metadata.append(self._time_penalty)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._mean)\n",
    "        metadata.append(self._stddev)\n",
    "        metadata.append(self._shift)\n",
    "        metadata.append(self._init_learning_rate)\n",
    "        metadata.append('ACT_LSTM_individual_2')\n",
    "        return metadata\n",
    "\n",
    "    def get_iterations(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        iterations_list = list()\n",
    "        collect_iterations = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_iterations: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    i_list = [list() for _ in range(self._num_nodes[0])]\n",
    "                    collect_iterations = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                for_distribution = self.num_iter.eval({self._sample_input: b[0]})\n",
    "                for node_idx in range(for_distribution.shape[1]):\n",
    "                    i_list[node_idx].append(for_distribution[0][node_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_iterations = False\n",
    "                    iterations_list.append(i_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, iterations_list  \n",
    "    \n",
    "    def get_average_iterations(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        iterations_list = list()\n",
    "        collect_iterations = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_iterations: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    i_list = list()\n",
    "                    collect_iterations = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                i_list.append(np.mean(self.num_iter.eval({self._sample_input: b[0]})))\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_iterations = False\n",
    "                    iterations_list.append(i_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, iterations_list  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ACT_LSTM_individual_2(64,\n",
    "            vocabulary,\n",
    "            characters_positions_in_vocabulary,\n",
    "            20,\n",
    "            [128],\n",
    "                    5,\n",
    "                    0.01,\n",
    "                    0.00003,\n",
    "            train_text,\n",
    "            {'html': valid_text_1, 'text': valid_text_2},\n",
    "            init_learning_rate=5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.277689 learning rate: 0.500000\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "áfÆ¡JÂXnÜ«ùHjÑb^ì3@ëÞ#sx'^MµbuoÏ3õmb+~õùïÐ÷ClCÆwÕ]\"!$yÂaaÄIu£cø>>uTy¢[i;óËï`#øoº\n",
      "Gé9ýy=%tÈ¶çúÓkRZ@t9púÉrBÇWM°oü­ª½â-^´|ÁÙ¿ké«+.Íd&­Ã=aEÏùlJT&ÀÒafóõraIÄ'ÍØÈïZiå¥\n",
      "p\t?LCÈc¸]éÝaÊ°¸é;nþ&Ò¶=Ü\tÉ¸\tÕÈO¥\"æúÛAÂ?uâÐ¥L\n",
      "·AV×ê620ù.0É¤@;¸qü¦h±hí(¢ÒH& Ó)$mó\n",
      ".Õ×~ïSúç0$B]¡<ì¢úäl^«\t\"ô|\t8áßå#mÓFá±Å+­g¼ Ãñ®¢Ñâ\"uH«É=«òß|-/.=&Ôfx_äò¤DÍ[ï¾aõðef\n",
      "$¼îªÆV\\ºÅÎvG)¾¸uõanËU´Bvm¬DÖ¤µR\"ùH÷éCÈRÉPVù¸Ìæõx°÷èc\tá?«\tcl-Oió\n",
      "Ð_Èe²òÝY[ñ;Ü\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6da76efd56aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0;31m#num_validation_prints=10,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint_intermediate_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m           validation_example_length=10)\n\u001b[0m",
      "\u001b[0;32m/home/rumpelschtizhen/WIKI/model_module.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_stairs, decay, train_frequency, min_num_points, stop_percent, num_train_points_per_1_validation_point, averaging_number, optional_feed_dict, print_intermediate_results, half_life_fixed, add_operations, add_text_operations, print_steps, validation_add_operations, num_validation_prints, validation_example_length, fuse_texts)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valid_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                                     validation_result = session.run(validation_operations,\n\u001b[0;32m--> 623\u001b[0;31m                                                                     {self._sample_input: b[0]})\n\u001b[0m\u001b[1;32m    624\u001b[0m                                     \u001b[0mvalidation_percentage_of_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpercent_of_correct_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mprint_intermediate_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rumpelschtizhen/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_steps = [1000 * i for i in range(200)]\n",
    "model.run(10,\n",
    "          0.9,\n",
    "            1000,\n",
    "            50,\n",
    "            3,\n",
    "            1,\n",
    "            100,\n",
    "          #add_operations = ['self.number_of_iterations'],\n",
    "          #add_text_operations=['self.first_inputs', 'self.first_outputs'],\n",
    "          #print_steps = print_steps,\n",
    "          #validation_add_operations=['self.probs'],\n",
    "           #validation_add_operations=['self.num_iter', 'self.probs', 'self.halting_units'],\n",
    "          #validation_add_operations=['self.num_iter'],\n",
    "          #num_validation_prints=10,\n",
    "            print_intermediate_results = True,\n",
    "          validation_example_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps = 20000     Percentage = 52.68%     Time = 10889s     Learning rate = 0.0739\n"
     ]
    }
   ],
   "source": [
    "model.simple_run(200,\n",
    "                   'ACT_LSTM_individual_2.0/variables/first_try_idt0.01_mf0.00003',\n",
    "                   20000,\n",
    "                   4000,\n",
    "                   5000,        #learning has a chance to be stopped after every block of steps\n",
    "                   40,\n",
    "                   0.9,\n",
    "                   3,\n",
    "                   fixed_num_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
