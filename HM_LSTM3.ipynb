{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\"+appendix)\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=True,\n",
    "                                     name=\"reduce_mean_in_L2_norm\"+appendix)\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\"+appendix)\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            tr_state2 = tf.transpose(state[2],\n",
    "                                     name=\"transposed_state2_in_top_down_prepaired\")\n",
    "            tr_top_down = tf.transpose(top_down,\n",
    "                                       name=\"transposed_top_down_in_top_down_prepaired\")\n",
    "            with self._graph.gradient_override_map({\"Mul\": self.gradient_name2}):\n",
    "                multiply_in_top_down_prepaired = tf.multiply(tr_state2,\n",
    "                                                             tr_top_down,\n",
    "                                                             name=\"multiply_in_top_down_prepaired\")\n",
    "            top_down_prepaired = tf.transpose(multiply_in_top_down_prepaired,\n",
    "                                              name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            tr_boundary_state_down = tf.transpose(boundary_state_down,\n",
    "                                                  name=\"transposed_boundary_state_down_in_bottom_down_prepaired\")\n",
    "            tr_bottom_up = tf.transpose(bottom_up,\n",
    "                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\")\n",
    "            with self._graph.gradient_override_map({\"Mul\": self.gradient_name2}):\n",
    "                multiply_in_bottom_up_prepaired = tf.multiply(tr_boundary_state_down,\n",
    "                                                              tr_bottom_up,\n",
    "                                                              name=\"multiply_in_bottom_up_prepaired\")\n",
    "            bottom_up_prepaired = tf.transpose(multiply_in_bottom_up_prepaired,\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            # following implements formula (8). Missing (1-z) is added\n",
    "            boundary_state_reversed = tf.subtract(one, state[2], name=\"boundary_state_reversed\")\n",
    "            tr_boundary_state_reversed = tf.transpose(boundary_state_reversed,\n",
    "                                                      name=\"transposed_boundary_state_reversed_in_state0_prepaired\")\n",
    "            tr_state0 = tf.transpose(state[0],\n",
    "                                     name=\"transposed_state0_state0_prepaired\")\n",
    "            with self._graph.gradient_override_map({\"Mul\": self.gradient_name2}):\n",
    "                multiply_in_state0_prepaired = tf.multiply(tr_boundary_state_reversed,\n",
    "                                                           tr_state0,\n",
    "                                                           name=\"multiply_in_state0_prepaired\")\n",
    "            state0_prepaired = tf.transpose(multiply_in_state0_prepaired,\n",
    "                                            name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                                      name=\"logical_and_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"),\n",
    "                                                     name=\"to_float_in_copy_flag\"),\n",
    "                                         name=\"copy_flag\")\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                                      name=\"to_float_in_flush_flag\"),\n",
    "                                          name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "        return new_hidden, new_memory\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": self.gradient_name1}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "        \"\"\"X = tf.sign(X)\"\"\"\n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\")\n",
    "        return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "            hidden, memory, boundary, helper = self.not_last_layer(0,\n",
    "                                                                   state[0],\n",
    "                                                                   inp,\n",
    "                                                                   state[1][0],\n",
    "                                                                   activated_boundary_states)\n",
    "\n",
    "            not_last_layer_helpers = list()\n",
    "            not_last_layer_helpers.append(helper)\n",
    "            new_state.append((hidden, memory, boundary))\n",
    "            boundaries.append(boundary)\n",
    "            # All layers except for the first and the last ones\n",
    "            if num_layers > 2:\n",
    "                for idx in range(num_layers-2):\n",
    "                    hidden, memory, boundary, helper = self.not_last_layer(idx+1,\n",
    "                                                                          state[idx+1],\n",
    "                                                                          hidden,\n",
    "                                                                          state[idx+2][0],\n",
    "                                                                          boundary)\n",
    "                    not_last_layer_helpers.append(helper)\n",
    "                    new_state.append((hidden, memory, boundary))\n",
    "                    boundaries.append(boundary)\n",
    "            hidden, memory = self.last_layer(state[-1],\n",
    "                                             hidden,\n",
    "                                             boundary)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\")}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.transpose(tf.sigmoid(tf.matmul(concat,\n",
    "                                                                    self.output_module_gates_weights,\n",
    "                                                                    name=\"matmul_in_output_module_gates\"),\n",
    "                                                          name=\"sigmoid_in_output_module_gates\"),\n",
    "                                               name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=\"tr_hidden_state_total_%s\"%idx)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=\"tr_gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"),\n",
    "                                               name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "            \n",
    "        \n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1.,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "                 matr_init_parameter=1000.,\n",
    "                 override_appendix=''):               \n",
    "                                                   \n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter\n",
    "        self.gradient_name1 = 'HardSigmoid' + override_appendix\n",
    "        self.gradient_name2 = 'NewMul' + override_appendix\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"matr_init_parameter\": 14,\n",
    "                         \"type\": 15}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                init_bias_name = \"HM_LSTM_bias_%s_initializer\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\"\n",
    "                \n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._embedding_size + self._num_nodes[0] + self._num_nodes[1],\n",
    "                                                                      4 * self._num_nodes[0] + 1],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(self._init_parameter*matr_init_parameter/(self._embedding_size+self._num_nodes[0]+self._num_nodes[1])),\n",
    "                                                                     name=init_matr_name%0),\n",
    "                                                 name=matr_name%0))\n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[0] + 1],\n",
    "                                                        name=init_bias_name%0),\n",
    "                                               name=bias_name%0))\n",
    "                if self._num_layers > 2:\n",
    "                    for i in range(self._num_layers - 2):\n",
    "                        self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2],\n",
    "                                                                              4 * self._num_nodes[i+1] + 1],\n",
    "                                                                             mean=0.,\n",
    "                                                                             stddev=math.sqrt(self._init_parameter*matr_init_parameter/(self._num_nodes[i]+self._num_nodes[i+1]+self._num_nodes[i+2])),\n",
    "                                                                             name=init_matr_name%(i+1)),\n",
    "                                                         name=matr_name%(i+1)))\n",
    "                        self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[i+1] + 1],\n",
    "                                                                name=init_bias_name%(i+1)),\n",
    "                                                       name=bias_name%(i+1)))\n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[-1] + self._num_nodes[-2],\n",
    "                                                                      4 * self._num_nodes[-1]],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(self._init_parameter*matr_init_parameter/(self._num_nodes[-1]+self._num_nodes[-2])),\n",
    "                                                                     name=init_matr_name%(self._num_layers-1)),\n",
    "                                                 name=matr_name%(self._num_layers-1)))     \n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[-1]],\n",
    "                                                        name=init_bias_name%(self._num_layers-1)),\n",
    "                                               name=bias_name%(self._num_layers-1)))\n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "                    train_inputs_for_slice = tf.stack(train_inputs,\n",
    "                                                      axis=1,\n",
    "                                                      name=\"train_inputs_for_slice\")\n",
    "                    self.train_input_print = tf.reshape(tf.split(train_inputs_for_slice,\n",
    "                                                                 [1, self._batch_size-1],\n",
    "                                                                 name=\"split_in_train_print\")[0],\n",
    "                                                        [self._num_unrollings, -1],\n",
    "                                                        name=\"train_print\")\n",
    "\n",
    "\n",
    "                    saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 0)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 0)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 1)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 1)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 2)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 2))))\n",
    "                    saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                        tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "                    \n",
    "                    @tf.RegisterGradient(self.gradient_name1)\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "                    @tf.RegisterGradient(self.gradient_name2)\n",
    "                    def new_mul_grad(op,                # op is operation for which gradient is computed\n",
    "                                     grad):             # loss partial gradients with respect to op outputs\n",
    "                        \"\"\"The gradient of scalar multiplication.\"\"\"\n",
    "                        x = op.inputs[0]\n",
    "                        y = op.inputs[1]\n",
    "                        assert x.dtype.base_dtype == y.dtype.base_dtype, (x.dtype, \" vs. \", y.dtype)\n",
    "                        sx = array_ops.shape(x)\n",
    "                        sy = array_ops.shape(y)\n",
    "                        rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n",
    "                        x = math_ops.conj(x)\n",
    "                        y = math_ops.conj(y)\n",
    "                        not_modified_1 = array_ops.reshape(math_ops.reduce_sum(grad * y, rx), sx)\n",
    "                        not_modified_2 = array_ops.reshape(math_ops.reduce_sum(x * grad, ry), sy)\n",
    "                        tr_not_modified_2 = array_ops.transpose(not_modified_2)\n",
    "                        tr_x = array_ops.transpose(x)\n",
    "                        modified_1 = math_ops.multiply(not_modified_1, x)\n",
    "                        modified_2 = array_ops.transpose(math_ops.multiply(tr_x, tr_not_modified_2))\n",
    "                        return (modified_1,\n",
    "                                modified_2)\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "                    \n",
    "                    self.train_hard_sigm_arg = tf.reshape(tf.split(train_helper[\"hard_sigm_arg\"],\n",
    "                                                                   [1, self._batch_size-1],\n",
    "                                                                   name=\"split_in_train_hard_sigm_arg\")[0],\n",
    "                                                          [self._num_unrollings, -1],\n",
    "                                                          name=\"train_hard_sigm_arg\")\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    #optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    print_list = tf.split(self._train_prediction, self._num_unrollings, name=\"print_list\")\n",
    "                    print_for_slice = tf.stack(print_list, axis=1, name=\"print_for_slice\")\n",
    "                    self.train_output_print = tf.reshape(tf.split(print_for_slice,\n",
    "                                                                  [1, self._batch_size-1],\n",
    "                                                                  name=\"split_in_train_print\")[0],\n",
    "                                                         [self._num_unrollings, -1],\n",
    "                                                         name=\"train_print\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "                    \n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "            \n",
    "    def reinit(self,\n",
    "               init_slope,\n",
    "               slope_growth,\n",
    "               slope_half_life,\n",
    "               init_parameter,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "               matr_init_parameter):\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter        \n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._matr_init_parameter)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [128, 128, 128],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.278114 learning rate: 0.002582\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Øq{æW ÎV;3»×AÂf»HHS0³ä%±sl»°V.yÎON$lþØ¸yÇ±\n",
      "ë/Bå.,\\ÅmV´$AÜ¶éÝ¯â{MñÕèÛÜâ¨¤GDIcëì¦\n",
      "4hÜê©RÀ^³-ÅÉ`*PçAkssêK05ðÌs×u¦Þ9+È¼ßÉÆÕèT*>ßcÔècZ^ÉW(¥ä*9Gcz3NCÒ¯2µ1 dm¥kå]±Óô\n",
      "&Ïül¹ÉäC\"Î~Nh^ð¶p³k¸ð@ÑAZ&Fé7Ã;ÞþÙSÏEÜÀl4Ð_ÚÊjØOûÊp,gÎ®§km99Â³X.Ñ´âÈñ*H¢ñåê¸/'ã\n",
      "0§3Ó8¾Ïf}.U\"#ÝË3i[¦ÕÃ(Ä\"Kàµì¹v6F~\\\"®¼L>åìxÝûÅ?nî)T¦dýß*Me°¦éDÀ#6ÚÓ74¤à×­*^Øyoá\n",
      "~uSèäí¹í£^;zlZ_$íûÍÏ¢½©ÔwÞd hc²ìãä¹¯KìÇÎ°ç¼º7ÉÕTÓ®ÈÔÒV4ZèZÙ!s,ª²ÿ÷ÈË jyË¢þ|d®F¾\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [ -5.76642776e-07  -1.07363146e-14]\n",
      "1:\n",
      "self.sigm_arg:  [ -1.02738204e-06  -1.07363146e-14]\n",
      "2:\n",
      "self.sigm_arg:  [ -3.45096191e-06  -1.07363146e-14]\n",
      "3:\n",
      "self.sigm_arg:  [  4.99079715e-06   8.76381762e-07]\n",
      "4:\n",
      "self.sigm_arg:  [  1.96840028e-06   9.69018402e-07]\n",
      "5:\n",
      "self.sigm_arg:  [  2.90037434e-07   1.22569691e-06]\n",
      "6:\n",
      "self.sigm_arg:  [  2.67821565e-06   1.35821585e-06]\n",
      "7:\n",
      "self.sigm_arg:  [  1.97063946e-06   1.40935765e-06]\n",
      "8:\n",
      "self.sigm_arg:  [ -2.36632741e-06   9.81511448e-07]\n",
      "9:\n",
      "self.sigm_arg:  [ -1.02853630e-06   9.96751623e-07]\n",
      "10:\n",
      "self.sigm_arg:  [  1.73793183e-07   1.81263351e-06]\n",
      "11:\n",
      "self.sigm_arg:  [ -1.03904051e-06   1.00804027e-06]\n",
      "12:\n",
      "self.sigm_arg:  [  2.89044192e-06   1.70591670e-06]\n",
      "13:\n",
      "self.sigm_arg:  [ -2.69537821e-07   1.01081002e-06]\n",
      "14:\n",
      "self.sigm_arg:  [  1.97634654e-06   1.70377848e-06]\n",
      "15:\n",
      "self.sigm_arg:  [  3.31083743e-06   1.48315121e-06]\n",
      "16:\n",
      "self.sigm_arg:  [  2.91363477e-07   1.47396452e-06]\n",
      "17:\n",
      "self.sigm_arg:  [  2.24616315e-06   1.47618084e-06]\n",
      "18:\n",
      "self.sigm_arg:  [  1.17607772e-06   1.48021900e-06]\n",
      "19:\n",
      "self.sigm_arg:  [  1.97097461e-06   1.47030528e-06]\n",
      "20:\n",
      "self.sigm_arg:  [  3.31078377e-06   1.48336665e-06]\n",
      "21:\n",
      "self.sigm_arg:  [  1.97098007e-06   1.47031551e-06]\n",
      "22:\n",
      "self.sigm_arg:  [  2.72535999e-07   1.47255423e-06]\n",
      "23:\n",
      "self.sigm_arg:  [  2.88154911e-06   1.47784715e-06]\n",
      "24:\n",
      "self.sigm_arg:  [ -2.02332444e-06   1.01170951e-06]\n",
      "25:\n",
      "self.sigm_arg:  [ -7.71432127e-08   1.01170917e-06]\n",
      "26:\n",
      "self.sigm_arg:  [ -5.49828428e-07   1.01170940e-06]\n",
      "27:\n",
      "self.sigm_arg:  [ -3.32884179e-06   1.01170906e-06]\n",
      "28:\n",
      "self.sigm_arg:  [  2.41668613e-06   1.90608023e-06]\n",
      "29:\n",
      "self.sigm_arg:  [ -8.10314248e-07   1.01170963e-06]\n",
      "30:\n",
      "self.sigm_arg:  [  7.32920995e-08   1.71802958e-06]\n",
      "31:\n",
      "self.sigm_arg:  [  1.97102941e-06   1.47031756e-06]\n",
      "32:\n",
      "self.sigm_arg:  [  2.34049935e-06   1.47827154e-06]\n",
      "33:\n",
      "self.sigm_arg:  [ -1.56296596e-06   1.01170963e-06]\n",
      "34:\n",
      "self.sigm_arg:  [ -8.03732632e-07   1.01170940e-06]\n",
      "35:\n",
      "self.sigm_arg:  [ -1.44151233e-08   1.01170940e-06]\n",
      "36:\n",
      "self.sigm_arg:  [ -6.61761476e-07   1.01170917e-06]\n",
      "37:\n",
      "self.sigm_arg:  [  3.56516381e-07   1.90778997e-06]\n",
      "38:\n",
      "self.sigm_arg:  [  2.91472190e-07   1.47407195e-06]\n",
      "39:\n",
      "self.sigm_arg:  [ -8.10419124e-07   1.01170974e-06]\n",
      "40:\n",
      "self.sigm_arg:  [  2.68589179e-06   1.71496606e-06]\n",
      "41:\n",
      "self.sigm_arg:  [  1.97103009e-06   1.47031756e-06]\n",
      "42:\n",
      "self.sigm_arg:  [  2.78070888e-06   1.47840956e-06]\n",
      "43:\n",
      "self.sigm_arg:  [ -5.91611251e-06   1.01170963e-06]\n",
      "44:\n",
      "self.sigm_arg:  [  1.60714217e-06   1.71342242e-06]\n",
      "45:\n",
      "self.sigm_arg:  [ -2.90548724e-06   1.01170951e-06]\n",
      "46:\n",
      "self.sigm_arg:  [  6.08433652e-07   1.70794772e-06]\n",
      "47:\n",
      "self.sigm_arg:  [  1.64378878e-06   1.48116237e-06]\n",
      "48:\n",
      "self.sigm_arg:  [ -4.00939689e-06   1.01170951e-06]\n",
      "49:\n",
      "self.sigm_arg:  [  1.98304861e-06   1.70016381e-06]\n",
      "50:\n",
      "self.sigm_arg:  [  1.75956893e-06   1.47884907e-06]\n",
      "51:\n",
      "self.sigm_arg:  [ -1.03910145e-06   1.01170963e-06]\n",
      "52:\n",
      "self.sigm_arg:  [ -8.83433700e-07   1.01170929e-06]\n",
      "53:\n",
      "self.sigm_arg:  [  1.07076301e-06   1.82753638e-06]\n",
      "54:\n",
      "self.sigm_arg:  [  4.60600995e-06   1.48371373e-06]\n",
      "55:\n",
      "self.sigm_arg:  [ -1.23373582e-06   1.01170963e-06]\n",
      "56:\n",
      "self.sigm_arg:  [ -1.02994932e-06   1.01170929e-06]\n",
      "57:\n",
      "self.sigm_arg:  [  4.86421141e-06   1.83055215e-06]\n",
      "58:\n",
      "self.sigm_arg:  [  2.67895939e-06   1.48126855e-06]\n",
      "59:\n",
      "self.sigm_arg:  [ -8.52215294e-08   1.01170963e-06]\n",
      "60:\n",
      "self.sigm_arg:  [  1.98271209e-06   1.70091096e-06]\n",
      "61:\n",
      "self.sigm_arg:  [  2.88161800e-06   1.47784704e-06]\n",
      "62:\n",
      "self.sigm_arg:  [  2.67888754e-06   1.48126981e-06]\n",
      "63:\n",
      "self.sigm_arg:  [  2.88154729e-06   1.47784772e-06]\n",
      "64:\n",
      "self.sigm_arg:  [ -3.71053125e-06   1.01170963e-06]\n",
      "65:\n",
      "self.sigm_arg:  [ -2.01264220e-06   1.01170929e-06]\n",
      "66:\n",
      "self.sigm_arg:  [  8.70779047e-07   1.82622932e-06]\n",
      "67:\n",
      "self.sigm_arg:  [  2.91458321e-07   1.47407206e-06]\n",
      "68:\n",
      "self.sigm_arg:  [  8.13802899e-07   1.47993092e-06]\n",
      "69:\n",
      "self.sigm_arg:  [ -8.52106226e-08   1.01170963e-06]\n",
      "70:\n",
      "self.sigm_arg:  [  1.98271209e-06   1.70091107e-06]\n",
      "71:\n",
      "self.sigm_arg:  [ -2.02327237e-06   1.01170963e-06]\n",
      "72:\n",
      "self.sigm_arg:  [ -8.02353384e-07   1.01170929e-06]\n",
      "73:\n",
      "self.sigm_arg:  [  3.89274447e-07   1.82462509e-06]\n",
      "74:\n",
      "self.sigm_arg:  [  1.33482956e-07   1.47320964e-06]\n",
      "75:\n",
      "self.sigm_arg:  [ -1.03911066e-06   1.01170963e-06]\n",
      "76:\n",
      "self.sigm_arg:  [  2.68776489e-06   1.71115391e-06]\n",
      "77:\n",
      "self.sigm_arg:  [  2.91412022e-07   1.47407241e-06]\n",
      "78:\n",
      "self.sigm_arg:  [  8.13802899e-07   1.47993092e-06]\n",
      "79:\n",
      "self.sigm_arg:  [ -8.52106226e-08   1.01170963e-06]\n",
      "80:\n",
      "self.sigm_arg:  [  8.25532140e-07   1.71052216e-06]\n",
      "81:\n",
      "self.sigm_arg:  [  1.97103691e-06   1.47031778e-06]\n",
      "82:\n",
      "self.sigm_arg:  [  2.91370384e-07   1.47407331e-06]\n",
      "83:\n",
      "self.sigm_arg:  [  2.67888095e-06   1.48126981e-06]\n",
      "84:\n",
      "self.sigm_arg:  [ -2.02334309e-06   1.01170963e-06]\n",
      "85:\n",
      "self.sigm_arg:  [ -2.61529976e-07   1.01170929e-06]\n",
      "86:\n",
      "self.sigm_arg:  [ -7.84877841e-07   1.01170940e-06]\n",
      "87:\n",
      "self.sigm_arg:  [  2.62655249e-06   1.88199556e-06]\n",
      "88:\n",
      "self.sigm_arg:  [  4.60603223e-06   1.48371350e-06]\n",
      "89:\n",
      "self.sigm_arg:  [  1.97097825e-06   1.47031858e-06]\n",
      "90:\n",
      "self.sigm_arg:  [  2.34049935e-06   1.47827154e-06]\n",
      "91:\n",
      "self.sigm_arg:  [  1.33410012e-07   1.47321134e-06]\n",
      "92:\n",
      "self.sigm_arg:  [  1.33396966e-07   1.47321134e-06]\n",
      "93:\n",
      "self.sigm_arg:  [  2.88155252e-06   1.47784795e-06]\n",
      "94:\n",
      "self.sigm_arg:  [  1.97098984e-06   1.47031869e-06]\n",
      "95:\n",
      "self.sigm_arg:  [  3.38420182e-06   1.47718765e-06]\n",
      "96:\n",
      "self.sigm_arg:  [ -8.10416168e-07   1.01170974e-06]\n",
      "97:\n",
      "self.sigm_arg:  [ -2.62585615e-07   1.01170929e-06]\n",
      "98:\n",
      "self.sigm_arg:  [  2.49986056e-06   1.82702433e-06]\n",
      "99:\n",
      "self.sigm_arg:  [  1.33496911e-07   1.47320975e-06]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "~k&ýÆ_à÷\"Kl¼¬¡p/& {l÷Õ>äÜC^ÀHOO¯ÓàuØïb76yj[`O¨»µl¼ÙæìãõÑ*5@¢¥¿I>`bßÅ½Ê°)Âý£°!ä)½Õ(×<_îu°3áÿ-)æLuÖø»s\n",
      "********************\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      "ertarian communism]] developin\n",
      "self.train_hard_sigm_arg:  [[ 0.09114773 -0.05103997]\n",
      " [-0.00380561 -0.00345707]\n",
      " [ 0.08327354 -0.05467166]\n",
      " [-0.00236743 -0.00317528]\n",
      " [ 0.05084588 -0.06563801]\n",
      " [ 0.0007868  -0.044589  ]\n",
      " [-0.00480901 -0.00375309]\n",
      " [ 0.05032714 -0.05587668]\n",
      " [-0.00136583 -0.00306868]\n",
      " [-0.00493022 -0.00306868]\n",
      " [ 0.08193897 -0.03475651]\n",
      " [-0.00847143 -0.00456471]\n",
      " [ 0.07773869 -0.06266084]\n",
      " [-0.00029114 -0.00266051]\n",
      " [ 0.05444167 -0.05457102]\n",
      " [-0.00218564 -0.00311722]\n",
      " [ 0.05306218 -0.07745686]\n",
      " [ 0.00355711 -0.06398599]\n",
      " [ 0.0014688   0.00858366]\n",
      " [-0.02149331 -0.02740215]\n",
      " [-0.02137879 -0.00654503]\n",
      " [-0.00651525 -0.00654503]\n",
      " [ 0.12370562 -0.05094815]\n",
      " [-0.00509963 -0.00360932]\n",
      " [ 0.1969973  -0.0402548 ]\n",
      " [-0.00727493 -0.00431585]\n",
      " [ 0.0866958  -0.03575679]\n",
      " [-0.00796024 -0.00456441]\n",
      " [ 0.04641347 -0.04508184]\n",
      " [-0.00531559 -0.00385464]]\n",
      "Average loss at step 100: 3.609587 learning rate: 0.002582\n",
      "Percentage_of correct: 13.54%\n",
      "0:\n",
      "self.sigm_arg:  [-0.00382631 -0.00375487]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.07062397 -0.04350886]\n",
      "2:\n",
      "self.sigm_arg:  [-0.00648209 -0.00407157]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.08278147 -0.05182783]\n",
      "4:\n",
      "self.sigm_arg:  [-0.0022166  -0.00343866]\n",
      "5:\n",
      "self.sigm_arg:  [-0.00631814 -0.00343866]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.05233883 -0.05595928]\n",
      "7:\n",
      "self.sigm_arg:  [-0.00096386 -0.00307756]\n",
      "8:\n",
      "self.sigm_arg:  [-0.00567286 -0.00307756]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.03318634 -0.0444255 ]\n",
      "10:\n",
      "self.sigm_arg:  [-0.0059598  -0.00383396]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.07023266 -0.04362573]\n",
      "12:\n",
      "self.sigm_arg:  [-0.0060403  -0.00394291]\n",
      "13:\n",
      "self.sigm_arg:  [ 0.04906537 -0.06982796]\n",
      "14:\n",
      "self.sigm_arg:  [ 0.00266729 -0.00083506]\n",
      "15:\n",
      "self.sigm_arg:  [-0.01818083 -0.00712114]\n",
      "16:\n",
      "self.sigm_arg:  [ 0.00505118 -0.04857376]\n",
      "17:\n",
      "self.sigm_arg:  [-0.00605155 -0.00357775]\n",
      "18:\n",
      "self.sigm_arg:  [ 0.1251277   0.03347989]\n",
      "19:\n",
      "self.sigm_arg:  [-0.03116739 -0.01478489]\n",
      "20:\n",
      "self.sigm_arg:  [-0.01088745 -0.00684329]\n",
      "21:\n",
      "self.sigm_arg:  [ 0.00942353 -0.00476272]\n",
      "22:\n",
      "self.sigm_arg:  [-0.01742302 -0.00702869]\n",
      "23:\n",
      "self.sigm_arg:  [ 0.01596227 -0.04663474]\n",
      "24:\n",
      "self.sigm_arg:  [-0.00574142 -0.00378717]\n",
      "25:\n",
      "self.sigm_arg:  [ 0.08092277 -0.05544274]\n",
      "26:\n",
      "self.sigm_arg:  [-0.00248193 -0.00315186]\n",
      "27:\n",
      "self.sigm_arg:  [ 0.04299714 -0.06635588]\n",
      "28:\n",
      "self.sigm_arg:  [ 0.00189039 -0.00091291]\n",
      "29:\n",
      "self.sigm_arg:  [-0.0190706  -0.00713196]\n",
      "30:\n",
      "self.sigm_arg:  [ 0.03681048 -0.135498  ]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.01747657  0.00047049]\n",
      "32:\n",
      "self.sigm_arg:  [-0.01679688 -0.02091196]\n",
      "33:\n",
      "self.sigm_arg:  [-0.01853811 -0.00669532]\n",
      "34:\n",
      "self.sigm_arg:  [ 0.19941783 -0.02759492]\n",
      "35:\n",
      "self.sigm_arg:  [-0.01217006 -0.00535495]\n",
      "36:\n",
      "self.sigm_arg:  [ 0.08534229 -0.0442432 ]\n",
      "37:\n",
      "self.sigm_arg:  [-0.00637421 -0.00399622]\n",
      "38:\n",
      "self.sigm_arg:  [ 0.07011345 -0.04343398]\n",
      "39:\n",
      "self.sigm_arg:  [-0.00593832 -0.00398431]\n",
      "40:\n",
      "self.sigm_arg:  [ 0.04239547 -0.05696591]\n",
      "41:\n",
      "self.sigm_arg:  [-0.00077967 -0.00300792]\n",
      "42:\n",
      "self.sigm_arg:  [-0.00382134 -0.00300792]\n",
      "43:\n",
      "self.sigm_arg:  [-0.03289672 -0.00300792]\n",
      "44:\n",
      "self.sigm_arg:  [-0.06417985 -0.00300792]\n",
      "45:\n",
      "self.sigm_arg:  [-0.05958693 -0.00300792]\n",
      "46:\n",
      "self.sigm_arg:  [-0.06304706 -0.00300792]\n",
      "47:\n",
      "self.sigm_arg:  [-0.05683862 -0.00300792]\n",
      "48:\n",
      "self.sigm_arg:  [-0.00830393 -0.00300792]\n",
      "49:\n",
      "self.sigm_arg:  [ 0.02182039 -0.00090182]\n",
      "50:\n",
      "self.sigm_arg:  [-0.01853948 -0.00716294]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.01314352 -0.04895391]\n",
      "52:\n",
      "self.sigm_arg:  [-0.00534202 -0.00356524]\n",
      "53:\n",
      "self.sigm_arg:  [ 0.09133402 -0.07587499]\n",
      "54:\n",
      "self.sigm_arg:  [ 0.00246532 -0.05396958]\n",
      "55:\n",
      "self.sigm_arg:  [-0.00155233 -0.00306044]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.00636664 -0.04484871]\n",
      "57:\n",
      "self.sigm_arg:  [-0.00570631 -0.0037681 ]\n",
      "58:\n",
      "self.sigm_arg:  [ 0.06468207 -0.05569196]\n",
      "59:\n",
      "self.sigm_arg:  [-0.00297299 -0.00308894]\n",
      "60:\n",
      "self.sigm_arg:  [ 0.07255144  0.00020417]\n",
      "61:\n",
      "self.sigm_arg:  [-0.02001418 -0.02403851]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.04576028 -0.05955075]\n",
      "63:\n",
      "self.sigm_arg:  [-0.00147133 -0.00296305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64:\n",
      "self.sigm_arg:  [ 0.05036789 -0.06583746]\n",
      "65:\n",
      "self.sigm_arg:  [-0.00028224 -0.00241312]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.08015585 -0.13537361]\n",
      "67:\n",
      "self.sigm_arg:  [ 0.01633691 -0.04417491]\n",
      "68:\n",
      "self.sigm_arg:  [-0.00508771 -0.00340331]\n",
      "69:\n",
      "self.sigm_arg:  [ 0.099842   -0.05366074]\n",
      "70:\n",
      "self.sigm_arg:  [-0.00185816 -0.00323109]\n",
      "71:\n",
      "self.sigm_arg:  [-0.00689201 -0.00323109]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.08170345 -0.03504641]\n",
      "73:\n",
      "self.sigm_arg:  [-0.0092513  -0.00453848]\n",
      "74:\n",
      "self.sigm_arg:  [ 0.07638346 -0.06209304]\n",
      "75:\n",
      "self.sigm_arg:  [-0.00098947 -0.00269036]\n",
      "76:\n",
      "self.sigm_arg:  [ 0.05383263 -0.05514825]\n",
      "77:\n",
      "self.sigm_arg:  [-0.00256934 -0.00307399]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.0522738  -0.07793839]\n",
      "79:\n",
      "self.sigm_arg:  [ 0.00308124 -0.05781172]\n",
      "80:\n",
      "self.sigm_arg:  [-0.00245183 -0.00278277]\n",
      "81:\n",
      "self.sigm_arg:  [ 0.10245616  0.00196758]\n",
      "82:\n",
      "self.sigm_arg:  [-0.02072463 -0.0256677 ]\n",
      "83:\n",
      "self.sigm_arg:  [ 0.04797843 -0.05942878]\n",
      "84:\n",
      "self.sigm_arg:  [-0.00224899 -0.00297525]\n",
      "85:\n",
      "self.sigm_arg:  [ 0.08149791 -0.06690482]\n",
      "86:\n",
      "self.sigm_arg:  [ 0.0005628  -0.04512492]\n",
      "87:\n",
      "self.sigm_arg:  [-0.00670527 -0.00372614]\n",
      "88:\n",
      "self.sigm_arg:  [ 0.12210175 -0.04833783]\n",
      "89:\n",
      "self.sigm_arg:  [-0.00345879 -0.00364492]\n",
      "90:\n",
      "self.sigm_arg:  [-0.00509191 -0.00364492]\n",
      "91:\n",
      "self.sigm_arg:  [-0.01341322 -0.00364492]\n",
      "92:\n",
      "self.sigm_arg:  [ 0.07744083 -0.06097605]\n",
      "93:\n",
      "self.sigm_arg:  [-0.00112    -0.00273914]\n",
      "94:\n",
      "self.sigm_arg:  [ 0.05266029 -0.00011743]\n",
      "95:\n",
      "self.sigm_arg:  [-0.01804709 -0.0072598 ]\n",
      "96:\n",
      "self.sigm_arg:  [-0.02801006 -0.0072598 ]\n",
      "97:\n",
      "self.sigm_arg:  [ 0.04150202 -0.07326113]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.0005814  -0.09634956]\n",
      "99:\n",
      "self.sigm_arg:  [ 0.00773678 -0.06341279]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "u e[Ht]hbcmmdturr·i  1ilo'dSne/bidy\n",
      " oitv0áe2ghdlg.einybe  bi esdio .=ot  behsb[=t/tol ia.slnee.pce\n",
      "********************\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      "hism as a movement and as a ph\n",
      "self.train_hard_sigm_arg:  [[ 0.18500136 -0.21118066]\n",
      " [ 0.0241675  -0.00336762]\n",
      " [ 0.04622149 -0.09028275]\n",
      " [ 0.06361159 -0.09176589]\n",
      " [-0.02295149  0.00286489]\n",
      " [-0.05262667 -0.05613685]\n",
      " [ 0.06540184 -0.08843099]\n",
      " [-0.01644962  0.00181563]\n",
      " [-0.0560567  -0.05284491]\n",
      " [-0.02266992 -0.00563883]\n",
      " [-0.00385332 -0.00563883]\n",
      " [ 0.16149397  0.01003687]\n",
      " [ 0.19918364 -0.28779021]\n",
      " [ 0.04565106 -0.01496438]\n",
      " [ 0.05526752 -0.09098982]\n",
      " [ 0.00806732 -0.01668616]\n",
      " [ 0.01083587 -0.05004783]\n",
      " [ 0.095744   -0.14036207]\n",
      " [-0.01025786  0.00560272]\n",
      " [-0.04946493 -0.05179206]\n",
      " [ 0.03348011 -0.04920821]\n",
      " [ 0.10769597 -0.14586718]\n",
      " [-0.00368941  0.00628242]\n",
      " [-0.04926591 -0.05230911]\n",
      " [ 0.06541319 -0.08854585]\n",
      " [-0.01637704  0.0018299 ]\n",
      " [-0.05604566 -0.0505081 ]\n",
      " [-0.0226697  -0.00571335]\n",
      " [ 0.0018812  -0.09565224]\n",
      " [ 0.18093511 -0.20896433]]\n",
      "Average loss at step 200: 3.324502 learning rate: 0.002582\n",
      "Percentage_of correct: 12.14%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.09920327 -0.13768725]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.02938693 -0.0149564 ]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.06028094 -0.10633788]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.01503273 -0.01601665]\n",
      "4:\n",
      "self.sigm_arg:  [-0.04328306 -0.00461922]\n",
      "5:\n",
      "self.sigm_arg:  [-0.06364658 -0.00461922]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.01759216 -0.04680229]\n",
      "7:\n",
      "self.sigm_arg:  [-0.03369531 -0.00234934]\n",
      "8:\n",
      "self.sigm_arg:  [-0.03974207 -0.00234934]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.04978211 -0.01166669]\n",
      "10:\n",
      "self.sigm_arg:  [ 0.08354504 -0.13953444]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.02268411 -0.01578863]\n",
      "12:\n",
      "self.sigm_arg:  [-0.01944131 -0.00393834]\n",
      "13:\n",
      "self.sigm_arg:  [ 0.07951123 -0.10250806]\n",
      "14:\n",
      "self.sigm_arg:  [-0.01732966  0.00256036]\n",
      "15:\n",
      "self.sigm_arg:  [-0.06290109 -0.02502432]\n",
      "16:\n",
      "self.sigm_arg:  [-0.0434382  -0.00655806]\n",
      "17:\n",
      "self.sigm_arg:  [ 0.09894712 -0.14613955]\n",
      "18:\n",
      "self.sigm_arg:  [-0.02797019  0.00635665]\n",
      "19:\n",
      "self.sigm_arg:  [-0.15212651 -0.03531916]\n",
      "20:\n",
      "self.sigm_arg:  [-0.07773825 -0.0061985 ]\n",
      "21:\n",
      "self.sigm_arg:  [-0.0637776 -0.0061985]\n",
      "22:\n",
      "self.sigm_arg:  [-0.03392542 -0.0061985 ]\n",
      "23:\n",
      "self.sigm_arg:  [ 0.08040766 -0.00604769]\n",
      "24:\n",
      "self.sigm_arg:  [ 0.0782785  -0.12948678]\n",
      "25:\n",
      "self.sigm_arg:  [ 0.1274893  -0.14284189]\n",
      "26:\n",
      "self.sigm_arg:  [ 0.01063935  0.00423574]\n",
      "27:\n",
      "self.sigm_arg:  [ 0.0519585  -0.14431719]\n",
      "28:\n",
      "self.sigm_arg:  [-0.00549007  0.00395407]\n",
      "29:\n",
      "self.sigm_arg:  [-0.05878109 -0.04537465]\n",
      "30:\n",
      "self.sigm_arg:  [ 0.08780045 -0.14679794]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.00171317  0.02255017]\n",
      "32:\n",
      "self.sigm_arg:  [-0.05722138 -0.0463759 ]\n",
      "33:\n",
      "self.sigm_arg:  [ 0.16251549 -0.24703522]\n",
      "34:\n",
      "self.sigm_arg:  [ 0.03167902  0.00566605]\n",
      "35:\n",
      "self.sigm_arg:  [ 0.05855997 -0.15211806]\n",
      "36:\n",
      "self.sigm_arg:  [ 0.02394717 -0.01504838]\n",
      "37:\n",
      "self.sigm_arg:  [ 0.08891704 -0.13924631]\n",
      "38:\n",
      "self.sigm_arg:  [ 0.01401312 -0.00236013]\n",
      "39:\n",
      "self.sigm_arg:  [-0.03329277 -0.00544461]\n",
      "40:\n",
      "self.sigm_arg:  [ 0.00700118 -0.04793189]\n",
      "41:\n",
      "self.sigm_arg:  [-0.03346115 -0.00225967]\n",
      "42:\n",
      "self.sigm_arg:  [-0.1034861  -0.00225967]\n",
      "43:\n",
      "self.sigm_arg:  [-0.27033538 -0.00225967]\n",
      "44:\n",
      "self.sigm_arg:  [-0.45677441 -0.00225967]\n",
      "45:\n",
      "self.sigm_arg:  [-0.39920023 -0.00225967]\n",
      "46:\n",
      "self.sigm_arg:  [-0.37245274 -0.00225967]\n",
      "47:\n",
      "self.sigm_arg:  [-0.20310611 -0.00225967]\n",
      "48:\n",
      "self.sigm_arg:  [ 0.03426745  0.04217827]\n",
      "49:\n",
      "self.sigm_arg:  [-0.06985003 -0.04849727]\n",
      "50:\n",
      "self.sigm_arg:  [-0.01669692 -0.00576047]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.14872162 -0.01098931]\n",
      "52:\n",
      "self.sigm_arg:  [ 0.08595421 -0.13631801]\n",
      "53:\n",
      "self.sigm_arg:  [ 0.08489566 -0.08986062]\n",
      "54:\n",
      "self.sigm_arg:  [ 0.01266214 -0.01619207]\n",
      "55:\n",
      "self.sigm_arg:  [ 0.07562203 -0.12892315]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.0187405  -0.01604829]\n",
      "57:\n",
      "self.sigm_arg:  [-0.01380812 -0.00400299]\n",
      "58:\n",
      "self.sigm_arg:  [ 0.05139672 -0.04597612]\n",
      "59:\n",
      "self.sigm_arg:  [ 0.09510286 -0.14123869]\n",
      "60:\n",
      "self.sigm_arg:  [-0.00766359  0.00526221]\n",
      "61:\n",
      "self.sigm_arg:  [-0.04595397 -0.05224467]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.02763551 -0.04769203]\n",
      "63:\n",
      "self.sigm_arg:  [-0.00565307 -0.00235096]\n",
      "64:\n",
      "self.sigm_arg:  [ 0.08059063 -0.10264013]\n",
      "65:\n",
      "self.sigm_arg:  [ 0.11126835 -0.13426231]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.18907838 -0.21283568]\n",
      "67:\n",
      "self.sigm_arg:  [ 0.02439631 -0.00200378]\n",
      "68:\n",
      "self.sigm_arg:  [ 0.04504497 -0.09025909]\n",
      "69:\n",
      "self.sigm_arg:  [ 0.10679357 -0.14301647]\n",
      "70:\n",
      "self.sigm_arg:  [-0.00884172  0.00554191]\n",
      "71:\n",
      "self.sigm_arg:  [ 0.05564472 -0.17952837]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.01451323  0.00488719]\n",
      "73:\n",
      "self.sigm_arg:  [ 0.04166576 -0.13740052]\n",
      "74:\n",
      "self.sigm_arg:  [ 0.08093068 -0.09374831]\n",
      "75:\n",
      "self.sigm_arg:  [ 0.01438088 -0.01582513]\n",
      "76:\n",
      "self.sigm_arg:  [ 0.01059303 -0.04874926]\n",
      "77:\n",
      "self.sigm_arg:  [-0.01376781 -0.00217998]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.05778759 -0.08434181]\n",
      "79:\n",
      "self.sigm_arg:  [ 0.10936703 -0.14222878]\n",
      "80:\n",
      "self.sigm_arg:  [ 0.07982962 -0.09076257]\n",
      "81:\n",
      "self.sigm_arg:  [-0.01837348  0.00296706]\n",
      "82:\n",
      "self.sigm_arg:  [-0.05222126 -0.05473236]\n",
      "83:\n",
      "self.sigm_arg:  [ 0.01748171 -0.04772956]\n",
      "84:\n",
      "self.sigm_arg:  [ 0.09500606 -0.13159165]\n",
      "85:\n",
      "self.sigm_arg:  [ 0.09543404 -0.10690442]\n",
      "86:\n",
      "self.sigm_arg:  [ 0.01640803 -0.01611502]\n",
      "87:\n",
      "self.sigm_arg:  [ 0.09266064 -0.14723213]\n",
      "88:\n",
      "self.sigm_arg:  [ 0.0220493  -0.01602337]\n",
      "89:\n",
      "self.sigm_arg:  [-0.041757  -0.0034903]\n",
      "90:\n",
      "self.sigm_arg:  [-0.08084468 -0.0034903 ]\n",
      "91:\n",
      "self.sigm_arg:  [ 0.0008737  -0.08942525]\n",
      "92:\n",
      "self.sigm_arg:  [ 0.06829438 -0.09506432]\n",
      "93:\n",
      "self.sigm_arg:  [ 0.00326797 -0.0084433 ]\n",
      "94:\n",
      "self.sigm_arg:  [-0.04749777 -0.00522068]\n",
      "95:\n",
      "self.sigm_arg:  [-0.08108521 -0.00522068]\n",
      "96:\n",
      "self.sigm_arg:  [-0.06960433 -0.00522068]\n",
      "97:\n",
      "self.sigm_arg:  [ 0.04902806 -0.10312395]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.11853732 -0.14973193]\n",
      "99:\n",
      "self.sigm_arg:  [ 0.08409894 -0.09593604]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "e]cy6fCIRte]otd&C 9fbUndce  t enDo/oui ]\n",
      "cb[*[r -nemi aottdaoter ieocttemaan.g hpna h]tiFsolotf olca\n",
      "********************\n",
      "Validation percentage of correct: 12.80%\n",
      "\n",
      "Pickling first.pickle\n",
      "Number of steps = 201     Percentage = 1.28%     Time = 43s     Learning rate = 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=201,\n",
    "            add_operations=['self.train_hard_sigm_arg'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200],\n",
    "            path_to_file_for_saving_collection='first.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='plotting_check.txt')\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
