{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.ops.rnn_cell \n",
    "from tensorflow.python.framework import registry\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not one byte characters:  0\n",
      "min order index:  9\n",
      "max order index:  255\n",
      "total number of characters:  196\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99371900 , Zeno &quot;repudiated the omnipotence of the state, its interv\n",
      "4000 <mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmln\n",
      "4000 reserve\">{{Anarchism}}\n",
      "'''Anarchism''' originated as a term of a\n",
      "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\n",
      "  <siteinfo>\n",
      "    <sitename>Wikipedia</sitename>\n",
      "    <base>http://en.wikipedia.org/wiki/Main_Page</base>\n",
      "    <generator>MediaWiki 1.6alpha</generator>\n",
      "    <case>first-letter</case>\n",
      "      <namespaces>\n",
      "      <namespace key=\"-2\">Media</namespace>\n",
      "      <namespace key=\"-1\">Special</namespace>\n",
      "      <namespace key=\"0\" />\n",
      "      <namespace key=\"1\">Talk</namespace>\n",
      "      <namespace key=\"2\">User</namespace>\n",
      "      <namespace key=\"3\">User talk</namespace>\n",
      "      <namespace key=\"4\">Wikipedia</namespace>\n",
      "      <namespace key=\"5\">Wikipedia talk</namespace>\n",
      "      <namespace key=\"6\">Image</namespace>\n",
      "      <namespace key=\"7\">Image talk</namespace>\n",
      "      <namespace key=\"8\">MediaWiki</namespace>\n",
      "      <namespace key=\"9\">MediaWiki talk</namespace>\n",
      "      <namespace key=\"10\">Template</namespace>\n",
      "      <namespace key=\"11\">Template talk</namespace>\n",
      "      <namespace key=\"12\">Help</namespace>\n",
      "      <namespace key=\"13\">Help talk</namespace>\n",
      "      <namespace key=\"14\">Category</namespace>\n",
      "      <namespace key=\"15\">Category talk</namespace>\n",
      "      <namespace key=\"100\">Portal</namespace>\n",
      "      <namespace key=\"101\">Portal talk</namespace>\n",
      "    </namespaces>\n",
      "  </siteinfo>\n",
      "  <page>\n",
      "    <title>AaA</title>\n",
      "    <id>1</id>\n",
      "    <revision>\n",
      "      <id>32899315</id>\n",
      "      <timestamp>2005-12-27T18:46:47Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Jsmethers</username>\n",
      "        <id>614213</id>\n",
      "      </contributor>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[AAA]]</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AlgeriA</title>\n",
      "    <id>5</id>\n",
      "    <revision>\n",
      "      <id>18063769</id>\n",
      "      <timestamp>2005-07-03T11:13:13Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Docu</username>\n",
      "        <id>8029</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>adding cur_id=5: {{R from CamelCase}}</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Algeria]]{{R from CamelCase}}</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AmericanSamoa</title>\n",
      "    <id>6</id>\n",
      "    <revision>\n",
      "      <id>18063795</id>\n",
      "      <timestamp>2005-07-03T11:14:17Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Docu</username>\n",
      "        <id>8029</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>adding to cur_id=6  {{R from CamelCase}}</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[American Samoa]]{{R from CamelCase}}</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AppliedEthics</title>\n",
      "    <id>8</id>\n",
      "    <revision>\n",
      "      <id>15898943</id>\n",
      "      <timestamp>2002-02-25T15:43:11Z</timestamp>\n",
      "      <contributor>\n",
      "        <ip>Conversion script</ip>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>Automated conversion</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Applied ethics]]\n",
      "</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AccessibleComputing</title>\n",
      "    <id>10</id>\n",
      "    <revision>\n",
      "      <id>15898945</id>\n",
      "      <timestamp>2003-04-25T22:18:38Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Ams80</username>\n",
      "        <id>7543</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <comment>Fixing redirect</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Accessible_computing]]</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>AdA</title>\n",
      "    <id>11</id>\n",
      "    <revision>\n",
      "      <id>15898946</id>\n",
      "      <timestamp>2002-09-22T16:02:58Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Andre Engels</username>\n",
      "        <id>300</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Ada programming language]]</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>Anarchism</title>\n",
      "    <id>12</id>\n",
      "    <revision>\n",
      "      <id>42136831</id>\n",
      "      <timestamp>2006-03-04T01:41:25Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>CJames745</username>\n",
      "        <id>832382</id>\n",
      "      </contributor>\n",
      "      <min\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reserve\">{{Anarchism}}\n",
      "'''Anarchism''' originated as a term of abuse first used against early [[working class]] [[radical]]s including the [[Diggers]] of the [[English Revolution]] and the [[sans-culotte|''sans-culottes'']] of the [[French Revolution]].[http://uk.encarta.msn.com/encyclopedia_761568770/Anarchism.html] Whilst the term is still used in a pejorative way to describe ''&quot;any act that used violent means to destroy the organization of society&quot;''&lt;ref&gt;[http://www.cas.sc.edu/socy/faculty/deflem/zhistorintpolency.html History of International Police Cooperation], from the final protocols of the &quot;International Conference of Rome for the Social Defense Against Anarchists&quot;, 1898&lt;/ref&gt;, it has also been taken up as a positive label by self-defined anarchists.\n",
      "\n",
      "The word '''anarchism''' is [[etymology|derived from]] the [[Greek language|Greek]] ''[[Wiktionary:&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&amp;#945;|&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&amp;#945;]]'' (&quot;without [[archon]]s (ruler, chief, king)&quot;). Anarchism as a [[political philosophy]], is the belief that ''rulers'' are unnecessary and should be abolished, although there are differing interpretations of what this means. Anarchism also refers to related [[social movement]]s) that advocate the elimination of authoritarian institutions, particularly the [[state]].&lt;ref&gt;[http://en.wikiquote.org/wiki/Definitions_of_anarchism Definitions of anarchism] on Wikiquote, accessed 2006&lt;/ref&gt; The word &quot;[[anarchy]],&quot; as most anarchists use it, does not imply [[chaos]], [[nihilism]], or [[anomie]], but rather a harmonious [[anti-authoritarian]] society. In place of what are regarded as authoritarian political structures and coercive economic institutions, anarchists advocate social relations based upon [[voluntary association]] of autonomous individuals, [[mutual aid]], and [[self-governance]]. \n",
      "    \n",
      "While anarchism is most easily defined by what it is against, anarchists also offer positive visions of what they believe to be a truly free society. However, ideas about how an anarchist society might work vary considerably, especially with respect to economics; there is also disagreement about how a free society might be brought about. \n",
      "\n",
      "== Origins and predecessors ==\n",
      "\n",
      "[[Peter Kropotkin|Kropotkin]], and others, argue that before recorded [[history]], human society was organized on anarchist principles.&lt;ref&gt;[[Peter Kropotkin|Kropotkin]], Peter. ''&quot;[[Mutual Aid: A Factor of Evolution]]&quot;'', 1902.&lt;/ref&gt; Most anthropologists follow Kropotkin and Engels in believing that hunter-gatherer bands were egalitarian and lacked division of labour, accumulated wealth, or decreed law, and had equal access to resources.&lt;ref&gt;[[Friedrich Engels|Engels]], Freidrich. ''&quot;[http://www.marxists.org/archive/marx/works/1884/origin-family/index.htm Origins of the Family, Private Property, and the State]&quot;'', 1884.&lt;/ref&gt;\n",
      "[[Image:WilliamGodwin.jpg|thumb|right|150px|William Godwin]]\n",
      "\n",
      "Anarchists including the [[The Anarchy Organisation]] and [[Murray Rothbard|Rothbard]] find anarchist attitudes in [[Taoism]] from [[History of China|Ancient China]].&lt;ref&gt;The Anarchy Organization (Toronto). ''Taoism and Anarchy.'' [[April 14]] [[2002]] [http://www.toxicpop.co.uk/library/taoism.htm Toxicpop mirror] [http://www.geocities.com/SoHo/5705/taoan.html Vanity site mirror]&lt;/ref&gt;&lt;ref&gt;[[Murray Rothbard|Rothbard]], Murray. ''&quot;[http://www.lewrockwell.com/rothbard/ancient-chinese.html The Ancient Chinese Libertarian Tradition]&quot;'', an extract from ''&quot;[http://www.mises.org/journals/jls/9_2/9_2_3.pdf Concepts of the Role of Intellectuals in Social Change Toward Laissez Faire]&quot;'', The Journal of Libertarian Studies, 9 (2) Fall 1990.&lt;/ref&gt; [[Peter Kropotkin|Kropotkin]] found similar ideas in [[stoicism|stoic]] [[Zeno of Citium]]. According to Kropotkin\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size_1, valid_text_1[:64])\n",
    "print(valid_size_2, valid_text_2[:64])\n",
    "print(valid_text_1)\n",
    "print('\\n\\n\\n')\n",
    "print(valid_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99335000  also:'' [[Insurrectionary anarchism]]\n",
      "\n",
      "*'''Small 'a' anarchism'\n",
      "25000 ture in Mutual Aid: A Factor of Evolution (1897). Subsequent ana\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 25000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ\n",
      "char2id(u'a') = 67,  char2id(u'z') = 92,  char2id(u' ') = 2\n",
      "id2char(78) = l,  id2char(156) = Ø,  id2char(140) = È\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" also:'' [[\", 'om its burr', 'on}}\\n\\n[[ar:', 'i:Itävaltal', '} while Hug', 'e for himse', 'e border=1&', 'a entry for', 'ands nation', 't he could ', 'ters with m', '981-1989\\n*[', 'special har', 'owever, bar', 'ion. While ', 'still a pit', '; The Oriol', \"he former '\", 'to run for ', 'ld, white a', 'pendent app', 'me>Chlewbot', 'List of Ont', ' the death ', '|none}}\\n\\nDe', ' magazine e', 'cooked usin', 'the kingdom', 'connaissanc', 'spute withi', 'sisting of ', ' starting w', 'heart rhyth', 'erences ==\\n', 'ode, circa ', '(520)\\n\\n==Re', 'Genialia pe', \"resley's re\", 'd of electr', '] (ACWP) or', 'ade of whic', 'erally, it ', 'omini</titl', ' of the gre', 'or between ', \"eper]''', a\", \"s]]'' \\n|[[W\", 'rmative Yea', 'he digraphs', \"'' or  ''He\", 'ster enjoys', 'tioning and', 'e &amp; the', 't, [[Ambros', 'the crowd w', 'hands of th', ' Heads of G', '. In the Ho', 'greed to we', 'but it does', 's also take', ' Albany|New', 'quot;1&quot', 'as added, c']\n",
      "['[Insurrecti', 'row in the ', ':]]\\n[[ast:A', 'lainen talo', 'go Grotius ', 'elf, by neg', '&gt;&lt;tr&', 'r Antonio C', 'nal footbal', ' make far m', 'mulitple in', '[[Syngman R', 'rdware such', 'rium does n', ' such condi', 'tcher, McGr', 'les knew th', \"''départeme\", ' mayor, som', 'and blue\\n| ', 'plications.', 't</username', 'tario Colle', ' penalty in', 'espite the ', 'established', 'ng rice. Ri', 'm in Northu', 'ce]], sweep', 'in Christia', ' the &quot;', 'with vowels', 'hm. It is t', '\\n\\n* [[Andre', ' 2005.]]  P', 'eferences==', 'ersonally d', 'eturn from ', 'ronic music', 'r [[effort ', 'ch, for Kub', ' can also b', 'le>\\n    <id', 'eat British', ' categories', 'an [[Associ', 'West German', 'ars, 1902  ', \"s ''ai'' an\", \"ellás''), o\", 's killing t', 'd the gravi', 'e [[New Wav', 'se Rokewood', 'with their ', 'he Scottish', 'Government ', 'ouse of Lor', 'ed Anne. On', 's offer imp', 'en on cases', 'w Albany]]\\n', 't;&gt;(NA)&', 'creating al']\n",
      "['<m']\n",
      "['me']\n"
     ]
    }
   ],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text_1,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix):\n",
    "        with tf.name_scope('L2_norm'):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\"+appendix)\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=True,\n",
    "                                     name=\"reduce_mean_in_L2_norm\"+appendix)\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\"+appendix)\n",
    "    \n",
    "    def step_function(self,\n",
    "                      inp_tensor,\n",
    "                      appendix):\n",
    "        sign_res = tf.sign(inp_tensor, name=\"sign_in_step_function\"+appendix)\n",
    "        add_res = tf.add(sign_res, 1., name=\"add_in_step_function\"+appendix)\n",
    "        return tf.divide(add_res, 2., name=\"step_func\"+appendix)\n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       emb_idx,\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down,   # A tensor z^{l-1}_t\n",
    "                       appendix):\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.transpose(tf.multiply(tf.transpose(state[2],\n",
    "                                                                       name=\"transposed_state2_in_top_down_prepaired\"+appendix),\n",
    "                                                          tf.transpose(top_down,\n",
    "                                                                       name=\"transposed_top_down_in_top_down_prepaired\"+appendix),\n",
    "                                                          name=\"multiply_in_top_down_prepaired\"+appendix),\n",
    "                                              name=\"top_down_prepaired\"+appendix)\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"+appendix),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"+appendix),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"+appendix),\n",
    "                                               name=\"bottom_up_prepaired\"+appendix)\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([state[0], top_down_prepaired, bottom_up_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\"+appendix)\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"+appendix),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\"+appendix)\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\"+appendix)\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    0,\n",
    "                                                    \"_hard_sigm\"+appendix)\n",
    "            \n",
    "            gate_concat = self.step_function(sigmoid_arg, \"_gate_concat\"+appendix)\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.sign(tanh_arg, name=\"modification_vector\"+appendix)\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.debug_compute_boundary_state(hard_sigm_arg,\n",
    "                                                               idx,\n",
    "                                                               appendix) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"+appendix),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"+appendix),\n",
    "                                                                      name=\"logical_and_in_update_flag\"+appendix),\n",
    "                                                       name=\"to_float_in_update_flag\"+appendix),\n",
    "                                           name=\"update_flag\"+appendix)\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"+appendix),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"+appendix),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"+appendix),\n",
    "                                                     name=\"to_float_in_copy_flag\"+appendix),\n",
    "                                         name=\"copy_flag\"+appendix)\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"+appendix),\n",
    "                                                      name=\"to_float_in_flush_flag\"+appendix),\n",
    "                                          name=\"flush_flag\"+appendix)\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\"+appendix)\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\"+appendix)\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\"+appendix)\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\"+appendix)\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\"+appendix)\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\"+appendix)\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"+appendix),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"+appendix),\n",
    "                                                 name=\"add_in_update_term\"+appendix),\n",
    "                                          name=\"update_term\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\"+appendix)\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tr_modification_vector,\n",
    "                                         name=\"flush_term\"+appendix) \n",
    "                \n",
    "                \"\"\"flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"+appendix),\n",
    "                                         name=\"flush_term\"+appendix)\"\"\"\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"+appendix),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\"+appendix)\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\"+appendix)\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\"+appendix)\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"+appendix),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"+appendix),\n",
    "                                        tf.sign(tr_new_memory, name=\"tanh_in_else_term\"+appendix),\n",
    "                                        name=\"else_term\"+appendix)\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"+appendix),\n",
    "                                          name=\"new_hidden\"+appendix)\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down,   # A tensor z^{L-1}_t\n",
    "                   appendix):\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"+appendix),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"+appendix),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"+appendix),\n",
    "                                               name=\"bottom_up_prepaired\"+appendix)\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([state[0], bottom_up_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\"+appendix)                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"+appendix),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\"+appendix)\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\"+appendix)                                          \n",
    "            gate_concat = self.step_function(sigmoid_arg, \"_gate_concat\"+appendix)\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.sign(tanh_arg, name=\"modification_vector\"+appendix)\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"+appendix),\n",
    "                                                       name=\"to_float_in_update_flag\"+appendix),\n",
    "                                           name=\"update_flag\"+appendix)\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\"+appendix)\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\"+appendix)\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\"+appendix)\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\"+appendix)\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\"+appendix)\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\"+appendix)\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\"+appendix)\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"+appendix),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"+appendix),\n",
    "                                                 name=\"add_in_update_term\"+appendix),\n",
    "                                          name=\"update_term\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\"+appendix)\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\"+appendix)\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\"+appendix)\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\"+appendix)\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\"+appendix)\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"+appendix),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"+appendix),\n",
    "                                        tf.sign(tr_new_memory, name=\"tanh_in_else_term\"+appendix),\n",
    "                                        name=\"else_term\"+appendix)\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"+appendix),\n",
    "                                          name=\"new_hidden\"+appendix)\n",
    "        return new_hidden, new_memory\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X,\n",
    "                               appendix):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": \"HardSigmoid\"}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\"+appendix)       \n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"+appendix),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\"+appendix)\n",
    "        return X\n",
    "    \n",
    "    def debug_compute_boundary_state(self,\n",
    "                                     X,\n",
    "                                     layer_idx,\n",
    "                                     appendix):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        shape = X.get_shape().as_list()\n",
    "        old_emb_idx = tf.reshape(self.emb_idx, [1], name=\"old_emb_idx\"+appendix)\n",
    "        with tf.control_dependencies([tf.assign(self.emb_idx,\n",
    "                                                tf.mod(self.emb_idx + 1,\n",
    "                                                       30,\n",
    "                                                       name=\"increment_emb_idx\"+appendix),\n",
    "                                                name=\"new_emb_idx\"+appendix)]):\n",
    "            slice_start = tf.concat([old_emb_idx, tf.constant([layer_idx])],\n",
    "                                    0,\n",
    "                                    name=\"slice_start\"+appendix)\n",
    "            reshaped_slice = tf.reshape(tf.slice(self.debug_boundaries,\n",
    "                                                 slice_start,\n",
    "                                                 [1, 1],\n",
    "                                                 name=\"slice_from_debug_boundaries\"+appendix),\n",
    "                                        [1, 1],\n",
    "                                        name=\"reshaped_slice\"+appendix)\n",
    "            return_value = tf.tile(reshaped_slice,\n",
    "                                   shape,\n",
    "                                   name=\"fixed_boundaries\"+appendix)\n",
    "        return return_value\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx, appendix):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\"+appendix)\n",
    "\n",
    "            new_appendix_templ = appendix + \"_layernum\"\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "            hidden, memory, boundary, helper = self.not_last_layer(0,\n",
    "                                                                   iter_idx,\n",
    "                                                                   state[0],\n",
    "                                                                   inp,\n",
    "                                                                   state[1][0],\n",
    "                                                                   activated_boundary_states,\n",
    "                                                                   new_appendix_templ+str(0))\n",
    "\n",
    "            not_last_layer_helpers = list()\n",
    "            not_last_layer_helpers.append(helper)\n",
    "            new_state.append((hidden, memory, boundary))\n",
    "            boundaries.append(boundary)\n",
    "            # All layers except for the first and the last ones\n",
    "            if num_layers > 2:\n",
    "                for idx in range(num_layers-2):\n",
    "                    hidden, memory, boundary, helper = self.not_last_layer(idx+1,\n",
    "                                                                           iter_idx,\n",
    "                                                                          state[idx+1],\n",
    "                                                                          hidden,\n",
    "                                                                          state[idx+2][0],\n",
    "                                                                          boundary,\n",
    "                                                                          new_appendix_templ+str(idx+1))\n",
    "                    not_last_layer_helpers.append(helper)\n",
    "                    new_state.append((hidden, memory, boundary))\n",
    "                    boundaries.append(boundary)\n",
    "            hidden, memory = self.last_layer(state[-1],\n",
    "                                             hidden,\n",
    "                                             boundary,\n",
    "                                             new_appendix_templ+str(self._num_layers-1))\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"+appendix)}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"+appendix), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs,\n",
    "                         appendix):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\"+appendix)\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\"+appendix)\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\"+appendix)\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state,\n",
    "                   appendix):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            new_appendix_templ = appendix + '_unrolling'\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx, new_appendix_templ+str(emb_idx))\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx+appendix)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx+appendix),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx+appendix))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\"+appendix)\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=(\"hidden_concat_in_RNN_module_on_layer%s\"%idx)+appendix)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"+appendix),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"+appendix),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states,\n",
    "                      appendix):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\"+appendix)\n",
    "            output_module_gates = tf.transpose(self.step_function(tf.matmul(concat,\n",
    "                                                                            self.output_module_gates_weights,\n",
    "                                                                            name=\"matmul_in_output_module_gates\"+appendix),\n",
    "                                                                  \"_sigmoid_in_output_module_gates\"+appendix),\n",
    "                                               name=\"output_module_gates\"+appendix)\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\"+appendix)\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=(\"tr_hidden_state_total_%s\"%idx)+appendix)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=(\"tr_gated_hidden_states_%s\"%idx)+appendix))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"+appendix),\n",
    "                                               name=\"gated_hidden_states\"+appendix)\n",
    "            return tf.add(tf.matmul(gated_hidden_states,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits_output\"+appendix),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\"+appendix)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def construct_boundary_test_line(self):\n",
    "        test_line = list()\n",
    "        samples = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "        def add1(index):\n",
    "            test_line.append(list(samples[index]))\n",
    "        def add2(index):\n",
    "            test_line.append(list(samples[index]))\n",
    "            test_line.append(list(samples[index]))\n",
    "        add2(0)\n",
    "        add1(2)\n",
    "        add2(0)\n",
    "        add1(1)\n",
    "        add2(0)\n",
    "        add1(3)\n",
    "        add2(0)\n",
    "        add2(2)\n",
    "        add2(0)\n",
    "        add2(1)\n",
    "        add2(0)\n",
    "        add2(3)\n",
    "        add2(0)\n",
    "        add2(3)\n",
    "        add1(3)\n",
    "        add2(0)\n",
    "        add2(0)\n",
    "        return test_line\n",
    "    \n",
    "    def make_negative(self,\n",
    "                      layer_idx,\n",
    "                      chunk_idx):\n",
    "        positive_coeffs = tf.constant(1., shape=[self._num_nodes[layer_idx]], name=\"positive_coeffs\")\n",
    "        negative_coeffs = tf.constant(-1., shape=[self._num_nodes[layer_idx]], name=\"negative_coeffs\")\n",
    "        list_for_coeffs = list()\n",
    "        for _ in range(4):\n",
    "            list_for_coeffs.append(positive_coeffs)\n",
    "        list_for_coeffs[chunk_idx] = negative_coeffs\n",
    "        coeffs = tf.concat(list_for_coeffs, 0, name=\"coeffs_for_layer%s\"%layer_idx)\n",
    "        modification_operation = tf.assign(self.Matrices[layer_idx],\n",
    "                                           tf.multiply(self.Matrices[layer_idx],\n",
    "                                                       coeffs,\n",
    "                                                       name=\"multiply_in_modification_operation_for_layer%s\"%layer_idx),\n",
    "                                           name=\"modification_operation_for_layer%s\"%layer_idx)\n",
    "        return modification_operation\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 negative_chunk_indices=None,\n",
    "                 embedding_size=128):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"type\": 12}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, name=\"global_step\")\n",
    "                self.emb_idx = tf.Variable([0], name=\"emb_idx\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                debug_boundaries = self.construct_boundary_test_line()\n",
    "                self.debug_boundaries = tf.constant(debug_boundaries, name=\"debug_boundaries\")\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.ones([self._vocabulary_size, self._embedding_size],\n",
    "                                                             name=\"embeddings_matrix_initialize\"),\n",
    "                                                     trainable=True,\n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                init_bias_name = \"HM_LSTM_bias_%s_initializer\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\"\n",
    "                \n",
    "                self.Matrices.append(tf.Variable(tf.ones([self._embedding_size + self._num_nodes[0] + self._num_nodes[1],\n",
    "                                                          4 * self._num_nodes[0] + 1],\n",
    "                                                         name=init_matr_name%0),\n",
    "                                                 trainable=False,\n",
    "                                                 name=matr_name%0))\n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[0] + 1],\n",
    "                                                        name=init_bias_name%0),\n",
    "                                               trainable=False,\n",
    "                                               name=bias_name%0))\n",
    "                if self._num_layers > 2:\n",
    "                    for i in range(self._num_layers - 2):\n",
    "                        self.Matrices.append(tf.Variable(tf.ones([self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2],\n",
    "                                                                  4 * self._num_nodes[i+1] + 1],\n",
    "                                                                 name=init_matr_name%(i+1)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=matr_name%(i+1)))\n",
    "                        self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[i+1] + 1],\n",
    "                                                                name=init_bias_name%(i+1)),\n",
    "                                                       trainable=False,\n",
    "                                                       name=bias_name%(i+1)))\n",
    "                self.Matrices.append(tf.Variable(tf.ones([self._num_nodes[-1] + self._num_nodes[-2],\n",
    "                                                          4 * self._num_nodes[-1]],\n",
    "                                                         name=init_matr_name%(self._num_layers-1)),\n",
    "                                                 trainable=False,\n",
    "                                                 name=matr_name%(self._num_layers-1)))     \n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[-1]],\n",
    "                                                        name=init_bias_name%(self._num_layers-1)),\n",
    "                                               trainable=False,\n",
    "                                               name=bias_name%(self._num_layers-1)))\n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.ones([dim_classifier_input, self._num_layers],\n",
    "                                                                       name=\"output_gates_weights_initializer\"),\n",
    "                                                               trainable=False,\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_weights = tf.Variable(tf.ones([dim_classifier_input, self._vocabulary_size],\n",
    "                                                          name=\"output_weights_initializer\"),\n",
    "                                                  trainable=False,\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               trainable=False,\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "\n",
    "\n",
    "                    self.saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 0)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 0)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 1)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 1)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 2)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 2))))\n",
    "                    self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                        tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "\n",
    "                    @tf.RegisterGradient(\"HardSigmoid\")\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "\n",
    "                    # appendix is used for constructing of tensor name. It is appended to tensor name\n",
    "                    # to indicate to which part of graph operation belongs\n",
    "                    appendix = \"_train\"\n",
    "                    state = self.saved_state\n",
    "                    if negative_chunk_indices is not None:\n",
    "                        mod_op = [self.make_negative(*negative_chunk_indices)]\n",
    "                    else:\n",
    "                        mod_op = []\n",
    "                    with tf.control_dependencies(mod_op):\n",
    "                        embedded_inputs = self.embedding_module(train_inputs, appendix)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, self.saved_state, appendix)\n",
    "                    logits = self.output_module(hidden_states, appendix)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"+appendix),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\"+appendix)\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(self.saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    self._learning_rate = tf.train.exponential_decay(10.0,\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    " \n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "                    appendix = \"_validation\"\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input], appendix)\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state,\n",
    "                                                                                            appendix)\n",
    "                    sample_logits = self.output_module(sample_hidden_states, appendix) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"L2_norm_of_hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = HM_LSTM(1,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 1,\n",
    "                 3,\n",
    "                 [3, 4, 5],\n",
    "                 .001,               # init_slope\n",
    "                 0.001,                  # slope_growth\n",
    "                 100,\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step: 0\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 0.  0.  0.]]\n",
      "     [0][1]: [[ 0.  0.  0.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 0.  0.  0.  0.]]\n",
      "     [1][1]: [[ 0.  0.  0.  0.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 0.  0.  0.  0.  0.]]\n",
      "     [2][1]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 1\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 30.  30.  30.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 0.  0.  0.  0.]]\n",
      "     [1][1]: [[ 0.  0.  0.  0.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 0.  0.  0.  0.  0.]]\n",
      "     [2][1]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 2\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 30.  30.  30.  30.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 0.  0.  0.  0.  0.]]\n",
      "     [2][1]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 3\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 30.  30.  30.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 30.  30.  30.  30.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 0.  0.  0.  0.  0.]]\n",
      "     [2][1]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 4\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 60.  60.  60.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 30.  30.  30.  30.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 0.  0.  0.  0.  0.]]\n",
      "     [2][1]: [[ 0.  0.  0.  0.  0.]]\n",
      "step: 5\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 90.  90.  90.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 30.  30.  30.  30.  30.]]\n",
      "step: 6\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 120.  120.  120.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 30.  30.  30.  30.  30.]]\n",
      "step: 7\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 150.  150.  150.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 30.  30.  30.  30.  30.]]\n",
      "step: 8\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 60.  60.  60.  60.  60.]]\n",
      "step: 9\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 30.  30.  30.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 60.  60.  60.  60.  60.]]\n",
      "step: 10\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 60.  60.  60.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 60.  60.  60.  60.  60.]]\n",
      "step: 11\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 31.  31.  31.  31.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 60.  60.  60.  60.  60.]]\n",
      "step: 12\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 61.  61.  61.  61.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 60.  60.  60.  60.  60.]]\n",
      "step: 13\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 30.  30.  30.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 61.  61.  61.  61.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 60.  60.  60.  60.  60.]]\n",
      "step: 14\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 60.  60.  60.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 61.  61.  61.  61.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 60.  60.  60.  60.  60.]]\n",
      "step: 15\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 90.  90.  90.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 90.  90.  90.  90.  90.]]\n",
      "step: 16\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 120.  120.  120.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 120.  120.  120.  120.  120.]]\n",
      "step: 17\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 150.  150.  150.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 120.  120.  120.  120.  120.]]\n",
      "step: 18\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 180.  180.  180.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 120.  120.  120.  120.  120.]]\n",
      "step: 19\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 150.  150.  150.  150.  150.]]\n",
      "step: 20\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 180.  180.  180.  180.  180.]]\n",
      "step: 21\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 30.  30.  30.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 180.  180.  180.  180.  180.]]\n",
      "step: 22\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 60.  60.  60.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 180.  180.  180.  180.  180.]]\n",
      "step: 23\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 210.  210.  210.  210.  210.]]\n",
      "step: 24\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 240.  240.  240.  240.  240.]]\n",
      "step: 25\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 1.  1.  1.]]\n",
      "     [0][2]: [[ 1.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 1.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 270.  270.  270.  270.  270.]]\n",
      "step: 26\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 30.  30.  30.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 270.  270.  270.  270.  270.]]\n",
      "step: 27\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 60.  60.  60.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 270.  270.  270.  270.  270.]]\n",
      "step: 28\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 90.  90.  90.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 270.  270.  270.  270.  270.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 29\n",
      "self.saved_state: \n",
      "   [0]:\n",
      "     [0][0]: [[ 1.  1.  1.]]\n",
      "     [0][1]: [[ 120.  120.  120.]]\n",
      "     [0][2]: [[ 0.]]\n",
      "   [1]:\n",
      "     [1][0]: [[ 1.  1.  1.  1.]]\n",
      "     [1][1]: [[ 1.  1.  1.  1.]]\n",
      "     [1][2]: [[ 0.]]\n",
      "   [2]:\n",
      "     [2][0]: [[ 1.  1.  1.  1.  1.]]\n",
      "     [2][1]: [[ 270.  270.  270.  270.  270.]]\n",
      "Number of steps = 35     Percentage = 0.00%     Time = 3s     Learning rate = 10.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            50,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            20,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=35,\n",
    "            print_intermediate_results=True,\n",
    "            add_operations = ['self.saved_state'],\n",
    "          print_steps=[i for i in range(30)],\n",
    "          block_validation=True)\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if True and False:\n",
    "    print(1)\n",
    "else:\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
