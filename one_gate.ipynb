{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not one byte characters:  0\n",
      "min order index:  9\n",
      "max order index:  255\n",
      "total number of characters:  196\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99285000 in, some believe that it is only a matter of when an environment\n",
      "75000 ture in Mutual Aid: A Factor of Evolution (1897). Subsequent ana\n"
     ]
    }
   ],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 75000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ\n",
      "char2id(u'a') = 67,  char2id(u'z') = 92,  char2id(u' ') = 2\n",
      "id2char(78) = l,  id2char(156) = Ø,  id2char(140) = È\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'in, some be', u'd, after su', u'uchan also ', u'bankruptcy ', u'\\n[[pl:Wglow', u' CQ 42, (19', u'[Sahara]] i', u'licensed to', u'actor\\n*[[19', u'  <page>\\n  ', u' the differ', u'nt, althoug', u'ision>\\n  </', u\".\\n*'''Write\", u'itative sou', u'uth was one', u' verdict wa', u'called &quo', u'(Kilmer rep', u'y the time ', u' book\\n | au', u' [[Rio Grou', u'=Some famou', u\"ics' was co\", u'==Recurring', u'olor blue o', u']], the Cha', u'{{main|Brit', u'tem and its', u' can be gen', u's and respo', u\"ula's locat\", u' to each ot', u'cremental Z', u'in an XML-e', u'\\n        <i', u'on]] ==\\n*[[', u'Vernon also', u'ed by all t', u'ly isolated', u\"''\\n\\n[[Perfo\", u'o compute x', u'eneral conc', u'on in Brita', u'nd commenta', u'ven though ', u'onatas]]\\n[[', u' publisher ', u']}}. {{IPA|', u'[Nazi]] occ', u's particles', u'datum, the ', u' and decide', u'kage is sup', u'and other f', u'th I of Eng', u't economic ', u'Minister. T', u'ns. Ironica', u'Automated c', u've mild win', u'ties within', u';&lt;FONT S', u'nic, no two']\n",
      "[u'elieve that', u'urpassing t', u' apply here', u' trustee]] ', u'wod\\xf3r aroma', u'992) 347-35', u'is the [[da', u'o the city.', u'978]] - [[G', u'   <title>A', u'rence is th', u'gh often ov', u'/page>\\n  <p', u\"ers''', '''\", u'urce of God', u'e of the fi', u'as passed d', u'ot;Boadicea', u'portedly wa', u' the inquir', u'uthor = [[W', u'up]] in [[2', u'us examples', u'oined by [[', u'g subject m', u'on the phys', u'aldeans dis', u't milah}}\\n\\n', u's input.  O', u'nerated by ', u'onses by cu', u'tion. Mina ', u'ther. This ', u'Zone Transf', u'enabled [[w', u'id>423367</', u'[Woden|W\\xf3de', u'o stated th', u'these new i', u'd.\\n\\n(This p', u'orming arts', u'x = one mor', u'census of s', u'ain hit one', u'ator\\n*[[196', u' it incorre', u'[nl:Wereldk', u' =\\n | id = ', u'|[n]}} and ', u'cupation, h', u's will imme', u' Trieste da', u'ed to appro', u'pposed to b', u'funk styles', u'gland|Eliza', u' advantages', u'The Prime M', u'ally the an', u'conversion<', u'nters and e', u'n this area', u'SIZE=&quot;', u'o squares a']\n",
      "[u'tu']\n",
      "[u'ur']\n"
     ]
    }
   ],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class one_gate(MODEL):\n",
    "    def layer(self, inp, state):\n",
    "        X = tf.concat(1, [inp,\n",
    "                            state[0],\n",
    "                            state[1]])\n",
    "        RES = tf.matmul(X, self.Matrix) + self.Bias\n",
    "        state = tf.tanh(RES)\n",
    "        return state\n",
    "\n",
    "    \n",
    "    def iteration(self, inp, state):\n",
    "        output = self.layer(inp, state)\n",
    "        gate = tf.sigmoid(tf.matmul(tf.concat(1, [inp, output, state[1]]), self.gate_matrix) + self.gate_bias)\n",
    "        gate = tf.reshape(gate, [-1])\n",
    "        batch_size = gate.get_shape().as_list()[0]\n",
    "        forget = tf.constant(1., shape=[batch_size]) - gate\n",
    "        memory = tf.transpose(tf.transpose(output) * gate) + tf.transpose(tf.transpose(state[1]) * forget)\n",
    "        return output, [output, memory], gate\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_bias,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 seed=None,\n",
    "                 mean=0.,\n",
    "                 stddev='default',\n",
    "                 shift=0.,\n",
    "                 init_learning_rate=1.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_bias = init_bias\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        \n",
    "        self._mean = mean\n",
    "        \n",
    "        self._stddev = list()\n",
    "        if stddev == 'default':\n",
    "            self._stddev = 1.0 * np.sqrt(1./(num_nodes[0] + vocabulary_size))\n",
    "        else:\n",
    "            self._stddev = stddev \n",
    "        self._shift = shift\n",
    "        self._init_learning_rate = init_learning_rate\n",
    "        \n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_bias\": 8,\n",
    "                         \"init_mean\": 9,\n",
    "                         \"init_stddev\": 10,\n",
    "                         \"init_shift\": 11,\n",
    "                         \"init_learning_rate\": 12,\n",
    "                         \"type\": 13}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with self._graph.device('/gpu:0'): \n",
    "                if seed is not None:\n",
    "                    tf.set_random_seed(random.randint(-2*10**9, 2*10**9))\n",
    "                self.Matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + 2*self._num_nodes[0],\n",
    "                                                               self._num_nodes[0]],\n",
    "                                                              mean=self._mean,\n",
    "                                                              stddev=self._stddev))\n",
    "                self.Bias = tf.Variable([self._shift for _ in range(self._num_nodes[0])])\n",
    "                \n",
    "                self.gate_matrix = tf.Variable(tf.truncated_normal([self._vocabulary_size + 2 * self._num_nodes[0], 1], stddev = 0.1))\n",
    "                self.gate_bias = tf.Variable([self._init_bias])\n",
    "                # classifier \n",
    "                weights = tf.Variable(tf.truncated_normal([self._num_nodes[-1], self._vocabulary_size], stddev = 0.1))\n",
    "                bias = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                self._train_data = list()\n",
    "                for i in range(self._num_unrollings + 1):\n",
    "                    self._train_data.append(\n",
    "                        tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size], name=('inp_%s'%i)))\n",
    "                train_inputs = self._train_data[: self._num_unrollings]\n",
    "                train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                # Unrolled LSTM loop.\n",
    "\n",
    "                saved_state = [tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False),\n",
    "                               tf.Variable(tf.zeros([self._batch_size, self._num_nodes[0]]), trainable=False)]\n",
    "                \n",
    "                \"\"\"global step\"\"\"\n",
    "                self._global_step = tf.Variable(0)\n",
    "  \n",
    "\n",
    "                outputs = list()\n",
    "                state = saved_state\n",
    "                for inp in train_inputs:\n",
    "                    output, state, _ = self.iteration(inp, state)\n",
    "                    outputs.append(output)\n",
    "\n",
    "                save_list = list()\n",
    "                save_list.append(saved_state[0].assign(state[0]))\n",
    "                save_list.append(saved_state[1].assign(state[1]))\n",
    "                \n",
    "                \"\"\"skip operation\"\"\"\n",
    "                self._skip_operation = tf.group(*save_list)\n",
    "\n",
    "                with tf.control_dependencies(save_list):\n",
    "                        # Classifier.\n",
    "                    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), weights, bias)\n",
    "                    \"\"\"loss\"\"\"\n",
    "                    self._loss = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                            logits, tf.concat(0, train_labels)))\n",
    "                # Optimizer.\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                self._half_life = tf.placeholder(tf.int32, name='half_life')\n",
    "                self._decay = tf.placeholder(tf.float32, name='decay')\n",
    "                \"\"\"learning rate\"\"\"\n",
    "                self._learning_rate = tf.train.exponential_decay(self._init_learning_rate,\n",
    "                                                                 self._global_step,\n",
    "                                                                 self._half_life,\n",
    "                                                                 self._decay,\n",
    "                                                                 staircase=True)\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                \"\"\"optimizer\"\"\"\n",
    "                self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                \"\"\"train prediction\"\"\"\n",
    "                self._train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                saved_sample_state = list()\n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                saved_sample_state.append(tf.Variable(tf.zeros([1, self._num_nodes[0]]), trainable=False)) \n",
    "                \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size], name='sample_input')\n",
    "                \n",
    "                reset_list = list()\n",
    "                reset_list.append(saved_sample_state[0].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "                reset_list.append(saved_sample_state[1].assign(tf.zeros([1, self._num_nodes[0]])))\n",
    "\n",
    "                \"\"\"reset sample state\"\"\"\n",
    "                self._reset_sample_state = tf.group(*reset_list)\n",
    "\n",
    "                \n",
    "                sample_output, sample_state, self.gate = self.iteration(self._sample_input, saved_sample_state)\n",
    "\n",
    "\n",
    "                sample_save_list = list()\n",
    "                sample_save_list.append(saved_sample_state[0].assign(sample_state[0]))\n",
    "                sample_save_list.append(saved_sample_state[1].assign(sample_state[1]))\n",
    "\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    \"\"\"sample prediction\"\"\"\n",
    "                    self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, weights, bias)) \n",
    "                \n",
    "                \n",
    "                \"\"\"saver\"\"\"\n",
    "                self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                            \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append(self._mean)\n",
    "        metadata.append(self._stddev)\n",
    "        metadata.append(self._shift)\n",
    "        metadata.append(self._init_learning_rate)\n",
    "        metadata.append('one_gate')\n",
    "        return metadata\n",
    "    \n",
    "        \n",
    "    def get_gates(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        gate_list = list()\n",
    "        collect_gates = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_gates: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    g_list = list()\n",
    "                    collect_gates = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                g_list.append(self.gate.eval({self._sample_input: b[0]})[0])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_gates = False\n",
    "                    gate_list.append(g_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return (text_list, gate_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = one_gate(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 1,\n",
    "                 [128],\n",
    "                 0.,\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.281330 learning rate: 1.000000\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "ØïOí0:M#û¥z/ºí)Ú_ùhÆNi®¶^T¼ìã}|pÊ»Ó3Ðõ°g$ÚeÆºÏ »ÛÈ;:6Ú.=Ãö/äØ5á¯õwª°ù7ã3q9eo{\n",
      "ñ+ÛíÎøÖOc²e¤ý`Á¿¤yÀIu­e¢Î×yÑ0+%ëðH¤á¤ï½¢¡Øÿ¨\"@@&æ\t\n",
      "G¯>ÖÛàª[Z6d7píÙó÷@JbßZÄ\n",
      "HÐôØ\n",
      "ÐÃDñhGÈ*ÒÞ×Ú®=¡à?®w&àÜa\n",
      "Ús60å{BâXÔ&Wa*hLÌß,¬½¹êÉgq«\n",
      "ðÖ\n",
      "0&²¯5xÊòë¼1H¿!';.NV®=îõf7\n",
      "U¡`1áW¾fID¢V¤h®(Ä1Ø»ø¨~Ë×T°:ä¡SÁuòHÚÃâ\"ñÝ¿Vh¹±båYìÛ<\\}óÍòìhc°r`\"@ÓlÉ!Ä²á~ðßwUT\n",
      "he-UÑ´W¢º~I ¬~Î%Ðt å|U,wJx%ûöb8|s`Ð®N¼Ô8KÜ bÉpºê).)À xâuaÇQÛâù!JQUÆ4¥|AH/uñÜwà+:\n",
      "================================================================================\n",
      "\n",
      "from fuse:\n",
      "================================================================================\n",
      "0. fuse: my name is\n",
      "my name is\n",
      "/>^{iÍ |hKg§ªGCÃÒÇ)WÑyèPßPx%¥½×:]âÆË\n",
      "¯%¾D7qß§²ïÜEÅãözS\tñ5OH!$@ÜÒ¢r`é0ð2ý>a¢ür\n",
      "1. fuse: december elegy\n",
      "december elegy¿ûMÎºËb;\\ 1ÔÝ2^XAýëxµý³ÒÚûmu+Yíj\n",
      "Ý-?Y>ïé13uüx²VhxF¨zã²qûËù-qSÝNj[S?r+û)·7\\eµ)i¿\n",
      "2. fuse: they have done\n",
      "they have doneD»¶Õª×ãäÍêR4& o÷Q^os½§$»Md·>î¹§ÛO¬UOô\"S×Úç4KS/µôfú°ñ5-ÐR.XrWÃó^©tMòø2-°>Á ÿo(f\n",
      "================================================================================\n",
      "Validation percentage of correct: 9.40%\n",
      "\n",
      "Average loss at step 200: 3.481241 learning rate: 1.000000\n",
      "Percentage_of correct: 17.73%\n",
      "Validation percentage of correct: 22.43%\n",
      "\n",
      "Average loss at step 400: 2.793920 learning rate: 0.900000\n",
      "Percentage_of correct: 26.98%\n",
      "Validation percentage of correct: 30.38%\n",
      "\n",
      "Average loss at step 600: 2.604511 learning rate: 0.900000\n",
      "Percentage_of correct: 30.36%\n",
      "Validation percentage of correct: 32.76%\n",
      "\n",
      "Average loss at step 800: 2.477974 learning rate: 0.810000\n",
      "Percentage_of correct: 33.81%\n",
      "Validation percentage of correct: 33.29%\n",
      "\n",
      "Average loss at step 1000: 2.388263 learning rate: 0.729000\n",
      "Percentage_of correct: 35.30%\n",
      "Validation percentage of correct: 35.75%\n",
      "\n",
      "Average loss at step 1200: 2.323501 learning rate: 0.729000\n",
      "Percentage_of correct: 36.52%\n",
      "Validation percentage of correct: 34.98%\n",
      "\n",
      "Average loss at step 1400: 2.272763 learning rate: 0.656100\n",
      "Percentage_of correct: 37.52%\n",
      "Validation percentage of correct: 36.99%\n",
      "\n",
      "Average loss at step 1600: 2.240748 learning rate: 0.656100\n",
      "Percentage_of correct: 37.95%\n",
      "Validation percentage of correct: 37.00%\n",
      "\n",
      "Average loss at step 1800: 2.208010 learning rate: 0.590490\n",
      "Percentage_of correct: 39.10%\n",
      "Validation percentage of correct: 36.89%\n",
      "\n",
      "Average loss at step 2000: 2.183488 learning rate: 0.531441\n",
      "Percentage_of correct: 40.24%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "ribhes of [[Jar-liomigs]] [[Juchalrm]]}*[[Nhy Hearn]] is lekextarndctstoby. The \n",
      "·anadeviz]]), seeres avertstoceaplacl Mqueeted fettriclan=\n",
      "\n",
      " [[R954- Cad and Ric\n",
      ".\n",
      "* [[Ath hadas owirn]] '\n",
      "\n",
      "[[fiemulevoly Rtum]]\n",
      "\n",
      "[[ptt iadoo]]\n",
      "[[ny:ò]]\n",
      "[[do:TAp\n",
      "¨ishan ChascomendDaverinle}bationtun-|Apppslevil]]&qurt;&atrich&quot;&lt;/re]&gt\n",
      "port-2didacung#.\n",
      "KAmpiry chuplatiched cers (ulbyer neallying thounep`uarden, thi\n",
      "================================================================================\n",
      "\n",
      "from fuse:\n",
      "================================================================================\n",
      "0. fuse: my name is\n",
      "my name is in [[dring]]- marrhe to ofiel erowires as mere fors whtrun ied. Catas fineralt\n",
      "1. fuse: december elegy\n",
      "december elegy and utlayed if this apared conthiced phicesiad artice erins is aseach thes car\n",
      "2. fuse: they have done\n",
      "they have dones. Scereczarsy with thal wusdith the bubly of the provurist, tusde it ard muts \n",
      "================================================================================\n",
      "Validation percentage of correct: 38.46%\n",
      "\n",
      "Average loss at step 2200: 2.134438 learning rate: 0.531441\n",
      "Percentage_of correct: 40.78%\n",
      "Validation percentage of correct: 39.24%\n",
      "\n",
      "Average loss at step 2400: 2.120836 learning rate: 0.478297\n",
      "Percentage_of correct: 41.72%\n",
      "Validation percentage of correct: 39.27%\n",
      "\n",
      "Average loss at step 2600: 2.087077 learning rate: 0.478297\n",
      "Percentage_of correct: 42.19%\n",
      "Validation percentage of correct: 39.14%\n",
      "\n",
      "Average loss at step 2800: 2.063409 learning rate: 0.430467\n",
      "Percentage_of correct: 42.68%\n",
      "Validation percentage of correct: 40.34%\n",
      "\n",
      "Average loss at step 3000: 2.052107 learning rate: 0.387420\n",
      "Percentage_of correct: 43.15%\n",
      "Validation percentage of correct: 40.14%\n",
      "\n",
      "Average loss at step 3200: 2.048898 learning rate: 0.387420\n",
      "Percentage_of correct: 42.99%\n",
      "Validation percentage of correct: 40.58%\n",
      "\n",
      "Average loss at step 3400: 2.045543 learning rate: 0.348678\n",
      "Percentage_of correct: 43.34%\n",
      "Validation percentage of correct: 40.50%\n",
      "\n",
      "Average loss at step 3600: 2.031501 learning rate: 0.348678\n",
      "Percentage_of correct: 43.86%\n",
      "Validation percentage of correct: 41.15%\n",
      "\n",
      "Average loss at step 3800: 2.016694 learning rate: 0.313810\n",
      "Percentage_of correct: 43.81%\n",
      "Validation percentage of correct: 41.18%\n",
      "\n",
      "Average loss at step 4000: 2.014712 learning rate: 0.282429\n",
      "Percentage_of correct: 44.03%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "`endeal fimst T*ingarm|'s.a nock]. Thins almand soonal is with usee foctran than\n",
      "½rais>\n",
      "      <ne>117556</id>\n",
      "      <text xplithiving amd tive rin con urding fam\n",
      "teents beto -'' andfited in [Bitp:/own.s.p.\n",
      "\n",
      " [http://E-orkgopationaterrems/thti\n",
      "Ne feptst con a socevennipestallermalor.]\n",
      ", 4IP O'  Wide wa ldepeetic of fely al\n",
      "ºpro15\n",
      "|[{tupppppor. Thilisorarnkingramigan|C78ß08imbucouspadial|Remmin]], [[Eis\n",
      "================================================================================\n",
      "\n",
      "from fuse:\n",
      "================================================================================\n",
      "0. fuse: my name is\n",
      "my name is [[Jevic canom]]]], ald can bat is explide and list=\n",
      "* [http://.urssumblis.uega\n",
      "1. fuse: december elegy\n",
      "december elegy/cizemantiocs, thic Sortednice in Fanamay louet, in hablenal the also dasert (s\n",
      "2. fuse: they have done\n",
      "they have doneed commenic0]\n",
      "* [[Cetsoothis]](-Aly, bits and-Wast-cameriglechisul/grephing Sea\n",
      "================================================================================\n",
      "Validation percentage of correct: 41.71%\n",
      "\n",
      "Average loss at step 4200: 2.008967 learning rate: 0.282429\n",
      "Percentage_of correct: 44.67%\n",
      "Validation percentage of correct: 41.07%\n",
      "\n",
      "Average loss at step 4400: 1.938563 learning rate: 0.254186\n",
      "Percentage_of correct: 45.57%\n",
      "Validation percentage of correct: 42.45%\n",
      "\n",
      "Average loss at step 4600: 1.987769 learning rate: 0.254186\n",
      "Percentage_of correct: 44.13%\n",
      "Validation percentage of correct: 40.99%\n",
      "\n",
      "Average loss at step 4800: 2.003685 learning rate: 0.228768\n",
      "Percentage_of correct: 44.48%\n",
      "Validation percentage of correct: 42.57%\n",
      "\n",
      "Average loss at step 5000: 1.995304 learning rate: 0.205891\n",
      "Percentage_of correct: 44.22%\n",
      "Validation percentage of correct: 42.01%\n",
      "\n",
      "Average loss at step 5200: 1.989088 learning rate: 0.205891\n",
      "Percentage_of correct: 44.54%\n",
      "Validation percentage of correct: 41.64%\n",
      "\n",
      "Average loss at step 5400: 1.959781 learning rate: 0.185302\n",
      "Percentage_of correct: 45.26%\n",
      "Validation percentage of correct: 42.31%\n",
      "\n",
      "Average loss at step 5600: 1.977459 learning rate: 0.185302\n",
      "Percentage_of correct: 44.79%\n",
      "Validation percentage of correct: 42.53%\n",
      "\n",
      "Average loss at step 5800: 1.954774 learning rate: 0.166772\n",
      "Percentage_of correct: 45.47%\n",
      "Validation percentage of correct: 42.84%\n",
      "\n",
      "Average loss at step 6000: 1.952219 learning rate: 0.150095\n",
      "Percentage_of correct: 45.34%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "¡ú Tiptor =cIncessober, Chice of Frintima|lofatital preveddeada in 4P. The fisst\n",
      "Qs and hosechominale. Usthen time tyly auputomeber is as asticlled &quot;norby&q\n",
      "spanthitation to a cloves a promican erancyly be ore pelstates, '&gt;tent- by an\n",
      "Z8ecestowned F.RN: Cuttrow Proficolu-|188ö.'' himp [[fres]] agreakous fich of th\n",
      "Òcreotain (thern of; NpDich he langer 4wien his ous requalicenismer fimmores alv\n",
      "================================================================================\n",
      "\n",
      "from fuse:\n",
      "================================================================================\n",
      "0. fuse: my name is\n",
      "my name is bowlegens and cameved cogroust; conners giyst, improverts gesed twe k-w.\n",
      " Sema\n",
      "1. fuse: december elegy\n",
      "december elegy-sreyils of Marofieen t. Amearn that bitallers =\n",
      "\n",
      "An Prodelts on loadrg enversp\n",
      "2. fuse: they have done\n",
      "they have doned. This gronhed beth-spies the cablentart, bates Sarmen thar. MScibes, and [[pa\n",
      "================================================================================\n",
      "Validation percentage of correct: 43.57%\n",
      "\n",
      "Average loss at step 6200: 1.963216 learning rate: 0.150095\n",
      "Percentage_of correct: 45.68%\n",
      "Validation percentage of correct: 43.70%\n",
      "\n",
      "Average loss at step 6400: 1.964163 learning rate: 0.135085\n",
      "Percentage_of correct: 45.44%\n",
      "Validation percentage of correct: 43.80%\n",
      "\n",
      "Average loss at step 6600: 1.933177 learning rate: 0.135085\n",
      "Percentage_of correct: 46.33%\n",
      "Validation percentage of correct: 43.74%\n",
      "\n",
      "Average loss at step 6800: 1.932312 learning rate: 0.121577\n",
      "Percentage_of correct: 46.13%\n",
      "Validation percentage of correct: 44.35%\n",
      "\n",
      "Average loss at step 7000: 1.936882 learning rate: 0.109419\n",
      "Percentage_of correct: 45.90%\n",
      "Validation percentage of correct: 44.06%\n",
      "\n",
      "Average loss at step 7200: 1.956019 learning rate: 0.109419\n",
      "Percentage_of correct: 45.09%\n",
      "Validation percentage of correct: 44.11%\n",
      "\n",
      "Average loss at step 7400: 1.921503 learning rate: 0.098477\n",
      "Percentage_of correct: 46.74%\n",
      "Validation percentage of correct: 44.38%\n",
      "\n",
      "Average loss at step 7600: 1.930719 learning rate: 0.098477\n",
      "Percentage_of correct: 46.94%\n",
      "Validation percentage of correct: 43.61%\n",
      "\n",
      "Average loss at step 7800: 1.965929 learning rate: 0.088629\n",
      "Percentage_of correct: 46.17%\n",
      "Validation percentage of correct: 43.70%\n",
      "\n",
      "Average loss at step 8000: 1.948147 learning rate: 0.079766\n",
      "Percentage_of correct: 45.87%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "W-alEgetian spurchorication but hor is a lotmenty nime.\n",
      "* cinlus aluatiativilati\n",
      "Griarish cenguting to the corigut earduct and gon-mase theorstrack decond theors\n",
      "3orgh: Brangory ''Brochianes, Etjom cin Jy-2 Aper-al Saplust of Btine the ewupes\n",
      "´ed.    (, 5</titactinutorperitient_ussalimalaunº commory farm (M8P02594{.  teen\n",
      "Ðis: Is a seworucile efr's [[ Crose tranclanies]]\n",
      "*[[N.SLawest.\n",
      "\n",
      "| Than (sire mu\n",
      "================================================================================\n",
      "\n",
      "from fuse:\n",
      "================================================================================\n",
      "0. fuse: my name is\n",
      "my name iser, whingen is a camaling this as a benkapls sytily becomend foy-cas weol prant\n",
      "1. fuse: december elegy\n",
      "december elegy]\n",
      "'''oul Dino of [[Gendia]] uffrrespunisita] ) 39% cextre.\n",
      "\n",
      "Geppelesare], why m\n",
      "2. fuse: they have done\n",
      "they have donergen cimestrial the [[Rokevicip 2¤.use]]\n",
      "\n",
      "==Wary: [[Limare]]</texphistting/hamp\n",
      "================================================================================\n",
      "Validation percentage of correct: 43.88%\n",
      "\n",
      "Average loss at step 8200: 1.968969 learning rate: 0.079766\n",
      "Percentage_of correct: 45.60%\n",
      "Validation percentage of correct: 43.83%\n",
      "\n",
      "Average loss at step 8400: 1.935657 learning rate: 0.071790\n",
      "Percentage_of correct: 46.41%\n",
      "Validation percentage of correct: 44.33%\n",
      "\n",
      "Average loss at step 8600: 1.917410 learning rate: 0.071790\n",
      "Percentage_of correct: 46.64%\n",
      "Validation percentage of correct: 44.16%\n",
      "\n",
      "Average loss at step 8800: 1.910847 learning rate: 0.064611\n",
      "Percentage_of correct: 46.63%\n",
      "Validation percentage of correct: 43.87%\n",
      "\n",
      "Average loss at step 9000: 1.945201 learning rate: 0.058150\n",
      "Percentage_of correct: 45.39%\n",
      "Validation percentage of correct: 44.43%\n",
      "\n",
      "Average loss at step 9200: 1.924328 learning rate: 0.058150\n",
      "Percentage_of correct: 46.78%\n",
      "Validation percentage of correct: 43.92%\n",
      "\n",
      "Average loss at step 9400: 1.904110 learning rate: 0.052335\n",
      "Percentage_of correct: 46.68%\n",
      "Validation percentage of correct: 44.58%\n",
      "\n",
      "Average loss at step 9600: 1.953587 learning rate: 0.052335\n",
      "Percentage_of correct: 45.90%\n",
      "Validation percentage of correct: 44.50%\n",
      "\n",
      "Average loss at step 9800: 1.971892 learning rate: 0.047101\n",
      "Percentage_of correct: 45.11%\n",
      "Validation percentage of correct: 44.20%\n",
      "\n",
      "Average loss at step 10000: 1.970772 learning rate: 0.042391\n",
      "Percentage_of correct: 44.98%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "@×ing [[Wartsbert withob]]\n",
      "*O\n",
      "[[Simision of Glee on]]\n",
      "\n",
      "{{Thatlagionchier Rin Cas\n",
      "Reperore: Hplitory ovition as Prgition of bit Errodmy&quot;&amp;ndarbitise= East\n",
      "¿e the placked it dopogon ligkeneds be] and the genther for to the friminus uita\n",
      "ëkition_, SÀbellthill stbligen Ho durcal landuigh user [[Clannce]], Juns of [[di\n",
      "3lsite, Andianoy ty undurd unvervew,\n",
      "\n",
      "[[hatory:And Aprauba diunds fariza]]s a xi\n",
      "================================================================================\n",
      "\n",
      "from fuse:\n",
      "================================================================================\n",
      "0. fuse: my name is\n",
      "my name is worket Docam }Ñ. Loden bater bechands phenecis &quot;Pinte0&quot; (&quot;1)\n",
      "\n",
      "T\n",
      "1. fuse: december elegy\n",
      "december elegy [[Eton in Inditar]]</text>\n",
      "    </reves ompental hot doued a thom the Schipt, T\n",
      "2. fuse: they have done\n",
      "they have doneletien. She came im othering, ale or Nurian-Arthriftrodine uncertion for theis \n",
      "================================================================================\n",
      "Validation percentage of correct: 44.52%\n",
      "\n",
      "Number of steps = 10001     Percentage = 46.01%     Time = 1658s     Learning rate = 0.0424\n"
     ]
    }
   ],
   "source": [
    "fuse_texts = [u'my name is', u'december elegy', u'they have done']\n",
    "model.run(30,\n",
    "          0.9,\n",
    "            200,\n",
    "            50,\n",
    "            3,\n",
    "            1,\n",
    "            20,\n",
    "            print_intermediate_results = True,\n",
    "          fuse_texts=fuse_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps = 10001     Percentage = 45.18%     Time = 519s     Learning rate = 0.0424\n"
     ]
    }
   ],
   "source": [
    "model.simple_run(200,\n",
    "                'one_gate/variables/gates_test',\n",
    "                10000,\n",
    "                4000,\n",
    "                5000,        #learning has a chance to be stopped after every block of steps\n",
    "                30,\n",
    "                0.9,\n",
    "                3,\n",
    "                fixed_num_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0:\n",
      "len(x_list): 1\n",
      "len(word['data']): 1\n",
      "len(word['error']): 1\n",
      "\n",
      "1:\n",
      "len(x_list): 2\n",
      "len(word['data']): 2\n",
      "len(word['error']): 2\n",
      "\n",
      "2:\n",
      "len(x_list): 3\n",
      "len(word['data']): 3\n",
      "len(word['error']): 3\n",
      "\n",
      "3:\n",
      "len(x_list): 4\n",
      "len(word['data']): 4\n",
      "len(word['error']): 4\n",
      "\n",
      "4:\n",
      "len(x_list): 5\n",
      "len(word['data']): 5\n",
      "len(word['error']): 5\n",
      "\n",
      "5:\n",
      "len(x_list): 6\n",
      "len(word['data']): 6\n",
      "len(word['error']): 6\n",
      "\n",
      "6:\n",
      "len(x_list): 7\n",
      "len(word['data']): 7\n",
      "len(word['error']): 7\n",
      "\n",
      "7:\n",
      "len(x_list): 8\n",
      "len(word['data']): 8\n",
      "len(word['error']): 8\n",
      "\n",
      "8:\n",
      "len(x_list): 9\n",
      "len(word['data']): 9\n",
      "len(word['error']): 9\n",
      "\n",
      "9:\n",
      "len(x_list): 10\n",
      "len(word['data']): 10\n",
      "len(word['error']): 10\n"
     ]
    }
   ],
   "source": [
    "text_list, gate_list = model.run_for_analitics(model.get_gates,\n",
    "                                                        'one_gate/variables/gates_test',\n",
    "                                                        [100, 75, None])\n",
    "structure_vocabulary_plots(text_list,\n",
    "                            gate_list,\n",
    "                            'gate for letter position',\n",
    "                            'mean gate',\n",
    "                            ['one_gate', 'test'],\n",
    "                            'gate_test_structure',\n",
    "                            ylims = [0., 1.],\n",
    "                            show=False)\n",
    "for i in range(99):\n",
    "    text_plot(text_list[i],\n",
    "                gate_list[i],\n",
    "                'gate',\n",
    "                'gate',\n",
    "                ['one_gate', 'test', 'text_plots'],\n",
    "                '#%s' % i,\n",
    "                show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      number of nodes: 256\n",
      "num_unrollings: 10\n",
      "Number of steps = 160001     Percentage = 56.04%     Time = 2944s     Learning rate = 0.0424\n",
      "num_unrollings: 20\n",
      "Number of steps = 165001     Percentage = 56.87%     Time = 6075s     Learning rate = 0.0424\n",
      "num_unrollings: 40\n",
      "Number of steps = 160001     Percentage = 57.24%     Time = 12083s     Learning rate = 0.0424\n",
      "      number of nodes: 384\n",
      "num_unrollings: 10\n",
      "Number of steps = 180001     Percentage = 38.07%     Time = 3668s     Learning rate = 0.0309\n",
      "num_unrollings: 20\n",
      "Number of steps = 170001     Percentage = 55.48%     Time = 6921s     Learning rate = 0.0382\n",
      "num_unrollings: 40\n",
      "Number of steps = 160001     Percentage = 59.26%     Time = 13252s     Learning rate = 0.0424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_nodes_values = [256, 384]\n",
    "num_unrollings_values = [10, 20, 40]\n",
    "\n",
    "results_GL = list()\n",
    "for num_nodes_value in num_nodes_values:\n",
    "    print(' '*5, 'number of nodes:', num_nodes_value)\n",
    "    for num_unrollings_value in num_unrollings_values:\n",
    "        print('num_unrollings:', num_unrollings_value)\n",
    "        model = one_gate(64,\n",
    "                            vocabulary,\n",
    "                            characters_positions_in_vocabulary,\n",
    "                            num_unrollings_value,\n",
    "                            1,\n",
    "                            [num_nodes_value],\n",
    "                          0.,\n",
    "                            train_text,\n",
    "                            valid_text) \n",
    "        model.simple_run(200,\n",
    "                        'one_gate/variables/ns80000_numstairs30_dc0.9/nn%s_nu%s' % (num_nodes_value, num_unrollings_value),\n",
    "                         160000,\n",
    "                         1000,\n",
    "                                   5000,        #learning has a chance to be stopped after every block of steps\n",
    "                                   30,\n",
    "                                   0.9,\n",
    "                                   3) \n",
    "        results_GL.extend(model._results)\n",
    "        model.destroy()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling one_gate/one_gate_ns_160000_decay_steps_30_dc_0.9_nn64-384_nu_10-40.pickle.\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'one_gate'\n",
    "file_name = 'one_gate_ns_160000_decay_steps_30_dc_0.9_nn64-384_nu_10-40.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_name = 'one_gate'\n",
    "pickle_file = 'one_gate_ns_160000_decay_steps_30_dc_0.9_nn64-384_nu_10-40.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_gate_plots = ComparePlots('rnn')\n",
    "one_gate_plots.add_network(results_GL, model._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEgCAYAAAAUmRE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzocxxhhjqmLnwxhjjDFV+X+/Eg2ZBm2cUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4b4a7aad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data, _ = one_gate_plots.one_key_layout_data('one_gate_1',\n",
    "                                                                'num_nodes[0]',\n",
    "                                         \"num_unrollings\")\n",
    "one_gate_plots.save_layout(plot_data[0],\n",
    "                    'number of nodes effect (1 gate RNN)',\n",
    "                    ['plots'],\n",
    "                    'ns80000;decay_steps_30;ilr_1.;dc0.9')\n",
    "one_gate_plots.draw(plot_data[0], 'number of nodes effect (1 gate RNN)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps = 20000     Percentage = 48.70%     Time = 1061s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.47%     Time = 1051s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.33%     Time = 1047s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.46%     Time = 1058s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.53%     Time = 1029s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.63%     Time = 1047s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.54%     Time = 1065s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.34%     Time = 1056s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.73%     Time = 1055s     Learning rate = 0.0424\n",
      "Number of steps = 20000     Percentage = 48.57%     Time = 1064s     Learning rate = 0.0424\n"
     ]
    }
   ],
   "source": [
    "iter_num = 10\n",
    "results_GL = list()   \n",
    "for i in range(iter_num):\n",
    "    model = one_gate(64,\n",
    "                             vocabulary,\n",
    "                             characters_positions_in_vocabulary,\n",
    "                             30,\n",
    "                             1,\n",
    "                             [128],\n",
    "                             0.,\n",
    "                             train_text,\n",
    "                             valid_text)\n",
    "    model.simple_run(200,\n",
    "                         'one_gate/variables/average#%s' % i,\n",
    "                            20000,\n",
    "                               4000,\n",
    "                               5000,        #learning has a chance to be stopped after every block of steps\n",
    "                               30,\n",
    "                               0.9,\n",
    "                               3,\n",
    "                    fixed_num_steps=True)\n",
    "    results_GL.extend(model._results)\n",
    "    model.destroy()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling one_gate/average10_ns20k_dc0.9_hl667.pickle.\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'one_gate'\n",
    "file_name = 'average%s_ns20k_dc0.9_hl667.pickle' % iter_num\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s.' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_num = 10\n",
    "pickle_file = 'average%s_ns20k_dc0.9_hl667.pickle' % num_iter\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average = 0\n",
    "for result in results_GL:\n",
    "    average += result['data']['train']['percentage'][-1]\n",
    "average /= len(results_GL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.5302864583\n"
     ]
    }
   ],
   "source": [
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iter_num = 10\n",
    "for i in range(iter_num):\n",
    "    model = one_gate(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 1,\n",
    "                                 [128],\n",
    "                                 0.,\n",
    "                                 train_text,\n",
    "                                 valid_text)\n",
    "    text_list, gate_list = model.run_for_analitics(model.get_gates,\n",
    "                                                            'one_gate/variables/average#%s' % i,\n",
    "                                                            [1000, 75, None])\n",
    "    structure_vocabulary_plots(text_list,\n",
    "                                gate_list,\n",
    "                                'gate for letter position',\n",
    "                                'mean gate',\n",
    "                                ['one_gate', 'ns20000_hl667_dc0.9', 'average#%s' % i],\n",
    "                                'vocabulary_structure_gate_average#%s' % i,\n",
    "                                ylims = [0., 1.],\n",
    "                                show=False)\n",
    "    for j in range(50):\n",
    "        text_plot(text_list[j],\n",
    "                    gate_list[j],\n",
    "                    'gate',\n",
    "                    'gate',\n",
    "                    ['one_gate', 'ns20000_hl667_dc0.9', 'average#%s' % i, 'text_plots'],\n",
    "                    '#%s' % j,\n",
    "                    show=False)\n",
    "    model.destroy()\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
