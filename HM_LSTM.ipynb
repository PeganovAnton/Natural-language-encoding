{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56184664\n"
     ]
    }
   ],
   "source": [
    "f = open('enwik8_clean2', 'rb')\n",
    "text = f.read().decode('utf8')\n",
    "print(len(text))\n",
    "f.close() \n",
    "(not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\t\n",
      " !\"'(),-.?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "print(vocabulary)\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(string_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix,\n",
    "                keep_dims=True):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\")\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=keep_dims,\n",
    "                                     name=\"reduce_mean_in_L2_norm\")\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\")\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.transpose(tf.multiply(tf.transpose(state[2],\n",
    "                                                                       name=\"transposed_state2_in_top_down_prepaired\"),\n",
    "                                                          tf.transpose(top_down,\n",
    "                                                                       name=\"transposed_top_down_in_top_down_prepaired\"),\n",
    "                                                          name=\"multiply_in_top_down_prepaired\"),\n",
    "                                              name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            boundary_state_reversed = tf.subtract(one, state[2], name=\"boundary_state_reversed\")\n",
    "            state0_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_reversed,\n",
    "                                                                     name=\"transposed_boundary_state_reversed_in_state0_prepaired\"),\n",
    "                                                        tf.transpose(state[0],\n",
    "                                                                     name=\"transposed_state0_state0_prepaired\"),\n",
    "                                                        name=\"multiply_in_state0_prepaired\"),\n",
    "                                            name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate, None, 'forget_gate_layer%s' % idx, keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                                      name=\"logical_and_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"),\n",
    "                                                     name=\"to_float_in_copy_flag\"),\n",
    "                                         name=\"copy_flag\")\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                                      name=\"to_float_in_flush_flag\"),\n",
    "                                          name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg,\n",
    "                          \"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate,\n",
    "                                          None,\n",
    "                                          \"forget_gate_layer%s\"%(self._num_layers-1),\n",
    "                                          keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                helper = {\"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, helper\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": self.gradient_name}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "        \"\"\"X = tf.sign(X)\"\"\"\n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\")\n",
    "        return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "            helpers = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "\n",
    "            hidden = inp\n",
    "            boundary = activated_boundary_states\n",
    "            # All layers except for the first and the last ones\n",
    "            for idx in range(num_layers-1):\n",
    "                hidden, memory, boundary, helper = self.not_last_layer(idx,\n",
    "                                                                       state[idx],\n",
    "                                                                       hidden,\n",
    "                                                                       state[idx+1][0],\n",
    "                                                                       boundary)\n",
    "                helpers.append(helper)\n",
    "                new_state.append((hidden, memory, boundary))\n",
    "                boundaries.append(boundary)\n",
    "            hidden, memory, helper = self.last_layer(state[-1],\n",
    "                                                     hidden,\n",
    "                                                     boundary)\n",
    "            helpers.append(helper)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\"),\n",
    "                      \"L2_forget_gate\": tf.stack([helper[\"L2_forget_gate\"] for helper in helpers],\n",
    "                                                 name=\"L2_forget_gate_for_iteration%s\"%iter_idx)}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm,\n",
    "                      \"L2_forget_gate\": tf.stack([helper['L2_forget_gate'] for helper in iteration_helpers],\n",
    "                                                 name=\"L2_forget_gate_all\")}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.transpose(tf.sigmoid(tf.matmul(concat,\n",
    "                                                                    self.output_module_gates_weights,\n",
    "                                                                    name=\"matmul_in_output_module_gates\"),\n",
    "                                                          name=\"sigmoid_in_output_module_gates\"),\n",
    "                                               name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=\"tr_hidden_state_total_%s\"%idx)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=\"tr_gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"),\n",
    "                                               name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "            \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1.,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "                 matr_init_parameter=1000.,\n",
    "                 override_appendix='',\n",
    "                 init_bias=-0.01):               \n",
    "                                                   \n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter\n",
    "        self._init_bias = init_bias\n",
    "        self.gradient_name = 'HardSigmoid' + override_appendix\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"matr_init_parameter\": 14,\n",
    "                         \"init_bias\":15,\n",
    "                         \"type\": 16}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            tf.set_random_seed(1)\n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                \n",
    "                def compute_dim_and_bias():\n",
    "                    bias_init_values = [0.]*(4*self._num_nodes[layer_idx])\n",
    "                    if layer_idx == self._num_layers - 1:\n",
    "                        input_dim = self._num_nodes[-1] + self._num_nodes[-2]\n",
    "                        output_dim = 4 * self._num_nodes[-1]\n",
    "                    else:\n",
    "                        output_dim = 4 * self._num_nodes[layer_idx] + 1\n",
    "                        bias_init_values.append(self._init_bias)\n",
    "                        if layer_idx == 0:\n",
    "                            input_dim = self._embedding_size + self._num_nodes[0] + self._num_nodes[1]\n",
    "                        else:\n",
    "                            input_dim = self._num_nodes[layer_idx - 1] + self._num_nodes[layer_idx] + self._num_nodes[layer_idx+1]\n",
    "                    stddev = math.sqrt(self._init_parameter*matr_init_parameter/input_dim)\n",
    "                    return input_dim, output_dim, bias_init_values, stddev\n",
    "                \n",
    "                for layer_idx in range(self._num_layers):\n",
    "                    input_dim, output_dim, bias_init_values, stddev = compute_dim_and_bias()\n",
    "                    self.Biases.append(tf.Variable(bias_init_values,\n",
    "                                                   name=bias_name%layer_idx))         \n",
    "                    self.Matrices.append(tf.Variable(tf.truncated_normal([input_dim,\n",
    "                                                                          output_dim],\n",
    "                                                                         mean=0.,\n",
    "                                                                         stddev=stddev,\n",
    "                                                                         name=init_matr_name%0),\n",
    "                                                     name=matr_name%layer_idx))     \n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_embedding_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "                    train_inputs_for_slice = tf.stack(train_inputs,\n",
    "                                                      axis=1,\n",
    "                                                      name=\"train_inputs_for_slice\")\n",
    "                    self.train_input_print = tf.reshape(tf.split(train_inputs_for_slice,\n",
    "                                                                 [1, self._batch_size-1],\n",
    "                                                                 name=\"split_in_train_print\")[0],\n",
    "                                                        [self._num_unrollings, -1],\n",
    "                                                        name=\"train_print\")\n",
    "\n",
    "\n",
    "                    self.saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 0)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 0)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 1)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 1)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                      name=saved_state_init_templ%(i, 2)),\n",
    "                                                             trainable=False,\n",
    "                                                              name=saved_state_templ%(i, 2))))\n",
    "                    self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                             tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "                    \n",
    "                    @tf.RegisterGradient(self.gradient_name)\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, self.saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "                    \n",
    "                    self.train_hard_sigm_arg = tf.reshape(tf.split(train_helper[\"hard_sigm_arg\"],\n",
    "                                                                   [1, self._batch_size-1],\n",
    "                                                                   name=\"split_in_train_hard_sigm_arg\")[0],\n",
    "                                                          [self._num_unrollings, -1],\n",
    "                                                          name=\"train_hard_sigm_arg\")\n",
    "                    \n",
    "                    L2_forget_gate_reduced = tf.reduce_mean(train_helper['L2_forget_gate'],\n",
    "                                                            axis=0,\n",
    "                                                            name=\"L2_forget_gate_reduced\")\n",
    "                    \n",
    "                    self.L2_forget_gate = tf.unstack(L2_forget_gate_reduced, name=\"L2_forget_gate_unstacked\")\n",
    "                    \n",
    "                    flush_fractions_stacked = tf.reduce_mean(train_helper['all_boundaries'],\n",
    "                                                             axis=[0, 1],\n",
    "                                                             name='flush_fractions_stacked')\n",
    "                    \n",
    "                    self.flush_fractions = tf.split(flush_fractions_stacked,\n",
    "                                                    self._num_layers-1,\n",
    "                                                    axis=0,\n",
    "                                                    name='flush_fractions')\n",
    "                    \n",
    "                    L2_hard_sigm_arg_stacked = self.L2_norm(train_helper['hard_sigm_arg'],\n",
    "                                                            [0, 1],\n",
    "                                                            'hard_sigm_arg_stacked',\n",
    "                                                            keep_dims=False)\n",
    "                    \n",
    "                    self.L2_hard_sigm_arg = tf.split(L2_hard_sigm_arg_stacked,\n",
    "                                                     self._num_layers-1,\n",
    "                                                     name='hard_sigm_arg')\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(self.saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    #optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    print_list = tf.split(self._train_prediction, self._num_unrollings, name=\"print_list\")\n",
    "                    print_for_slice = tf.stack(print_list, axis=1, name=\"print_for_slice\")\n",
    "                    self.train_output_print = tf.reshape(tf.split(print_for_slice,\n",
    "                                                                  [1, self._batch_size-1],\n",
    "                                                                  name=\"split_in_train_print\")[0],\n",
    "                                                         [self._num_unrollings, -1],\n",
    "                                                         name=\"train_print\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "                    \n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "                # creating control dictionary\n",
    "                all_vars = tf.global_variables()\n",
    "                self.control_dictionary = dict()\n",
    "                #print('building graph')\n",
    "                for variable in all_vars:\n",
    "                    \n",
    "                    list_to_form_name = variable.name.split('/')\n",
    "                    if ':' in list_to_form_name[-1]:\n",
    "                        list_to_form_name[-1] = list_to_form_name[-1].split(':')[0]\n",
    "                    if len(list_to_form_name) < 2:\n",
    "                        name = list_to_form_name[0] \n",
    "                    else:\n",
    "                        name = list_to_form_name[0] + '_' + list_to_form_name[-1]\n",
    "                    norm = self.L2_norm(tf.to_float(variable,\n",
    "                                                    name=\"to_float_in_control_dictionary_for_\"+list_to_form_name[-1]),\n",
    "                                        None,\n",
    "                                        list_to_form_name[-1],\n",
    "                                        keep_dims=False)\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        self.control_dictionary[name] = tf.summary.scalar(name+'_sum', \n",
    "                                                                          norm)\n",
    "                    #print(name, ': ', norm.get_shape().as_list())\n",
    "            forget_template = 'self.L2_forget_gate[%s]'\n",
    "            for layer_idx in range(self._num_layers):\n",
    "                self.control_dictionary[forget_template % layer_idx] = tf.summary.scalar(forget_template % layer_idx +'_sum', \n",
    "                                                                                         self.L2_forget_gate[layer_idx])\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "            \n",
    "    def reinit(self,\n",
    "               init_slope,\n",
    "               slope_growth,\n",
    "               slope_half_life,\n",
    "               init_parameter,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "               matr_init_parameter):\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter        \n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._matr_init_parameter)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n"
     ]
    }
   ],
   "source": [
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 10,\n",
    "                 3,\n",
    "                 [128, 128, 128],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.158884 learning rate: 0.002582\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "VNROQjdekOzYevlHxA.msIA\tJdCInJ-\n",
      "e!\"t?IB-pTGdDZ!JNFwBwWdEiLxHfOAmiKUCQLSJEg,cns.M\n",
      ") VS\"kYrpl)uRu'-cIas\tqb,z!gbs\"o-I.qXBXk'(L)\tEU\"DBcUvWzql JnO RUKwOcsPBvA\tmwFotC'\n",
      "fGhGUAC,?IawFg.VF\tLievUtRUYnEl n!'!Mtvs(UO,-EGjHOlK\"uOUEkorbL\n",
      "te)uuV wUWJPPziZ\"!\n",
      "\tsMTSAZMd\tISJJZ-huuQiD(LnCrLQ?fynGd(PfEXqzV\n",
      "nvkJLVSxKCdRiZjueuhzszEEHjmpOKdckTIL\n",
      ")p\tywp!Yyyg.ZUN\n",
      "\tqp?Cvv\n",
      "Zq rDFlsR\"J(.swgWx.fP\"zMzozrToDe?S?jjhbXMSaRJmTj?BHXsKzM\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [-0.01000292 -0.01      ]\n",
      "1:\n",
      "self.sigm_arg:  [-0.00999576 -0.01      ]\n",
      "2:\n",
      "self.sigm_arg:  [-0.00999703 -0.01      ]\n",
      "3:\n",
      "self.sigm_arg:  [-0.0099966 -0.01     ]\n",
      "4:\n",
      "self.sigm_arg:  [-0.01000597 -0.01      ]\n",
      "5:\n",
      "self.sigm_arg:  [-0.01000021 -0.01      ]\n",
      "6:\n",
      "self.sigm_arg:  [-0.00999534 -0.01      ]\n",
      "7:\n",
      "self.sigm_arg:  [-0.01000254 -0.01      ]\n",
      "8:\n",
      "self.sigm_arg:  [-0.00999818 -0.01      ]\n",
      "9:\n",
      "self.sigm_arg:  [-0.01000596 -0.01      ]\n",
      "10:\n",
      "self.sigm_arg:  [-0.00999406 -0.01      ]\n",
      "11:\n",
      "self.sigm_arg:  [-0.00999759 -0.01      ]\n",
      "12:\n",
      "self.sigm_arg:  [-0.00999655 -0.01      ]\n",
      "13:\n",
      "self.sigm_arg:  [-0.01000253 -0.01      ]\n",
      "14:\n",
      "self.sigm_arg:  [-0.00999875 -0.01      ]\n",
      "15:\n",
      "self.sigm_arg:  [-0.00999657 -0.01      ]\n",
      "16:\n",
      "self.sigm_arg:  [-0.01000227 -0.01      ]\n",
      "17:\n",
      "self.sigm_arg:  [-0.01000197 -0.01      ]\n",
      "18:\n",
      "self.sigm_arg:  [-0.00999471 -0.01      ]\n",
      "19:\n",
      "self.sigm_arg:  [-0.01000235 -0.01      ]\n",
      "20:\n",
      "self.sigm_arg:  [-0.00999692 -0.01      ]\n",
      "21:\n",
      "self.sigm_arg:  [-0.00999404 -0.01      ]\n",
      "22:\n",
      "self.sigm_arg:  [-0.01000234 -0.01      ]\n",
      "23:\n",
      "self.sigm_arg:  [-0.01000595 -0.01      ]\n",
      "24:\n",
      "self.sigm_arg:  [-0.00999566 -0.01      ]\n",
      "25:\n",
      "self.sigm_arg:  [-0.00999656 -0.01      ]\n",
      "26:\n",
      "self.sigm_arg:  [-0.01000595 -0.01      ]\n",
      "27:\n",
      "self.sigm_arg:  [-0.00999566 -0.01      ]\n",
      "28:\n",
      "self.sigm_arg:  [-0.01000597 -0.01      ]\n",
      "29:\n",
      "self.sigm_arg:  [-0.00999472 -0.01      ]\n",
      "30:\n",
      "self.sigm_arg:  [-0.00999817 -0.01      ]\n",
      "31:\n",
      "self.sigm_arg:  [-0.01000197 -0.01      ]\n",
      "32:\n",
      "self.sigm_arg:  [-0.01000236 -0.01      ]\n",
      "33:\n",
      "self.sigm_arg:  [-0.00999692 -0.01      ]\n",
      "34:\n",
      "self.sigm_arg:  [-0.00999903 -0.01      ]\n",
      "35:\n",
      "self.sigm_arg:  [-0.00999655 -0.01      ]\n",
      "36:\n",
      "self.sigm_arg:  [-0.01000018 -0.01      ]\n",
      "37:\n",
      "self.sigm_arg:  [-0.01000595 -0.01      ]\n",
      "38:\n",
      "self.sigm_arg:  [-0.00999692 -0.01      ]\n",
      "39:\n",
      "self.sigm_arg:  [-0.00999654 -0.01      ]\n",
      "40:\n",
      "self.sigm_arg:  [-0.01000014 -0.01      ]\n",
      "41:\n",
      "self.sigm_arg:  [-0.00999758 -0.01      ]\n",
      "42:\n",
      "self.sigm_arg:  [-0.01000253 -0.01      ]\n",
      "43:\n",
      "self.sigm_arg:  [-0.00999656 -0.01      ]\n",
      "44:\n",
      "self.sigm_arg:  [-0.00999705 -0.01      ]\n",
      "45:\n",
      "self.sigm_arg:  [-0.01000254 -0.01      ]\n",
      "46:\n",
      "self.sigm_arg:  [-0.01000775 -0.01      ]\n",
      "47:\n",
      "self.sigm_arg:  [-0.00999691 -0.01      ]\n",
      "48:\n",
      "self.sigm_arg:  [-0.00999404 -0.01      ]\n",
      "49:\n",
      "self.sigm_arg:  [-0.01000252 -0.01      ]\n",
      "50:\n",
      "self.sigm_arg:  [-0.0100061 -0.01     ]\n",
      "51:\n",
      "self.sigm_arg:  [-0.0100042 -0.01     ]\n",
      "52:\n",
      "self.sigm_arg:  [-0.00999655 -0.01      ]\n",
      "53:\n",
      "self.sigm_arg:  [-0.01000595 -0.01      ]\n",
      "54:\n",
      "self.sigm_arg:  [-0.00999556 -0.01      ]\n",
      "55:\n",
      "self.sigm_arg:  [-0.00999406 -0.01      ]\n",
      "56:\n",
      "self.sigm_arg:  [-0.01000197 -0.01      ]\n",
      "57:\n",
      "self.sigm_arg:  [-0.01000254 -0.01      ]\n",
      "58:\n",
      "self.sigm_arg:  [-0.00999472 -0.01      ]\n",
      "59:\n",
      "self.sigm_arg:  [-0.00999656 -0.01      ]\n",
      "60:\n",
      "self.sigm_arg:  [-0.01000253 -0.01      ]\n",
      "61:\n",
      "self.sigm_arg:  [-0.00999875 -0.01      ]\n",
      "62:\n",
      "self.sigm_arg:  [-0.00999657 -0.01      ]\n",
      "63:\n",
      "self.sigm_arg:  [-0.01000342 -0.01      ]\n",
      "64:\n",
      "self.sigm_arg:  [-0.00999706 -0.01      ]\n",
      "65:\n",
      "self.sigm_arg:  [-0.01000198 -0.01      ]\n",
      "66:\n",
      "self.sigm_arg:  [-0.00999759 -0.01      ]\n",
      "67:\n",
      "self.sigm_arg:  [-0.00999655 -0.01      ]\n",
      "68:\n",
      "self.sigm_arg:  [-0.00999787 -0.01      ]\n",
      "69:\n",
      "self.sigm_arg:  [-0.01000235 -0.01      ]\n",
      "70:\n",
      "self.sigm_arg:  [-0.00999565 -0.01      ]\n",
      "71:\n",
      "self.sigm_arg:  [-0.01000039 -0.01      ]\n",
      "72:\n",
      "self.sigm_arg:  [-0.0100002 -0.01     ]\n",
      "73:\n",
      "self.sigm_arg:  [-0.01000253 -0.01      ]\n",
      "74:\n",
      "self.sigm_arg:  [-0.00999905 -0.01      ]\n",
      "75:\n",
      "self.sigm_arg:  [-0.00999656 -0.01      ]\n",
      "76:\n",
      "self.sigm_arg:  [-0.01000253 -0.01      ]\n",
      "77:\n",
      "self.sigm_arg:  [-0.00999875 -0.01      ]\n",
      "78:\n",
      "self.sigm_arg:  [-0.00999657 -0.01      ]\n",
      "79:\n",
      "self.sigm_arg:  [-0.01000826 -0.01      ]\n",
      "80:\n",
      "self.sigm_arg:  [-0.01000254 -0.01      ]\n",
      "81:\n",
      "self.sigm_arg:  [-0.01000022 -0.01      ]\n",
      "82:\n",
      "self.sigm_arg:  [-0.00999656 -0.01      ]\n",
      "83:\n",
      "self.sigm_arg:  [-0.01000235 -0.01      ]\n",
      "84:\n",
      "self.sigm_arg:  [-0.00999692 -0.01      ]\n",
      "85:\n",
      "self.sigm_arg:  [-0.00999654 -0.01      ]\n",
      "86:\n",
      "self.sigm_arg:  [-0.01000731 -0.01      ]\n",
      "87:\n",
      "self.sigm_arg:  [-0.01000235 -0.01      ]\n",
      "88:\n",
      "self.sigm_arg:  [-0.00999405 -0.01      ]\n",
      "89:\n",
      "self.sigm_arg:  [-0.01000196 -0.01      ]\n",
      "90:\n",
      "self.sigm_arg:  [-0.01000235 -0.01      ]\n",
      "91:\n",
      "self.sigm_arg:  [-0.00999565 -0.01      ]\n",
      "92:\n",
      "self.sigm_arg:  [-0.00999656 -0.01      ]\n",
      "93:\n",
      "self.sigm_arg:  [-0.00999973 -0.01      ]\n",
      "94:\n",
      "self.sigm_arg:  [-0.01000254 -0.01      ]\n",
      "95:\n",
      "self.sigm_arg:  [-0.00999557 -0.01      ]\n",
      "96:\n",
      "self.sigm_arg:  [-0.01000343 -0.01      ]\n",
      "97:\n",
      "self.sigm_arg:  [-0.0100042 -0.01     ]\n",
      "98:\n",
      "self.sigm_arg:  [-0.00999656 -0.01      ]\n",
      "99:\n",
      "self.sigm_arg:  [-0.01000018 -0.01      ]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "CK-fliJTSyNbv'kqQWiD,z(nN'!.ozOLIIHA\tqNygwQ-PiOoZJqPfnbdlP-XYzKsTw\tDVBAXIDSG fA'xcjULPjIoLynrDYAUSPw\n",
      "********************\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      " spiritual\n",
      "self.train_hard_sigm_arg:  [[ 0.08435075 -0.08957674]\n",
      " [-0.05151174  0.0306717 ]\n",
      " [ 0.17297195 -0.22560503]\n",
      " [ 0.02431886 -0.09983058]\n",
      " [-0.00331685 -0.0041204 ]\n",
      " [ 0.17317188 -0.06342967]\n",
      " [ 0.00514197 -0.2761142 ]\n",
      " [ 0.03189064 -0.15556356]\n",
      " [-0.01153042 -0.01244307]\n",
      " [ 0.03286396 -0.22818522]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.68679237]\n",
      "   [1]: [ 0.11509434]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.10076871]\n",
      "   [1]: [ 0.27643117]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.477174\n",
      "   [1]: 0.477038\n",
      "   [2]: 0.496308\n",
      "Average loss at step 100: 3.286080 learning rate: 0.002582\n",
      "Percentage_of correct: 16.54%\n",
      "0:\n",
      "self.sigm_arg:  [-0.0117608  -0.00438095]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.023659   -0.08762402]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.01379779 -0.33291566]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.02750112  0.01816683]\n",
      "4:\n",
      "self.sigm_arg:  [-0.05265713 -0.00156842]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.03659968 -0.68573481]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.1278795  -0.92797983]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.0914989  -0.27319992]\n",
      "8:\n",
      "self.sigm_arg:  [-0.02603008 -0.00064612]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.10812236 -0.0627719 ]\n",
      "10:\n",
      "self.sigm_arg:  [-0.00370758 -0.00884424]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.17050174 -0.17276168]\n",
      "12:\n",
      "self.sigm_arg:  [-0.00169037 -0.02752075]\n",
      "13:\n",
      "self.sigm_arg:  [-0.06772798 -0.02752075]\n",
      "14:\n",
      "self.sigm_arg:  [ 0.04133319 -0.57880104]\n",
      "15:\n",
      "self.sigm_arg:  [ 0.05809012 -0.04021987]\n",
      "16:\n",
      "self.sigm_arg:  [-0.05681654  0.0280711 ]\n",
      "17:\n",
      "self.sigm_arg:  [ 0.08176845 -0.68393928]\n",
      "18:\n",
      "self.sigm_arg:  [ 0.10358019 -0.50581616]\n",
      "19:\n",
      "self.sigm_arg:  [ 0.02406083 -0.12239218]\n",
      "20:\n",
      "self.sigm_arg:  [ -1.40465228e-02  -8.20336863e-05]\n",
      "21:\n",
      "self.sigm_arg:  [ 0.19422507 -0.24676964]\n",
      "22:\n",
      "self.sigm_arg:  [ 0.02060857 -0.10254528]\n",
      "23:\n",
      "self.sigm_arg:  [-0.02266577 -0.00310357]\n",
      "24:\n",
      "self.sigm_arg:  [ 0.02195074 -0.18998675]\n",
      "25:\n",
      "self.sigm_arg:  [ 0.0041634   0.05317779]\n",
      "26:\n",
      "self.sigm_arg:  [-0.0489617  -0.00227574]\n",
      "27:\n",
      "self.sigm_arg:  [ 0.00867503 -0.18677944]\n",
      "28:\n",
      "self.sigm_arg:  [ 0.016757   -0.09838729]\n",
      "29:\n",
      "self.sigm_arg:  [-0.00155091 -0.0065322 ]\n",
      "30:\n",
      "self.sigm_arg:  [ 0.18509729 -0.18859811]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.05072397 -0.77301139]\n",
      "32:\n",
      "self.sigm_arg:  [ 0.09008957 -0.23096982]\n",
      "33:\n",
      "self.sigm_arg:  [-0.01931177  0.00187536]\n",
      "34:\n",
      "self.sigm_arg:  [ 0.19610217 -0.27019569]\n",
      "35:\n",
      "self.sigm_arg:  [ 0.02182424  0.02869213]\n",
      "36:\n",
      "self.sigm_arg:  [-0.02851491 -0.00209637]\n",
      "37:\n",
      "self.sigm_arg:  [ 0.24488895 -0.07579621]\n",
      "38:\n",
      "self.sigm_arg:  [ 0.0123026  -0.33133546]\n",
      "39:\n",
      "self.sigm_arg:  [ 0.02730072  0.01824479]\n",
      "40:\n",
      "self.sigm_arg:  [-0.05359022 -0.00192263]\n",
      "41:\n",
      "self.sigm_arg:  [-0.01832487 -0.0003373 ]\n",
      "42:\n",
      "self.sigm_arg:  [ 0.0892837  -0.07291303]\n",
      "43:\n",
      "self.sigm_arg:  [-0.01185883 -0.01461289]\n",
      "44:\n",
      "self.sigm_arg:  [-0.07625945 -0.01461289]\n",
      "45:\n",
      "self.sigm_arg:  [-0.01653002 -0.01461289]\n",
      "46:\n",
      "self.sigm_arg:  [ 0.03178066 -0.24269225]\n",
      "47:\n",
      "self.sigm_arg:  [ 0.03592766 -0.37977937]\n",
      "48:\n",
      "self.sigm_arg:  [ 0.04543294 -0.37421006]\n",
      "49:\n",
      "self.sigm_arg:  [ 0.02409118 -0.14162177]\n",
      "50:\n",
      "self.sigm_arg:  [ 0.0069104  -0.52967519]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.1160834  -1.12304819]\n",
      "52:\n",
      "self.sigm_arg:  [ 0.10735448 -0.13984957]\n",
      "53:\n",
      "self.sigm_arg:  [-0.07884924  0.0364861 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54:\n",
      "self.sigm_arg:  [-0.00036817  0.00077886]\n",
      "55:\n",
      "self.sigm_arg:  [ 0.05009682 -0.26911715]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.06826755 -0.80342716]\n",
      "57:\n",
      "self.sigm_arg:  [ 0.09274493 -0.26491225]\n",
      "58:\n",
      "self.sigm_arg:  [-0.01460539 -0.00340996]\n",
      "59:\n",
      "self.sigm_arg:  [ 0.170114    0.03330792]\n",
      "60:\n",
      "self.sigm_arg:  [-0.03437696 -0.00162333]\n",
      "61:\n",
      "self.sigm_arg:  [ 0.04659138 -0.55069643]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.06492753 -0.04389616]\n",
      "63:\n",
      "self.sigm_arg:  [-0.06767505  0.02695757]\n",
      "64:\n",
      "self.sigm_arg:  [-0.10008349 -0.00106431]\n",
      "65:\n",
      "self.sigm_arg:  [ 0.01416    -0.71452212]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.09983386 -0.37217236]\n",
      "67:\n",
      "self.sigm_arg:  [-0.00937856 -0.02679764]\n",
      "68:\n",
      "self.sigm_arg:  [-0.0559099  -0.02679764]\n",
      "69:\n",
      "self.sigm_arg:  [ 0.12445274 -0.07777749]\n",
      "70:\n",
      "self.sigm_arg:  [-0.0161144  -0.00221665]\n",
      "71:\n",
      "self.sigm_arg:  [ 0.12510268 -0.34106898]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.07450366 -0.80716079]\n",
      "73:\n",
      "self.sigm_arg:  [ 0.08875646 -0.26176265]\n",
      "74:\n",
      "self.sigm_arg:  [-0.01542783 -0.00289332]\n",
      "75:\n",
      "self.sigm_arg:  [ 0.17350326  0.03297667]\n",
      "76:\n",
      "self.sigm_arg:  [-0.03456973 -0.00155127]\n",
      "77:\n",
      "self.sigm_arg:  [ 0.04675833 -0.55060953]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.06490631 -0.04387878]\n",
      "79:\n",
      "self.sigm_arg:  [-0.05235087  0.02695836]\n",
      "80:\n",
      "self.sigm_arg:  [ 0.06095873 -0.07003261]\n",
      "81:\n",
      "self.sigm_arg:  [ 0.03066707 -0.71388263]\n",
      "82:\n",
      "self.sigm_arg:  [ 0.0815821  -0.07668276]\n",
      "83:\n",
      "self.sigm_arg:  [-0.06496219  0.02929813]\n",
      "84:\n",
      "self.sigm_arg:  [ 0.00654682 -0.30433714]\n",
      "85:\n",
      "self.sigm_arg:  [ 0.02728031  0.01975518]\n",
      "86:\n",
      "self.sigm_arg:  [-0.04065876 -0.00211148]\n",
      "87:\n",
      "self.sigm_arg:  [ 0.11286323 -0.05317457]\n",
      "88:\n",
      "self.sigm_arg:  [ 0.00544159 -0.28777495]\n",
      "89:\n",
      "self.sigm_arg:  [ 0.06802292 -0.80486947]\n",
      "90:\n",
      "self.sigm_arg:  [ 0.09028427 -0.23356116]\n",
      "91:\n",
      "self.sigm_arg:  [-0.02747195  0.00260714]\n",
      "92:\n",
      "self.sigm_arg:  [ 0.09775679  0.05983047]\n",
      "93:\n",
      "self.sigm_arg:  [-0.01441753 -0.00313383]\n",
      "94:\n",
      "self.sigm_arg:  [ 0.20020325 -0.08443454]\n",
      "95:\n",
      "self.sigm_arg:  [ 0.0007383  -0.10514345]\n",
      "96:\n",
      "self.sigm_arg:  [-0.01419917 -0.01435092]\n",
      "97:\n",
      "self.sigm_arg:  [-0.03241174 -0.01435092]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.61078501 -0.03694316]\n",
      "99:\n",
      "self.sigm_arg:  [ 0.00368618 -0.43496826]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "i hmje  hg  dieotra-vsstcmyg osesdesiheGdstIidir m iyrlanrckwidJue ja. oe onoalo\" pTtanei d  yanu e \n",
      "********************\n",
      "Validation percentage of correct: 14.80%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      " of the st\n",
      "self.train_hard_sigm_arg:  [[ 0.14238606 -0.01232044]\n",
      " [-0.04784624  0.06879169]\n",
      " [-0.01724266 -0.01530085]\n",
      " [ 0.26589718 -0.0157513 ]\n",
      " [-0.0270942   0.05674458]\n",
      " [ 0.27240551  0.02271509]\n",
      " [ 0.0240859   0.00949162]\n",
      " [-0.02593227  0.00095944]\n",
      " [-0.22693577 -0.00175541]\n",
      " [ 0.19915104  0.01852217]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.52264154]\n",
      "   [1]: [ 0.53018874]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.14195597]\n",
      "   [1]: [ 0.02683958]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.498172\n",
      "   [1]: 0.479238\n",
      "   [2]: 0.496319\n",
      "Average loss at step 200: 2.952804 learning rate: 0.002582\n",
      "Percentage_of correct: 18.43%\n",
      "0:\n",
      "self.sigm_arg:  [-0.01411749 -0.01031322]\n",
      "1:\n",
      "self.sigm_arg:  [-0.03721911 -0.01031322]\n",
      "2:\n",
      "self.sigm_arg:  [-0.03293174 -0.01031322]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.21251537 -0.02146433]\n",
      "4:\n",
      "self.sigm_arg:  [-0.04852096  0.06535973]\n",
      "5:\n",
      "self.sigm_arg:  [-0.0051475  -0.01214678]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.45962995  0.04048518]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.01215552 -0.00065792]\n",
      "8:\n",
      "self.sigm_arg:  [-0.01335485  0.00074467]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.16476981  0.00226947]\n",
      "10:\n",
      "self.sigm_arg:  [-0.00502365 -0.00157   ]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.24594572  0.00963991]\n",
      "12:\n",
      "self.sigm_arg:  [-0.02871902  0.0004166 ]\n",
      "13:\n",
      "self.sigm_arg:  [-0.24933973 -0.00167551]\n",
      "14:\n",
      "self.sigm_arg:  [-0.09421629 -0.00505439]\n",
      "15:\n",
      "self.sigm_arg:  [ 0.25584227 -0.01611977]\n",
      "16:\n",
      "self.sigm_arg:  [-0.03298453  0.0614564 ]\n",
      "17:\n",
      "self.sigm_arg:  [ 0.19379182  0.03102225]\n",
      "18:\n",
      "self.sigm_arg:  [ 0.03268406  0.0267961 ]\n",
      "19:\n",
      "self.sigm_arg:  [ 0.00029093 -0.00032884]\n",
      "20:\n",
      "self.sigm_arg:  [-0.01091404  0.00605313]\n",
      "21:\n",
      "self.sigm_arg:  [ 0.24382669  0.0284223 ]\n",
      "22:\n",
      "self.sigm_arg:  [ 0.00180551 -0.00058444]\n",
      "23:\n",
      "self.sigm_arg:  [-0.03018713  0.0054583 ]\n",
      "24:\n",
      "self.sigm_arg:  [-0.03192918 -0.00250002]\n",
      "25:\n",
      "self.sigm_arg:  [ 0.1357417  -0.01825788]\n",
      "26:\n",
      "self.sigm_arg:  [-0.05045234  0.07242182]\n",
      "27:\n",
      "self.sigm_arg:  [-0.02537203 -0.0145025 ]\n",
      "28:\n",
      "self.sigm_arg:  [ 0.15233247 -0.00560963]\n",
      "29:\n",
      "self.sigm_arg:  [-0.00703012 -0.00373526]\n",
      "30:\n",
      "self.sigm_arg:  [ 0.21396133  0.01991633]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.03071047  0.04249444]\n",
      "32:\n",
      "self.sigm_arg:  [  1.13182524e-02  -2.58032233e-05]\n",
      "33:\n",
      "self.sigm_arg:  [-0.00961896  0.00306257]\n",
      "34:\n",
      "self.sigm_arg:  [ 0.24015087  0.02457955]\n",
      "35:\n",
      "self.sigm_arg:  [-0.01675791  0.00104777]\n",
      "36:\n",
      "self.sigm_arg:  [-0.23185809 -0.00145129]\n",
      "37:\n",
      "self.sigm_arg:  [ 0.11054033 -0.00341399]\n",
      "38:\n",
      "self.sigm_arg:  [-0.00647277 -0.00011782]\n",
      "39:\n",
      "self.sigm_arg:  [ 0.20668413 -0.01123396]\n",
      "40:\n",
      "self.sigm_arg:  [-0.03414422  0.06388047]\n",
      "41:\n",
      "self.sigm_arg:  [ 0.1216676  -0.00169774]\n",
      "42:\n",
      "self.sigm_arg:  [-0.00870832 -0.03170497]\n",
      "43:\n",
      "self.sigm_arg:  [-0.05487822 -0.03170497]\n",
      "44:\n",
      "self.sigm_arg:  [-0.25653744 -0.03170497]\n",
      "45:\n",
      "self.sigm_arg:  [-0.11757638 -0.03170497]\n",
      "46:\n",
      "self.sigm_arg:  [-0.03971016 -0.03170497]\n",
      "47:\n",
      "self.sigm_arg:  [ 0.23710063 -0.00357593]\n",
      "48:\n",
      "self.sigm_arg:  [ 0.01489809 -0.01967437]\n",
      "49:\n",
      "self.sigm_arg:  [-0.01189666 -0.04970453]\n",
      "50:\n",
      "self.sigm_arg:  [-0.00922196 -0.04970453]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.40025315  0.0263886 ]\n",
      "52:\n",
      "self.sigm_arg:  [-0.00612828  0.00203457]\n",
      "53:\n",
      "self.sigm_arg:  [-0.24298814 -0.00125105]\n",
      "54:\n",
      "self.sigm_arg:  [-0.11195175 -0.00416601]\n",
      "55:\n",
      "self.sigm_arg:  [-0.04752591 -0.00416601]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.27473253  0.03849038]\n",
      "57:\n",
      "self.sigm_arg:  [  1.07532563e-02   9.41762701e-05]\n",
      "58:\n",
      "self.sigm_arg:  [-0.01055269 -0.00167328]\n",
      "59:\n",
      "self.sigm_arg:  [ 0.18593326 -0.01575604]\n",
      "60:\n",
      "self.sigm_arg:  [-0.04794822  0.06635742]\n",
      "61:\n",
      "self.sigm_arg:  [-0.01404369 -0.01330667]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.26176235 -0.01840897]\n",
      "63:\n",
      "self.sigm_arg:  [-0.05552681  0.05935626]\n",
      "64:\n",
      "self.sigm_arg:  [-0.16645163 -0.01285917]\n",
      "65:\n",
      "self.sigm_arg:  [-0.04866467 -0.00965331]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.37781748  0.00416927]\n",
      "67:\n",
      "self.sigm_arg:  [-0.0296981  -0.00041514]\n",
      "68:\n",
      "self.sigm_arg:  [-0.23004876 -0.00565382]\n",
      "69:\n",
      "self.sigm_arg:  [ 0.18983573 -0.00350716]\n",
      "70:\n",
      "self.sigm_arg:  [-0.01257764 -0.00026412]\n",
      "71:\n",
      "self.sigm_arg:  [ 0.16736469  0.032119  ]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.04123077  0.05017882]\n",
      "73:\n",
      "self.sigm_arg:  [ 0.01523154  0.00093179]\n",
      "74:\n",
      "self.sigm_arg:  [-0.01091321 -0.00120779]\n",
      "75:\n",
      "self.sigm_arg:  [ 0.17972308 -0.01551299]\n",
      "76:\n",
      "self.sigm_arg:  [-0.04807004  0.06707819]\n",
      "77:\n",
      "self.sigm_arg:  [-0.01384676 -0.01330519]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.2617847  -0.01819145]\n",
      "79:\n",
      "self.sigm_arg:  [-0.0336968   0.05935115]\n",
      "80:\n",
      "self.sigm_arg:  [ 0.13898608 -0.00951822]\n",
      "81:\n",
      "self.sigm_arg:  [ 0.01546408  0.03652881]\n",
      "82:\n",
      "self.sigm_arg:  [-0.0018538 -0.000531 ]\n",
      "83:\n",
      "self.sigm_arg:  [-0.24332626 -0.00363468]\n",
      "84:\n",
      "self.sigm_arg:  [-0.09982762 -0.00363468]\n",
      "85:\n",
      "self.sigm_arg:  [ 0.20674182 -0.01613976]\n",
      "86:\n",
      "self.sigm_arg:  [-0.02919787  0.06595015]\n",
      "87:\n",
      "self.sigm_arg:  [ 0.22806126 -0.00849293]\n",
      "88:\n",
      "self.sigm_arg:  [ 0.00035652  0.01825694]\n",
      "89:\n",
      "self.sigm_arg:  [ 0.03477353  0.0438309 ]\n",
      "90:\n",
      "self.sigm_arg:  [ 0.01132394  0.00011218]\n",
      "91:\n",
      "self.sigm_arg:  [-0.01551641 -0.00186583]\n",
      "92:\n",
      "self.sigm_arg:  [ 0.1315947  -0.01762863]\n",
      "93:\n",
      "self.sigm_arg:  [-0.03854355  0.07241902]\n",
      "94:\n",
      "self.sigm_arg:  [ 0.13567176 -0.01118565]\n",
      "95:\n",
      "self.sigm_arg:  [-0.02032319 -0.00958242]\n",
      "96:\n",
      "self.sigm_arg:  [-0.06270455 -0.00958242]\n",
      "97:\n",
      "self.sigm_arg:  [-0.11078081 -0.00958242]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.59500277 -0.01382779]\n",
      "99:\n",
      "self.sigm_arg:  [-0.02604043  0.04260099]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ous advocate of Christian anarchism was Leo Tolstoy, author of \"The Kingdom of God is Within You\", w\n",
      "********************\n",
      "output:\n",
      "nsinr siinearntta ee,hfl fsynie ciiJsxeDotnftaei l  Gtae eiThehJa. llcni  rlAi esenf eao iarnilcseEs\n",
      "********************\n",
      "Validation percentage of correct: 19.20%\n",
      "\n",
      "Pickling first.pickle\n",
      "Number of steps = 201     Percentage = 1.75%     Time = 19s     Learning rate = 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=201,\n",
    "            add_operations=['self.train_hard_sigm_arg', 'self.flush_fractions', 'self.L2_hard_sigm_arg', 'self.L2_forget_gate'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200],\n",
    "            path_to_file_for_saving_collection='first.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='plotting_check.txt')\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self.train_input_print': ['e [[Spanish Civil War]] (1936-39) was underway. \\n\\nIn reponse to the army rebellion [[Anarchism in Sp', 'sm]], which sees the domination of nature as a metaphor for the domination of women. Green anarchism'], 'self.train_hard_sigm_arg': [array([[ 0.02667233,  0.05716731],\n",
      "       [-0.00393393, -0.06405934],\n",
      "       [ 0.09011293, -0.01508802],\n",
      "       [ 0.08015606, -0.01934055],\n",
      "       [-0.05960941,  0.00930282],\n",
      "       [-0.03023605, -0.06283204],\n",
      "       [-0.01795208,  0.0098001 ],\n",
      "       [-0.05768362, -0.06297438],\n",
      "       [-0.00234846,  0.00980703],\n",
      "       [-0.11052101, -0.06323457],\n",
      "       [-0.14944284,  0.00981735],\n",
      "       [ 0.15891951, -0.06042811],\n",
      "       [-0.11329076,  0.00915652],\n",
      "       [-0.02035111, -0.06349671],\n",
      "       [-0.22491711,  0.00982776],\n",
      "       [ 0.06852727, -0.02753093],\n",
      "       [-0.19820814,  0.00583803],\n",
      "       [ 0.11053218, -0.06495447],\n",
      "       [-0.08151877,  0.00972066],\n",
      "       [-0.03023626, -0.06403664],\n",
      "       [-0.08415863,  0.00984925],\n",
      "       [ 0.07435532, -0.0539517 ],\n",
      "       [-0.03343182,  0.00843449],\n",
      "       [ 0.04957724, -0.06941799],\n",
      "       [ 0.01904528, -0.00396347],\n",
      "       [ 0.10635369, -0.02834202],\n",
      "       [ 0.11456136, -0.02686169],\n",
      "       [ 0.11049786, -0.02384662],\n",
      "       [ 0.10197129, -0.0218526 ],\n",
      "       [ 0.02628678,  0.00192135],\n",
      "       [ 0.09515808, -0.09826759],\n",
      "       [ 0.10365232, -0.02321959],\n",
      "       [ 0.03062857,  0.00446017],\n",
      "       [ 0.01488105, -0.07068592],\n",
      "       [-0.16176182,  0.0106327 ],\n",
      "       [ 0.00473842, -0.0286885 ],\n",
      "       [-0.20667845,  0.0060637 ],\n",
      "       [ 0.11591981, -0.06406307],\n",
      "       [-0.11149514,  0.00965784],\n",
      "       [-0.05778341, -0.06361958],\n",
      "       [-0.12417955,  0.0098328 ],\n",
      "       [ 0.00264698, -0.0159863 ],\n",
      "       [-0.18068711,  0.00494961],\n",
      "       [-0.067168  , -0.0643882 ],\n",
      "       [ 0.01101115,  0.04588573],\n",
      "       [-0.18110666, -0.06427936],\n",
      "       [ 0.03772295,  0.03196601],\n",
      "       [ 0.00730931, -0.07076716],\n",
      "       [ 0.0488676 , -0.01508804],\n",
      "       [ 0.07418546, -0.01694489],\n",
      "       [ 0.01235717,  0.00353899],\n",
      "       [-0.12425441, -0.06294364],\n",
      "       [ 0.09174684,  0.00883968],\n",
      "       [-0.14549957, -0.06237727],\n",
      "       [-0.0285692 ,  0.00978209],\n",
      "       [-0.00531932, -0.06273899],\n",
      "       [-0.00164552,  0.00979754],\n",
      "       [-0.06236599, -0.06308445],\n",
      "       [-0.09724785,  0.00981134],\n",
      "       [-0.00686811, -0.06329881],\n",
      "       [ 0.09677964,  0.01095841],\n",
      "       [-0.11899571, -0.06273098],\n",
      "       [ 0.0065709 ,  0.03819237],\n",
      "       [ 0.00443506, -0.06974901],\n",
      "       [-0.13075365,  0.01046915],\n",
      "       [-0.18068711, -0.06298073],\n",
      "       [ 0.02545852,  0.05689409],\n",
      "       [-0.00403528, -0.06342986],\n",
      "       [-0.05656522,  0.0098257 ],\n",
      "       [-0.0878001 , -0.06357249],\n",
      "       [-0.04584106,  0.00983085],\n",
      "       [-0.07709188, -0.06360485],\n",
      "       [ 0.11823624,  0.01189093],\n",
      "       [-0.14679775, -0.06294527],\n",
      "       [-0.0284669 ,  0.0098046 ],\n",
      "       [-0.07082855, -0.06310172],\n",
      "       [-0.01998098,  0.00981189],\n",
      "       [-0.09056564, -0.06331255],\n",
      "       [-0.0687415 ,  0.00982037],\n",
      "       [ 0.01738431, -0.02580546],\n",
      "       [-0.09776565,  0.00572142],\n",
      "       [-0.06926493, -0.06412091],\n",
      "       [ 0.09648032,  0.01002954],\n",
      "       [ 0.06674691, -0.08896838],\n",
      "       [ 0.06347455, -0.01442656],\n",
      "       [-0.01304861,  0.00901823],\n",
      "       [-0.08518687, -0.06199621],\n",
      "       [-0.0039191 ,  0.00976574],\n",
      "       [-0.08111008, -0.06240583],\n",
      "       [-0.06870787,  0.00978408],\n",
      "       [-0.15668875, -0.0628767 ],\n",
      "       [ 0.05784563,  0.04739949],\n",
      "       [-0.19545747, -0.06321332],\n",
      "       [-0.03558632,  0.00981674],\n",
      "       [ 0.10340118, -0.06217678],\n",
      "       [-0.09815533,  0.00940271],\n",
      "       [-0.06355675, -0.06347661],\n",
      "       [ 0.0971364 ,  0.01014165],\n",
      "       [-0.06635045, -0.06282658],\n",
      "       [-0.02879305,  0.0097998 ]], dtype=float32), array([[-0.10459504, -0.09611879],\n",
      "       [-0.18896325,  0.01722709],\n",
      "       [-0.0451993 , -0.09133576],\n",
      "       [-0.09385356,  0.0171113 ],\n",
      "       [-0.39280277, -0.0878566 ],\n",
      "       [ 0.02969377,  0.01215458],\n",
      "       [-0.23034291, -0.08540243],\n",
      "       [-0.0687419 ,  0.01697092],\n",
      "       [ 0.31688979, -0.00150799],\n",
      "       [-0.20238633,  0.02181824],\n",
      "       [-0.05679221, -0.08825666],\n",
      "       [ 0.05035742,  0.03655804],\n",
      "       [-0.1541563 , -0.08778295],\n",
      "       [ 0.07780004,  0.10548586],\n",
      "       [ 0.05705088, -0.00400748],\n",
      "       [-0.11788162,  0.02141631],\n",
      "       [ 0.0156382 , -0.08821009],\n",
      "       [-0.44257835,  0.01878954],\n",
      "       [-0.06634868, -0.09008436],\n",
      "       [ 0.10728855,  0.11554135],\n",
      "       [-0.00585805, -0.0936683 ],\n",
      "       [-0.18015918,  0.01717234],\n",
      "       [ 0.24708793, -0.0239733 ],\n",
      "       [-0.22424757,  0.01949614],\n",
      "       [ 0.27204049, -0.02581651],\n",
      "       [-0.07512008,  0.01895296],\n",
      "       [ 0.23796636, -0.01718023],\n",
      "       [-0.39331257,  0.01965659],\n",
      "       [ 0.27535191, -0.03586223],\n",
      "       [ 0.19672769,  0.08093883],\n",
      "       [-0.03012083, -0.0960461 ],\n",
      "       [ 0.00162993,  0.02617258],\n",
      "       [ 0.17659712, -0.03075278],\n",
      "       [-0.08533326,  0.01708249],\n",
      "       [ 0.04008075, -0.07579953],\n",
      "       [-0.10935146,  0.0182558 ],\n",
      "       [ 0.23684767, -0.01652768],\n",
      "       [-0.39433023,  0.01947987],\n",
      "       [ 0.08413897, -0.01758513],\n",
      "       [-0.18417576,  0.02181941],\n",
      "       [ 0.07185381, -0.00620646],\n",
      "       [-0.01961802,  0.02288551],\n",
      "       [ 0.20294097, -0.02357768],\n",
      "       [-0.1158547 ,  0.01831757],\n",
      "       [ 0.01578096, -0.08867673],\n",
      "       [ 0.18031573,  0.08719212],\n",
      "       [ 0.01189427, -0.09557406],\n",
      "       [-0.25787377,  0.01692517],\n",
      "       [ 0.06539566, -0.00962349],\n",
      "       [-0.40835947,  0.02255182],\n",
      "       [ 0.24489054, -0.03463653],\n",
      "       [-0.19419791,  0.01846366],\n",
      "       [-0.07134376, -0.0923165 ],\n",
      "       [ 0.26881692,  0.09203794],\n",
      "       [-0.16003633, -0.09426302],\n",
      "       [ 0.00956589,  0.0205672 ],\n",
      "       [-0.12756655, -0.09063934],\n",
      "       [ 0.25750497,  0.0901509 ],\n",
      "       [-0.15643756, -0.09307664],\n",
      "       [ 0.00966565,  0.02068341],\n",
      "       [-0.43400979, -0.08985478],\n",
      "       [-0.06584757,  0.01707697],\n",
      "       [ 0.10755715,  0.01170862],\n",
      "       [-0.01839137, -0.09051694],\n",
      "       [-0.18054092,  0.01709789],\n",
      "       [ 0.24682009, -0.02211428],\n",
      "       [-0.22435474,  0.01943995],\n",
      "       [ 0.27204329, -0.02444435],\n",
      "       [-0.07506621,  0.01893177],\n",
      "       [ 0.23796995, -0.01630314],\n",
      "       [-0.3932671 ,  0.0196446 ],\n",
      "       [ 0.27535433, -0.03531094],\n",
      "       [ 0.19674593,  0.08093373],\n",
      "       [-0.03011081, -0.09569852],\n",
      "       [ 0.0016303 ,  0.02616553],\n",
      "       [ 0.17659627, -0.03052487],\n",
      "       [-0.0853226 ,  0.01707846],\n",
      "       [ 0.04008148, -0.07565417],\n",
      "       [-0.2198908 ,  0.01825291],\n",
      "       [ 0.22314315, -0.03284178],\n",
      "       [-0.22656575,  0.01819286],\n",
      "       [ 0.06693704, -0.00820544],\n",
      "       [-0.08159649,  0.02256222],\n",
      "       [-0.07585887, -0.0922251 ],\n",
      "       [-0.02776129,  0.01713717],\n",
      "       [-0.22241385, -0.08877306],\n",
      "       [-0.18115513,  0.0170516 ],\n",
      "       [ 0.06246258, -0.00607505],\n",
      "       [ 0.03613267,  0.1095003 ],\n",
      "       [-0.03004167, -0.09243765],\n",
      "       [ 0.00123207,  0.02617244],\n",
      "       [ 0.19512276, -0.0194045 ],\n",
      "       [-0.06141435,  0.01804858],\n",
      "       [ 0.23825738, -0.01643353],\n",
      "       [-0.16364537,  0.0197063 ],\n",
      "       [-0.1798466 , -0.09328973],\n",
      "       [-0.05807724,  0.0171609 ],\n",
      "       [ 0.31892106, -0.00603879],\n",
      "       [-0.12480651,  0.02205761],\n",
      "       [-0.18988648, -0.09169008]], dtype=float32)], 'step': [100, 200]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_file = 'first.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    save  # hint to help gc free up memory\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_parameter_value = 1e-6\n",
    "matr_init_parameter_value = 10000\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:peganov/HM_LSTM/folder_name/name_of_run/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 17.49%     Time = 18s     Learning rate = 0.0009\n"
     ]
    }
   ],
   "source": [
    "model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ 'folder_name' +'/'+'name_of_run'+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 18.29095699999999, 'metadata': [64, 30, 3, [128, 128, 128], 10, 0.8, 100, 100, 0.5, 0.5, 1000, 128, 1024, 1e-06, 10000, 'HM_LSTM'], 'data': {'train': {'perplexity': [24.632861709594728], 'BPC': [4.5201946005579066], 'step': [-1], 'percentage': [17.48645833333333]}, 'validation': {'perplexity': [24.349007891654967], 'BPC': [4.355342844963074], 'step': [-1], 'percentage': [17.8]}}}\n"
     ]
    }
   ],
   "source": [
    "print(model._results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           init parameter:  1e-05\n",
      "      matr init parameter:  50\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 24.06%     Time = 17s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp50/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      matr init parameter:  100\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 14.06%     Time = 18s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp100/variables\n",
      "      matr init parameter:  1000\n",
      "INFO:tensorflow:peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Number of steps = 100     Percentage = 13.94%     Time = 16s     Learning rate = 0.0028\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/nn128is0.5sg0.5shl1000/ip1e-05_imp1000/variables\n",
      "      matr init parameter:  10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5956ab78e8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                          \u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# a factor by which the learning rate decreases each 'half_life'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                          \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                          fixed_num_steps=True)\n\u001b[0m\u001b[1;32m     38\u001b[0m         text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n\u001b[1;32m     39\u001b[0m                                                 \u001b[0;34m'peganov/HM_LSTM/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname_of_run\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/variables'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36msimple_run\u001b[0;34m(self, num_averaging_iterations, save_path, min_num_steps, loss_frequency, block_of_steps, num_stairs, decay, stop_percent, save_steps, optional_feed_dict, half_life_fixed, fixed_num_steps)\u001b[0m\n\u001b[1;32m    424\u001b[0m                                     \u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf_life_fixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                                     fixed_num_steps=fixed_num_steps)\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mdata_for_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_percentages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_averaging_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_num_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mGLOBAL_STEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_num_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Natural-language-encoding/model_module.py\u001b[0m in \u001b[0;36mcalculate_percentages\u001b[0;34m(self, session, num_averaging_iterations)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_unrollings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_parameters = [1e-5, 1e-6, 1e-7, 1e-8]\n",
    "matr_init_parameters = [50, 100, 1000, 10000, 100000]\n",
    "num_nodes = 128\n",
    "init_slope = .5\n",
    "slope_growth = .5\n",
    "slope_half_life = 1000\n",
    "results_GL = list()\n",
    "run_idx = 0\n",
    "for init_parameter_value in init_parameters:\n",
    "    print(' '*10, \"init parameter: \", init_parameter_value)\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        print(' '*5, \"matr init parameter: \", matr_init_parameter_value)\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        folder_name = 'nn%sis%ssg%sshl%s' % (num_nodes, init_slope, slope_growth, slope_half_life)\n",
    "        model = HM_LSTM(64,\n",
    "                                 vocabulary,\n",
    "                                 characters_positions_in_vocabulary,\n",
    "                                 30,\n",
    "                                 3,\n",
    "                                 [num_nodes, num_nodes, num_nodes],\n",
    "                                 init_slope,\n",
    "                                 slope_growth,\n",
    "                                  slope_half_life,\n",
    "                                 train_text,\n",
    "                                 valid_text,\n",
    "                        init_parameter=init_parameter_value,\n",
    "                        matr_init_parameter=matr_init_parameter_value,\n",
    "                        override_appendix=str(run_idx))\n",
    "        model.simple_run(100,                # number of percents values used for final averaging\n",
    "                         'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                         100,              # minimum number of learning iterations\n",
    "                         20000,              # period of checking loss function. It is used defining if learning should be stopped\n",
    "                         20000,              # learning has a chance to be stopped after every block of steps\n",
    "                         10,                 # number of times 'learning_rate' is multiplied on 'decay'\n",
    "                         .8,                 # a factor by which the learning rate decreases each 'half_life'\n",
    "                         3,                  # if fixed_num_steps=False this parameter defines when the learning process should be stopped. If during half the total learning time loss function decreased less than by 'stop_percent' percents the learning would be stopped\n",
    "                         fixed_num_steps=True)\n",
    "        text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/'+ folder_name +'/'+name_of_run+'/variables',\n",
    "                                                [10, 75, None])\n",
    "        for i in range(4):\n",
    "            text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', folder_name, name_of_run, 'plots'],\n",
    "                            name_of_run+'#%s' % i,\n",
    "                            show=False)\n",
    "        results_GL.append(model._results[-1])\n",
    "        run_idx += 1\n",
    "        model.destroy()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9bb3de01da55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ops'"
     ]
    }
   ],
   "source": [
    "print(tf.ops._gradient_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/server/thirteenth'\n",
    "pickle_file = 'thirteenth.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory\n",
    "model._results = results_GL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 110020.39302300001, 'data': {'validation': {'perplexity': [195.99988384094237, 10.467789069133996, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'BPC': [7.6117769776105879, 2.832817962588043, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'percentage': [12.775, 43.73, 53.87, 55.135, 56.9, 58.745, 58.875, 60.545, 59.54, 60.795, 60.955, 61.38, 61.895, 61.785, 61.85, 63.445, 62.89, 63.285, 63.725, 63.245, 63.155, 63.605, 64.65, 64.265, 63.46, 63.83, 64.27, 64.445, 63.895, 64.99, 66.29, 64.815, 65.305, 65.02, 65.14, 65.145, 64.805, 64.72, 65.55, 65.45, 65.065, 64.78, 65.505, 64.975, 64.945, 64.935, 66.935, 66.205, 65.96, 66.32, 65.985, 66.155, 66.04, 67.065, 66.3, 65.77, 66.28, 66.78, 66.17, 65.91, 66.345, 67.635, 66.98, 67.355, 66.98, 66.895, 66.73, 66.475, 65.64, 67.235, 66.985, 66.66, 66.85, 67.165, 66.595, 66.86, 66.51, 67.91, 67.49, 67.55, 66.82, 67.04, 66.93, 67.155, 67.965, 67.66, 67.28, 67.725, 67.7, 67.0, 66.7, 67.46, 67.965, 67.895, 67.785, 67.735, 67.05, 68.06, 67.71, 67.11], 'step': [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000]}, 'train': {'perplexity': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'BPC': [2.7865733221410793, 2.0493870594653871, 2.1674407665318434, 1.9813355883107844, 1.8377986959180332, 1.7614817385629638, 1.8811998937244701, 1.7296558330728484, 1.8048564510411675, 1.7873761344074717, 1.7738732605402801, 1.6607305180692116, 1.6467988914534188, 1.7770926037817809, 1.6505262714455684, 1.6052716166107832, 1.7615197467288044, 1.6192923302397229, 1.7483666855734676, 1.7528483814723017, 1.5910895832553344, 1.6952391809301357, 1.5267539692552732, 1.7235683351629085, 1.6438820656947855, 1.5985402156560129, 1.5204074654731339, 1.5423032646850814, 1.5547850775541006, 1.550472268618414, 1.6423607071652531, 1.5746492457114059, 1.5442477005357365, 1.6502863556476148, 1.5285240146979533, 1.4652920133684786, 1.5753043276285423, 1.582623393301311, 1.6377809811554274, 1.5653613226515599, 1.4695107477941434, 1.5843019439737318, 1.4999650916436278, 1.5443710120964049, 1.58015750605324, 1.5420086584041535, 1.6351085427436702, 1.6834205331628436, 1.4744229162680869, 1.5715033390618234, 1.6152271763212787, 1.4928412262795885, 1.4771144447540898, 1.483107868154008, 1.4682599179744182, 1.4363929086307465, 1.5302393696576588, 1.456724697615748, 1.4121405171453514, 1.6878219131637244, 1.4875988511159883, 1.5716275105357465, 1.4206301827312176, 1.461062100070508, 1.4573845950471085, 1.4419927497341651, 1.4917585954924071, 1.4965789252307899, 1.4900540754397058, 1.525790178479838, 1.4228527145283176, 1.5018359189197135, 1.4453021258933927, 1.5010950176597964, 1.3955426434932325, 1.442063692577646, 1.4119617411797796, 1.5175915935305571, 1.422104074049112, 1.4614287670821473, 1.4681519128696763, 1.5051825293228529, 1.5067042318176869, 1.468864436992201, 1.4400652541746202, 1.4809104458239337, 1.4486057406338164, 1.4047576459091127, 1.4298748521543716, 1.3591707205928607, 1.4296861011950497, 1.4298665109958049, 1.4047457791062032, 1.4008060005402381, 1.5033576214145477, 1.445990056496844, 1.508461894509505, 1.4008953455273612, 1.5322759882091763], 'percentage': [33.06328125, 53.2296875, 57.15234375, 59.59296875, 61.05078125, 62.365625, 61.9421875, 62.821875, 63.12265625, 63.91953125, 64.46015625, 64.5921875, 65.06015625, 65.5265625, 65.0640625, 65.4953125, 65.35234375, 65.75390625, 65.278125, 65.38515625, 66.32109375, 66.27890625, 66.303125, 66.05703125, 66.571875, 66.47109375, 67.584375, 67.665625, 67.08046875, 67.10859375, 67.23828125, 67.32734375, 67.80703125, 67.2875, 67.3140625, 67.42109375, 67.41796875, 67.66796875, 67.85625, 67.8890625, 68.36484375, 68.23671875, 68.1890625, 68.56328125, 68.2015625, 68.41328125, 67.7515625, 68.32578125, 68.34140625, 68.21796875, 68.2046875, 68.77578125, 68.5359375, 68.3375, 68.39296875, 68.8109375, 69.0875, 69.15234375, 69.09140625, 69.04296875, 68.76875, 68.80859375, 68.72421875, 68.865625, 68.921875, 68.684375, 69.33984375, 69.35, 68.95234375, 69.31640625, 69.1265625, 69.46015625, 69.6546875, 69.35859375, 70.0046875, 69.71328125, 69.3546875, 69.1921875, 69.31640625, 68.9953125, 69.446875, 68.73203125, 69.63125, 69.5703125, 69.55390625, 69.56953125, 69.86484375, 69.634375, 69.70234375, 69.9734375, 70.05859375, 70.00703125, 69.96171875, 69.60859375, 70.12578125, 70.0890625, 69.734375, 69.76015625, 70.0359375], 'step': [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000]}}, 'metadata': [64, 100, 3, [512, 512, 512], 5000, 0.9, 100000, 20, 1.0, 0.01, 1000, 128, 1024, 1e-07, 'HM_LSTM']}\n"
     ]
    }
   ],
   "source": [
    "print(results_GL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_all([0],\n",
    "               plot_validation=True,\n",
    "               indent=1,\n",
    "               save_folder='HM_LSTM/server/thirteenth',\n",
    "               show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling test_folder/plot_debug.pickle\n"
     ]
    }
   ],
   "source": [
    "results_GL = model._results\n",
    "folder_name = 'test_folder'\n",
    "file_name = 'plot_debug.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/server/HM_LSTM3/nn128is0.5sg0.5shl1000'\n",
    "pickle_file = 'nn128is0.5sg0.5shl1000.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAEkCAYAAADpUq91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEipjjDHGmBD9PzMgBiOeLGROAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3eca2dcc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['HM_LSTM3', 'init_tuning', 'plots'],\n",
    "                    'nn128is0.5sg0.5shl1000',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "var = 'aaa'\n",
    "if isinstance(var, str):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def split_to_path_name(path):\n",
    "        parts = path.split('/')\n",
    "        name = parts[-1]\n",
    "        path = '/'.join(parts[:-1])\n",
    "        return path, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "bbbb.pickle\n"
     ]
    }
   ],
   "source": [
    "path, name = split_to_path_name('bbbb.pickle')\n",
    "print(path == '')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/server/plotting_check'\n",
    "pickle_file = 'intermediate_plotting_check.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    five_input = dictionary['self.train_input_print'][0]\n",
    "    ten_input = dictionary['self.train_input_print'][1]\n",
    "    five_boundaries = dictionary['self.train_hard_sigm_arg'][0]\n",
    "    ten_boundaries = dictionary['self.train_hard_sigm_arg'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_boundaries(nparray):\n",
    "    nparray[nparray > 0.] = 1.\n",
    "    nparray[nparray < 0.] = 0.\n",
    "    return nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_double_list(nparray):\n",
    "    length = nparray.shape[0]\n",
    "    double_list = [list(), list()]\n",
    "    for i in range(length):\n",
    "        double_list[0].append(nparray[i, 0])\n",
    "        double_list[1].append(nparray[i, 1])\n",
    "    return double_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "five_boundaries = transform_to_double_list(transform_to_boundaries(five_boundaries))\n",
    "ten_boundaries = transform_to_double_list(transform_to_boundaries(ten_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "[[0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n",
      "ho are incapable of deviating from their caste-oriented &quot;values&quot;, which naturally include \n",
      "ed a relationship between Semitic and Hausa, but this would long remain a topic of dispute and uncer\n"
     ]
    }
   ],
   "source": [
    "print(five_boundaries)\n",
    "print(ten_boundaries)\n",
    "print(five_input)\n",
    "print(ten_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self.train_hard_sigm_arg': [array([[ -1.76893866e+00,   7.06919432e-02],\n",
      "       [  8.02100778e-01,  -1.61422133e+00],\n",
      "       [ -1.22373533e+00,   1.26379347e+00],\n",
      "       [ -1.99120891e+00,   7.88532197e-03],\n",
      "       [ -3.55150127e+00,  -3.02836895e-02],\n",
      "       [ -1.50237143e+00,   5.07119782e-02],\n",
      "       [ -1.52045250e+00,  -4.08056974e-02],\n",
      "       [ -2.71533060e+00,   4.56770696e-02],\n",
      "       [  2.09132135e-01,  -1.35579681e+00],\n",
      "       [ -1.55549634e+00,   3.69267464e-01],\n",
      "       [ -1.24235845e+00,   1.03158429e-02],\n",
      "       [ -9.87059176e-01,   9.21514630e-03],\n",
      "       [  4.43094850e-01,  -8.05277586e-01],\n",
      "       [  1.14021516e+00,   2.17401654e-01],\n",
      "       [  2.09892809e-01,  -1.46395016e+00],\n",
      "       [  3.43428016e-01,  -5.00795186e-01],\n",
      "       [ -9.62356091e-01,   6.55584335e-01],\n",
      "       [ -1.43426406e+00,   9.57106501e-02],\n",
      "       [ -5.39380252e-01,   4.19271588e-02],\n",
      "       [ -2.78759575e+00,   2.57370025e-02],\n",
      "       [ -2.16789007e+00,   1.99353471e-02],\n",
      "       [ -6.53168321e-01,   1.70652047e-02],\n",
      "       [ -2.93137503e+00,   1.56867802e-02],\n",
      "       [  9.35311615e-02,  -1.15385628e+00],\n",
      "       [ -1.33340144e+00,   1.34487534e+00],\n",
      "       [  1.51191425e+00,  -1.21091270e+00],\n",
      "       [  1.83058977e-02,   8.15034628e-01],\n",
      "       [ -9.25330698e-01,   1.65815294e-01],\n",
      "       [  7.27639437e-01,   2.25693420e-01],\n",
      "       [ -6.04348063e-01,   6.37538508e-02],\n",
      "       [ -3.31284523e-01,   5.38330339e-02],\n",
      "       [ -2.04489374e+00,   4.59023006e-02],\n",
      "       [  2.84092367e-01,  -2.06598783e+00],\n",
      "       [ -1.59485304e+00,   7.36682534e-01],\n",
      "       [ -1.47633541e+00,   9.73524153e-02],\n",
      "       [ -1.80517340e+00,   5.65983020e-02],\n",
      "       [ -1.37436616e+00,   4.52389829e-02],\n",
      "       [ -5.54227293e-01,   4.21734154e-02],\n",
      "       [ -3.80317956e-01,   3.93863916e-02],\n",
      "       [ -2.22796011e+00,   3.74728888e-02],\n",
      "       [ -2.11508465e+00,   3.60107645e-02],\n",
      "       [ -3.23793316e+00,   3.47796977e-02],\n",
      "       [ -1.72000432e+00,   3.36695611e-02],\n",
      "       [  2.56604373e-01,  -2.18971777e+00],\n",
      "       [  1.80386269e+00,  -9.41528738e-01],\n",
      "       [  8.68318677e-01,  -1.05601668e+00],\n",
      "       [ -7.61659741e-01,   5.55710912e-01],\n",
      "       [ -1.90764284e+00,   8.57729465e-02],\n",
      "       [ -2.84613276e+00,   6.54081553e-02],\n",
      "       [ -2.03372574e+00,   5.12474924e-02],\n",
      "       [ -9.94371712e-01,   4.27615531e-02],\n",
      "       [  1.87840426e+00,   9.01061416e-01],\n",
      "       [ -4.57584828e-01,   3.85024190e-01],\n",
      "       [  1.10061049e+00,  -1.44336605e+00],\n",
      "       [  1.17931139e+00,  -8.74822915e-01],\n",
      "       [ -1.14595819e+00,   4.97966349e-01],\n",
      "       [ -3.51076889e+00,   1.88885570e-01],\n",
      "       [  9.21158671e-01,   8.57107341e-01],\n",
      "       [  5.54667950e-01,  -4.52585399e-01],\n",
      "       [  2.43677950e+00,  -1.86988521e+00],\n",
      "       [  2.46077156e+00,  -1.21471143e+00],\n",
      "       [ -1.46720731e+00,   9.90959466e-01],\n",
      "       [ -4.48587084e+00,   1.60462767e-01],\n",
      "       [ -8.29706252e-01,   1.17510527e-01],\n",
      "       [ -9.23366845e-01,   7.60515481e-02],\n",
      "       [ -3.76664311e-01,   5.52851968e-02],\n",
      "       [  6.57214224e-02,  -7.92387784e-01],\n",
      "       [  1.50257075e+00,  -8.66914093e-02],\n",
      "       [ -2.23408842e+00,   3.78493428e-01],\n",
      "       [  2.84964228e+00,   1.58688140e+00],\n",
      "       [ -2.09574699e+00,   2.25552991e-01],\n",
      "       [  1.72733510e+00,  -2.91264606e+00],\n",
      "       [  1.52690613e+00,  -2.10532284e+00],\n",
      "       [ -1.07279003e+00,   3.93882364e-01],\n",
      "       [  1.76395667e+00,  -1.53621507e+00],\n",
      "       [ -2.54733372e+00,   5.75415611e-01],\n",
      "       [ -3.11661625e+00,   9.99065116e-02],\n",
      "       [ -1.99319458e+00,   7.22729564e-02],\n",
      "       [ -9.81257737e-01,   5.84077314e-02],\n",
      "       [ -4.79348242e-01,   4.73310910e-02],\n",
      "       [  1.14632201e+00,  -2.90630317e+00],\n",
      "       [ -1.75217295e+00,   7.16410160e-01],\n",
      "       [ -1.76490951e+00,  -1.37337670e-02],\n",
      "       [ -1.49586499e+00,   1.21137977e-01],\n",
      "       [ -1.40405297e-02,   1.39036775e-02],\n",
      "       [ -1.92633033e+00,   2.97817588e-03],\n",
      "       [ -8.56012702e-02,  -4.27014381e-03],\n",
      "       [  4.11816955e-01,  -7.42980182e-01],\n",
      "       [  1.05894530e+00,  -3.58675867e-01],\n",
      "       [  6.92451119e-01,  -5.56193531e-01],\n",
      "       [ -5.16263723e-01,   5.72324038e-01],\n",
      "       [ -2.03916812e+00,   1.88241899e-03],\n",
      "       [ -2.43083811e+00,  -2.71432847e-02],\n",
      "       [ -3.52901965e-01,   8.21349993e-02],\n",
      "       [ -1.75512147e+00,  -2.51491219e-02],\n",
      "       [ -1.36604965e+00,   8.16405565e-02],\n",
      "       [ -1.72028017e+00,  -1.95113346e-02],\n",
      "       [  9.48175430e-01,  -2.51164412e+00],\n",
      "       [  1.87738359e-01,  -8.16535652e-01],\n",
      "       [ -1.37273967e+00,   7.18362808e-01]], dtype=float32), array([[-0.6934669 ,  0.6848771 ],\n",
      "       [ 0.64556295,  1.79936349],\n",
      "       [ 0.16909361,  1.08472311],\n",
      "       [-1.05989504,  0.43983451],\n",
      "       [-1.38338375,  0.25771311],\n",
      "       [-2.28656912,  0.15385771],\n",
      "       [-0.23032537,  0.11348304],\n",
      "       [-0.33761621,  0.10613349],\n",
      "       [-0.08145192,  0.1067018 ],\n",
      "       [ 0.55607671, -1.47630191],\n",
      "       [-0.84529555,  0.65155917],\n",
      "       [ 0.23037446,  1.29083908],\n",
      "       [-0.81982601,  0.5781166 ],\n",
      "       [ 0.31774092,  1.91910028],\n",
      "       [-0.54740083,  0.628488  ],\n",
      "       [-0.06929111,  0.50044054],\n",
      "       [-0.51311475,  0.39507899],\n",
      "       [-0.26239806,  0.30095372],\n",
      "       [-0.63587207,  0.22654191],\n",
      "       [-0.18284078,  0.17127576],\n",
      "       [ 1.02614617, -2.97404099],\n",
      "       [ 0.53091216, -0.85419792],\n",
      "       [-0.01417199,  0.11302986],\n",
      "       [ 0.78237218, -0.28615656],\n",
      "       [ 0.64863521,  0.02680618],\n",
      "       [-0.80852795,  0.96195424],\n",
      "       [-1.85355425,  0.67459488],\n",
      "       [-0.85957497,  0.46140227],\n",
      "       [-1.15331793,  0.31207019],\n",
      "       [-0.28979525,  0.22146875],\n",
      "       [-0.13250698,  0.166325  ],\n",
      "       [ 0.18505758,  1.10876548],\n",
      "       [-0.24960804,  0.16750652],\n",
      "       [-0.25180048,  0.11189645],\n",
      "       [-1.33714807,  0.08167263],\n",
      "       [-1.13127255,  0.06961982],\n",
      "       [-0.65215504,  0.06343749],\n",
      "       [-0.82113767,  0.05834004],\n",
      "       [-2.34577823,  0.05467214],\n",
      "       [-0.83487344,  0.05217699],\n",
      "       [-1.50213194,  0.05049974],\n",
      "       [-0.58831751,  0.04940251],\n",
      "       [ 0.35438144, -1.90838313],\n",
      "       [ 2.26750278, -1.77168703],\n",
      "       [-1.71149707,  0.60897344],\n",
      "       [-1.09799802,  0.18661611],\n",
      "       [-0.59202379,  0.1632317 ],\n",
      "       [ 0.65971404, -2.47681737],\n",
      "       [-1.22091866,  0.2567324 ],\n",
      "       [-0.81105995,  0.02862704],\n",
      "       [-0.10874957,  0.03312632],\n",
      "       [ 0.10498989, -1.93053508],\n",
      "       [-0.47956771,  0.16265157],\n",
      "       [-0.40597475,  0.29152259],\n",
      "       [-1.84528661,  0.169346  ],\n",
      "       [-1.11168325,  0.11452194],\n",
      "       [-1.14598727,  0.09081948],\n",
      "       [ 0.66902572,  0.07761639],\n",
      "       [-0.81634206,  0.304629  ],\n",
      "       [-0.10651331,  0.28227672],\n",
      "       [-0.65258217,  0.23655868],\n",
      "       [ 0.18859172, -1.83343649],\n",
      "       [-0.01017585,  0.73665535],\n",
      "       [ 0.19962835,  0.48743656],\n",
      "       [-0.06015289,  0.93092149],\n",
      "       [-1.58028173,  0.66414148],\n",
      "       [-0.16594289,  0.49476469],\n",
      "       [-1.06366038,  0.39760441],\n",
      "       [-0.5495469 ,  0.33484611],\n",
      "       [-0.4805246 ,  0.29148147],\n",
      "       [ 0.25319642, -0.95633298],\n",
      "       [-0.29921412,  0.3175585 ],\n",
      "       [-1.37326407,  0.64409   ],\n",
      "       [-1.22149467,  0.37807   ],\n",
      "       [-1.82768083,  0.27373931],\n",
      "       [-0.57272357,  0.22745538],\n",
      "       [-1.72099209,  0.202454  ],\n",
      "       [-0.20040943,  0.18438174],\n",
      "       [ 0.73303574,  0.24437222],\n",
      "       [ 0.40877205,  0.83649611],\n",
      "       [-0.92583877,  0.16336568],\n",
      "       [-0.31859541,  0.11314364],\n",
      "       [-1.01708436,  0.09198025],\n",
      "       [-1.37048459,  0.0865306 ],\n",
      "       [-0.71211898,  0.08429945],\n",
      "       [-0.82589817,  0.0840677 ],\n",
      "       [-1.96491766,  0.08448825],\n",
      "       [-0.47345763,  0.08484863],\n",
      "       [ 1.51370382, -2.5707612 ],\n",
      "       [ 0.08697948, -0.37090665],\n",
      "       [-0.36380789,  0.35375795],\n",
      "       [-1.28100348,  0.45747826],\n",
      "       [-0.7088272 ,  0.28074864],\n",
      "       [-0.19863272,  0.18740365],\n",
      "       [-0.56976432,  0.14469774],\n",
      "       [-2.01432085,  0.12534615],\n",
      "       [-0.16510376,  0.11515943],\n",
      "       [-0.66569138,  0.10896116],\n",
      "       [-0.09329794,  0.10453915],\n",
      "       [ 0.1664418 ,  0.02539772]], dtype=float32)], 'self.train_input_print': ['ho are incapable of deviating from their caste-oriented &quot;values&quot;, which naturally include ', 'ed a relationship between Semitic and Hausa, but this would long remain a topic of dispute and uncer'], 'step': [5000, 10000]}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import text_boundaries_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABvMAAABFCAYAAAB30BoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5ad38dc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_boundaries_plot(ten_input, ten_boundaries, 'train 10000', ['HM_LSTM', 'server', 'plotting_check', 'plots'], 'train_10000.png', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HM_LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cecbf5cff237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = HM_LSTM(64,\n\u001b[0m\u001b[1;32m      6\u001b[0m                  \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                  \u001b[0mcharacters_positions_in_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HM_LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "model = HM_LSTM(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 10,\n",
    "                 3,\n",
    "                 [56, 59, 62],\n",
    "                 1.,               # init_slope\n",
    "                 0.1,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=1e-7,\n",
    "                 matr_init_parameter=10000)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "summary_dict = {'summaries_collection_frequency': 10,\n",
    "                'summary_tensors': [\"self.control_dictionary['embeddings_matrix_variable']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_2']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_2']\",\n",
    "                                    \"self.control_dictionary['output_gates_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_bias']\",\n",
    "                                    \"self.control_dictionary['output_weights']\",\n",
    "                                    \"self.control_dictionary['output_bias']\"]}\n",
    "\n",
    "saved_state_templ = \"'train_1_saved_state_layer%s_number%s'\"\n",
    "\n",
    "for i in range(model._num_layers):\n",
    "    for j in range(2):\n",
    "        summary_dict['summary_tensors'].append('self.control_dictionary[' + saved_state_templ % (i, j) + ']')\n",
    "for layer_idx in range(model._num_layers):\n",
    "    summary_dict['summary_tensors'].append(\"self.control_dictionary['self.L2_forget_gate[%s]']\"%layer_idx)\n",
    "\n",
    "\n",
    "logdir = \"peganov/HM_LSTM/track_nan/logging/first_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            10,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            10,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=101,\n",
    "            add_operations=['self.train_hard_sigm_arg'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "           #debug=True,\n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "\n",
    "            path_to_file_for_saving_collection='peganov/HM_LSTM/track_nan/track_nan.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='peganov/HM_LSTM/track_nan/track_nan.txt',\n",
    "           save_path=\"peganov/HM_LSTM/track_nan/variables\",\n",
    "             summarizing_logdir=logdir,\n",
    "            summary_dict=summary_dict)\n",
    "results_GL = list(model._results)\n",
    "text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/track_nan/variables',\n",
    "                                                [10, 75, None])\n",
    "\n",
    "for i in range(4):\n",
    "    text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', 'track_nan', 'plots'],\n",
    "                            'plot#%s' % i,\n",
    "                            show=False)\n",
    "\n",
    "folder_name = 'peganov/HM_LSTM/track_nan'\n",
    "file_name = 'track_nan_result.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
