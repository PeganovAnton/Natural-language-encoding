{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import _linear\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'w')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'r')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 22500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "print(\"Vocabulary: \", string_vocabulary)\n",
    "print(\"char2id(u'a') = %s,  char2id(u'z') = %s,  char2id(u' ') = %s\" % (char2id(u'a', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u'z', characters_positions_in_vocabulary),\n",
    "                                                                        char2id(u' ', characters_positions_in_vocabulary)))\n",
    "print(\"id2char(78) = %s,  id2char(156) = %s,  id2char(140) = %s\" % (id2char(78,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(156,\n",
    "                                                                            vocabulary),\n",
    "                                                                    id2char(140,\n",
    "                                                                            vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n",
    "\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(train_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))\n",
    "print(batches2string(valid_batches_test.next(), vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_GL = {\"batch_size\": 0,\n",
    "              \"num_unrollings\": 1,\n",
    "              \"num_layers\": 2,\n",
    "              \"num_nodes\": 3,\n",
    "              \"half_life\": 4,\n",
    "              \"decay\": 5,\n",
    "              \"num_steps\": 6,\n",
    "              \"averaging_number\": 7,\n",
    "              \"type\": 8}\n",
    "\n",
    "\n",
    "class delay_deterministic(MODEL):\n",
    "    \n",
    "    def analize(self, compressed_inp):\n",
    "        memory_concat = list(self.Sample_Memory[0])\n",
    "        memory_concat.append(compressed_inp)\n",
    "        for num_layer in range(1, self._memory_layers):\n",
    "            memory_concat.extend(self.Sample_Memory[num_layer])\n",
    "        X = tf.concat(1, memory_concat)\n",
    "        for Analisys_Matrix, Analisys_Bias in zip(self.Analisys_Matrices, self.Analisys_Biases):\n",
    "            X = tf.nn.relu(tf.nn.xw_plus_b(X, Analisys_Matrix, Analisys_Bias))\n",
    "        return X \n",
    "    \n",
    "    def update_below_including_layer(self, compressed_inp, layer_num):\n",
    "        assign_list = list()\n",
    "        not_changed_layers = [i for i in range(layer_num+1, self._memory_layers)]\n",
    "        not_changed_layers.sort(reverse=True)\n",
    "        for lay_num in not_changed_layers:\n",
    "            assign_list.extend(self.Sample_Memory[lay_num])\n",
    "        \n",
    "        if layer_num > 0:\n",
    "            layers_in_cycle = [i for i in range(1, layer_num+1)]\n",
    "            layers_in_cycle.sort(reverse=True)\n",
    "            for lay_num in layers_in_cycle:\n",
    "                for block_num in range(self._memory_lengths[lay_num]-1):\n",
    "                    with tf.control_dependencies(assign_list):\n",
    "                        assign_list.append(tf.assign(self.Sample_Memory[lay_num][block_num], self.Sample_Memory[lay_num][block_num+1]))\n",
    "                lower_layer_summary = tf.concat(1, self.Sample_Memory[lay_num-1])\n",
    "                new_block = tf.tanh(tf.nn.xw_plus_b(lower_layer_summary,\n",
    "                                                    self.Compress_Matrices[lay_num],\n",
    "                                                    self.Compress_Biases[lay_num]))\n",
    "                with tf.control_dependencies(assign_list):\n",
    "                    assign_list.append(tf.assign(self.Sample_Memory[lay_num][self._memory_lengths[lay_num]-1], new_block))\n",
    "        for block_num in range(self._memory_lengths[0]-1):\n",
    "            with tf.control_dependencies(assign_list):\n",
    "                assign_list.append(tf.assign(self.Sample_Memory[0][block_num], self.Sample_Memory[0][block_num+1]))\n",
    "        with tf.control_dependencies(assign_list):\n",
    "            assign_list.append(tf.assign(self.Sample_Memory[0][self._memory_lengths[0]-1], compressed_inp))\n",
    "        return assign_list\n",
    "    \n",
    "    def update_memory(self, compressed_inp, layer_num):\n",
    "        if layer_num > 0:\n",
    "            assign_list = tf.cond(tf.equal(tf.mod(self.counter, tf.constant(self.abs_freqs[layer_num], dtype=tf.int32)),\n",
    "                                           tf.constant(0, dtype=tf.int32)),\n",
    "                                  lambda: self.update_below_including_layer(compressed_inp, layer_num),\n",
    "                                  lambda: self.update_memory(compressed_inp, layer_num-1))  \n",
    "        else:\n",
    "            assign_list = self.update_below_including_layer(compressed_inp, 0)\n",
    "        return assign_list\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 memory_layers,             # integer\n",
    "                 memory_nodes,              # number of nodes in vector on memory layer (list of length memory_layers)\n",
    "                 memory_lengths,            # list of memory lengths for all memory levels (list of length memory_layers)\n",
    "                 analisys_layers,           # integer\n",
    "                 analisys_nodes,             # list of numbers of nodes on each analisys layer\n",
    "                 frequency,              # number of times the highest memory layer fully updates during one epoch\n",
    "                 init_bias,\n",
    "                 threshold,    #{'fixed': True/False, 'min':  , 'max':  ,'epochs':  }\n",
    "                 normal_run_prob,\n",
    "                 swap_prob,\n",
    "                 support,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 mean=0.,\n",
    "                 stddev='default',\n",
    "                 init_learning_rate=1.):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        \n",
    "        self._num_unrollings = memory_lengths[0]\n",
    "        for num_layer in range(1, memory_layers):\n",
    "            self._num_unrollings *= memory_lengths[num_layer]\n",
    "        self._num_unrollings *= frequency\n",
    "\n",
    "        self._memory_layers = memory_layers                 # number of memory layers\n",
    "        self._memory_nodes = memory_nodes                   # number of nodes in vector on memory layer\n",
    "        self._memory_lengths = memory_lengths               # number of vectors on memory layer\n",
    "        self._analisys_layers = analisys_layers\n",
    "        self._analisys_nodes = analisys_nodes\n",
    "        self._frequency = frequency\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        \n",
    "        \n",
    "        self._mean = mean\n",
    "        \n",
    "        self._stddev = list()\n",
    "        if stddev == 'default':\n",
    "            self._stddev = 1.4\n",
    "        else:\n",
    "            self._stddev = stddev\n",
    "\n",
    "        self._init_learning_rate = init_learning_rate\n",
    "  \n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"memory_layers\": 2,\n",
    "                         \"memory_nodes\": 3,\n",
    "                         \"memory_lengths\": 4,\n",
    "                         \"analisys_layers\": 5,\n",
    "                         \"analisys_nodes\": 6,\n",
    "                         \"frequency\": 7,\n",
    "                         \"half_life\": 8,\n",
    "                         \"decay\": 9,\n",
    "                         \"num_steps\": 10,\n",
    "                         \"averaging_number\": 11,\n",
    "                         \"init_mean\": 12,\n",
    "                         \"init_stddev\": 13,\n",
    "                         \"init_learning_rate\": 14,                         \n",
    "                         \"type\": 15}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with self._graph.device('/gpu:0'): \n",
    "                \n",
    "                second_dim_of_trigger_matrix = 0\n",
    "                second_dim_of_trigger_matrix = self._vocabulary_size\n",
    "                for memory_layer_idx, memory_length in enumerate(memory_lengths):\n",
    "                    second_dim_of_trigger_matrix += memory_length * memory_nodes[memory_layer_idx]\n",
    "                self.trigger_matrix = tf.Variable(tf.truncated_normal([second_dim_of_trigger_matrix, 1], stddev = 0.1))\n",
    "                self.trigger_bias = tf.Variable([self._init_bias])\n",
    "                \n",
    "                self.Saved_Memory = list()\n",
    "                for layer_num in range(self._memory_layers):\n",
    "                    layer_memory = list()\n",
    "                    for _ in range(self._memory_lengths[layer_num]):\n",
    "                        layer_memory.append(tf.Variable(tf.zeros([self._batch_size, self._memory_nodes[layer_num]]),\n",
    "                                                        trainable=False))\n",
    "                    self.Saved_Memory.append(layer_memory)\n",
    "                self.Compress_Matrices = list()\n",
    "                self.Compress_Biases = list()\n",
    "                self.Compress_Matrices.append(\n",
    "                    tf.Variable(tf.truncated_normal([self._vocabulary_size, self._memory_nodes[0]], mean=self._mean, stddev =self._stddev / self._vocabulary_size**0.5),\n",
    "                                trainable=True)\n",
    "                                        )\n",
    "    \n",
    "                self.Compress_Biases.append(\n",
    "                    tf.Variable(tf.zeros([self._memory_nodes[0]]),\n",
    "                                trainable=True)\n",
    "                                       )\n",
    "                for layer_num in range(1, self._memory_layers):\n",
    "                    self.Compress_Matrices.append(\n",
    "                        tf.Variable(tf.truncated_normal([self._memory_lengths[layer_num-1]*self._memory_nodes[layer_num-1],\n",
    "                                                         self._memory_nodes[layer_num]],\n",
    "                                                        mean=self._mean,\n",
    "                                                        stddev = self._stddev / (self._memory_lengths[layer_num-1]*self._memory_nodes[layer_num-1])**0.5),\n",
    "                                    trainable=True)\n",
    "                                                 )\n",
    "                    self.Compress_Biases.append(\n",
    "                        tf.Variable(tf.zeros([self._memory_nodes[layer_num]]),\n",
    "                                    trainable=True)\n",
    "                                                )\n",
    "                self.Analisys_Matrices = list()\n",
    "                self.Analisys_Biases = list()\n",
    "                first_dim = self._memory_nodes[0]\n",
    "                for layer_num in range(self._memory_layers):\n",
    "                    first_dim += self._memory_lengths[layer_num] * self._memory_nodes[layer_num]\n",
    "                if self._analisys_layers > 1:\n",
    "                    self.Analisys_Matrices.append(\n",
    "                        tf.Variable(\n",
    "                            tf.truncated_normal([first_dim, self._analisys_nodes[0]], mean=self._mean, stddev = self._stddev / first_dim**0.5),\n",
    "                            trainable=True\n",
    "                                    )\n",
    "                                                  )\n",
    "                    self.Analisys_Biases.append(\n",
    "                        tf.Variable(\n",
    "                            tf.zeros([self._analisys_nodes[0]]),\n",
    "                            trainable=True\n",
    "                                    )\n",
    "                                                )\n",
    "                    for layer_num in range(1, self._analisys_layers):\n",
    "                        self.Analisys_Matrices.append(\n",
    "                            tf.Variable(\n",
    "                                tf.truncated_normal([self._analisys_nodes[layer_num-1], self._analisys_nodes[layer_num]], mean=self._mean, stddev = self._stddev / self._analisys_nodes[layer_num-1]**0.5),\n",
    "                                trainable=True\n",
    "                                        )\n",
    "                                                      ) \n",
    "                        self.Analisys_Biases.append(\n",
    "                            tf.Variable(\n",
    "                                tf.zeros([self._analisys_nodes[layer_num]]),\n",
    "                                trainable=True\n",
    "                                        )\n",
    "                                                    )\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    self.Analisys_Matrices.append(\n",
    "                        tf.Variable(\n",
    "                            tf.truncated_normal([first_dim, self._analisys_nodes[0]], mean=self._mean, stddev = self._stddev / first_dim**0.5),\n",
    "                            trainable=True\n",
    "                                    )\n",
    "                                            )\n",
    "                    self.Analisys_Biases.append(\n",
    "                        tf.Variable(\n",
    "                            tf.zeros([self._analisys_nodes[0]]),\n",
    "                            trainable=True\n",
    "                                    )\n",
    "                                          )\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "                        \n",
    "\n",
    "                # classifier \n",
    "                weights = tf.Variable(tf.truncated_normal([self._analisys_nodes[-1], self._vocabulary_size], stddev = self._stddev / self._analisys_nodes[-1]**0.5))\n",
    "                bias = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                self._train_data = list()\n",
    "                for _ in range(self._num_unrollings + 1):\n",
    "                    self._train_data.append(\n",
    "                        tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size]))\n",
    "                train_inputs = self._train_data[: self._num_unrollings]\n",
    "                train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                # Unrolled LSTM loop.\n",
    "                \n",
    "                \"\"\"global step\"\"\"\n",
    "                self._global_step = tf.Variable(0, trainable=False)\n",
    "                        \n",
    "                memory = list()\n",
    "                for saved_memory_layer in self.Saved_Memory:\n",
    "                    memory.append(list(saved_memory_layer))\n",
    "                \n",
    "                compressed_inputs = tf.nn.tanh(\n",
    "                    tf.nn.xw_plus_b(tf.concat(0, train_inputs),\n",
    "                                    self.Compress_Matrices[0],\n",
    "                                    self.Compress_Biases[0])\n",
    "                                               )\n",
    "                    \n",
    "                new_memory = list()\n",
    "                new_memory.append(tf.split(0, self._num_unrollings, compressed_inputs))\n",
    "                current_num_blocks = self._num_unrollings\n",
    "                for num_layer in range(1, self._memory_layers):\n",
    "                    block_size = self._memory_lengths[num_layer-1]\n",
    "                    current_num_blocks /= block_size\n",
    "                    for_next_layer = list()\n",
    "                    for i in range(current_num_blocks):\n",
    "                        for_next_layer.append(tf.concat(1, new_memory[num_layer-1][i*block_size : (i+1)*block_size]))\n",
    "                    layer_compressed = tf.tanh(tf.nn.xw_plus_b(tf.concat(0, for_next_layer),\n",
    "                                                               self.Compress_Matrices[num_layer],\n",
    "                                                               self.Compress_Biases[num_layer]))\n",
    "                    new_memory.append(tf.split(0, current_num_blocks, layer_compressed))\n",
    "                    \n",
    "                for idx, new_memory_layer in enumerate(new_memory):\n",
    "                    memory[idx].extend(new_memory_layer)\n",
    "                \n",
    "                X = list()\n",
    "                offsets = [0]*self._memory_layers\n",
    "                for i in range(self._num_unrollings):\n",
    "                    offsets[0] = i\n",
    "                    for layer_num in range(1, self._memory_layers):\n",
    "                        offsets[layer_num] = offsets[layer_num-1] / self._memory_lengths[layer_num-1]\n",
    "                    X_t = list()\n",
    "                    X_t.extend(memory[0][offsets[0]:offsets[0]+self._memory_lengths[0]+1])\n",
    "                    for layer_num in range(1, self._memory_layers):\n",
    "                        X_t.extend(memory[layer_num][offsets[layer_num]:offsets[layer_num]+self._memory_lengths[layer_num]])\n",
    "                    X_t = tf.concat(1, X_t)\n",
    "                    for Analisys_Matrix, Analisys_Bias in zip(self.Analisys_Matrices, self.Analisys_Biases):\n",
    "                        X_t = tf.nn.relu(tf.nn.xw_plus_b(X_t, Analisys_Matrix, Analisys_Bias))\n",
    "                    X.append(X_t)\n",
    "                        \n",
    "                self.X = tf.concat(0, X)\n",
    "                \n",
    "                with tf.control_dependencies(X):\n",
    "                    save_list = list()\n",
    "                    for layer_num in range(self._memory_layers):\n",
    "                        for i in range(self._memory_lengths[layer_num]):\n",
    "                            save_list.append(tf.assign(self.Saved_Memory[layer_num][-1-i], memory[layer_num][-1-i]))\n",
    "                \n",
    "                \"\"\"skip operation\"\"\"\n",
    "                self._skip_operation = tf.group(*save_list)\n",
    "\n",
    "                with tf.control_dependencies(save_list):\n",
    "                        # Classifier.\n",
    "                    logits = tf.nn.xw_plus_b(self.X, weights, bias)\n",
    "                    \"\"\"loss\"\"\"\n",
    "                    self._loss = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "                # Optimizer.\n",
    "                \n",
    "                \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                self._half_life = tf.placeholder(tf.int32)\n",
    "                self._decay = tf.placeholder(tf.float32)\n",
    "                \"\"\"learning rate\"\"\"\n",
    "                self._learning_rate = tf.train.exponential_decay(self._init_learning_rate,\n",
    "                                                                 self._global_step,\n",
    "                                                                 self._half_life,\n",
    "                                                                 self._decay,\n",
    "                                                                 staircase=True)\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                \"\"\"optimizer\"\"\"\n",
    "                self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                \"\"\"train prediction\"\"\"\n",
    "                self._train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "                        \n",
    "                self.Sample_Memory = list()\n",
    "                for layer_num in range(self._memory_layers):\n",
    "                    layer_memory = list()\n",
    "                    for block_num in range(self._memory_lengths[layer_num]):\n",
    "                        layer_memory.append(tf.Variable(tf.zeros([1, self._memory_nodes[layer_num]]), trainable=False))\n",
    "                    self.Sample_Memory.append(layer_memory) \n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                self.abs_freqs = [1]\n",
    "                for i in range(1, self._memory_layers):\n",
    "                    self.abs_freqs.append(self._memory_lengths[i-1] * self.abs_freqs[i-1])\n",
    "                        \n",
    "                \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size])\n",
    "                        \n",
    "                self.counter = tf.Variable(0, trainable=False)\n",
    "                reset_list = list()\n",
    "                for layer_num in range(self._memory_layers):\n",
    "                    for block_num in range(self._memory_lengths[layer_num]):\n",
    "                        reset_list.append(tf.assign(self.Sample_Memory[layer_num][block_num], tf.zeros([1, self._memory_nodes[layer_num]])))\n",
    "                reset_list.append(self.counter.assign(tf.constant(0)))\n",
    "                \"\"\"reset sample state\"\"\"\n",
    "                self._reset_sample_state = tf.group(*reset_list)\n",
    "                self.compressed_inp = tf.tanh(tf.nn.xw_plus_b(self._sample_input, self.Compress_Matrices[0], self.Compress_Biases[0]))\n",
    "                #print('self.compressed_inp.shape =', self.compressed_inp.get_shape().as_list())\n",
    "                self.sample_output = self.analize(self.compressed_inp)\n",
    "                \n",
    "                with tf.control_dependencies([self.sample_output]):\n",
    "                    sample_save_list = self.update_memory(self.compressed_inp, self._memory_layers-1)\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    sample_save_list.append(tf.assign_add(self.counter, tf.constant(1)))\n",
    "\n",
    "                with tf.control_dependencies(sample_save_list):\n",
    "                    \"\"\"sample prediction\"\"\"\n",
    "                    self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(self.sample_output, weights, bias)) \n",
    "                    self.saved_sample_memory = [memory_layer + 0.00000001 for memory_layer in self.Sample_Memory[0]]\n",
    "                \n",
    "                \"\"\"saver\"\"\"\n",
    "                self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                            \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._memory_layers)\n",
    "        metadata.append(self._memory_nodes)\n",
    "        metadata.append(self._memory_lengths)\n",
    "        metadata.append(self._analisys_layers)\n",
    "        metadata.append(self._analisys_nodes)   \n",
    "        metadata.append(self._frequency)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._mean)\n",
    "        metadata.append(self._stddev)\n",
    "        metadata.append(self._init_learning_rate)\n",
    "        metadata.append('delay_deterministic')\n",
    "        return metadata\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
